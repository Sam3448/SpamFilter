{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import data_helpers\n",
    "from text_cnn import TextCNN\n",
    "from tensorflow.contrib import learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainDataDir = \"/Users/SamZhang/Documents/Capstone/dataset/small/train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading params\n",
    "tf.flags.DEFINE_float(\"dev_sample_percentage\", .2, \"Percentage of the training data to use for validation\")\n",
    "tf.flags.DEFINE_string(\"positive_data_file\", trainDataDir + \"/spam/SMS_train.spam\", \"Data source for the positive data.\")\n",
    "tf.flags.DEFINE_string(\"negative_data_file\", trainDataDir + \"/ham/SMS_train.ham\", \"Data source for the negative data.\")\n",
    "\n",
    "# Model Hyperparameters\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 256, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.1, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 128, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 20, \"Number of training epochs (default: 200)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "BATCH_SIZE=128\n",
      "CHECKPOINT_EVERY=100\n",
      "DEV_SAMPLE_PERCENTAGE=0.2\n",
      "DROPOUT_KEEP_PROB=0.5\n",
      "EMBEDDING_DIM=128\n",
      "EVALUATE_EVERY=100\n",
      "FILTER_SIZES=3,4,5\n",
      "L2_REG_LAMBDA=0.1\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "NEGATIVE_DATA_FILE=/Users/SamZhang/Documents/Capstone/dataset/small/train/ham/SMS_train.ham\n",
      "NUM_CHECKPOINTS=5\n",
      "NUM_EPOCHS=20\n",
      "NUM_FILTERS=256\n",
      "POSITIVE_DATA_FILE=/Users/SamZhang/Documents/Capstone/dataset/small/train/spam/SMS_train.spam\n",
      "\n"
     ]
    }
   ],
   "source": [
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def processData():\n",
    "    # Data Preparation\n",
    "    # ==================================================\n",
    "\n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    x_text, y = data_helpers.load_data_and_labels(FLAGS.positive_data_file, FLAGS.negative_data_file)\n",
    "    print(x_text[0])\n",
    "\n",
    "    # Build vocabulary\n",
    "    max_document_length = max([len(x.split(\" \")) for x in x_text])\n",
    "    vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "    x = np.array(list(vocab_processor.fit_transform(x_text)))\n",
    "\n",
    "    # Randomly shuffle data\n",
    "    np.random.seed(10)\n",
    "    shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "    x_shuffled = x[shuffle_indices]\n",
    "    y_shuffled = y[shuffle_indices]\n",
    "\n",
    "    # Split train/test set\n",
    "    # TODO: This is very crude, should use cross-validation\n",
    "    dev_sample_index = -1 * int(FLAGS.dev_sample_percentage * float(len(y)))\n",
    "    x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "    y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "\n",
    "    del x, y, x_shuffled, y_shuffled\n",
    "\n",
    "    print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
    "    print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))\n",
    "    \n",
    "    return x_train, y_train, x_dev, y_dev, vocab_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry question \\( std txt rate \\) t c 's apply 08452810075over18 's\n",
      "Vocabulary Size: 6556\n",
      "Train/Dev split: 2470/617\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_dev, y_dev, vocab_processor = processData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to /Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel\n",
      "\n",
      "2018-03-22T01:27:56.974855: step 1, loss 1.95538, acc 0.507812\n",
      "2018-03-22T01:27:57.606201: step 2, loss 1.70593, acc 0.695312\n",
      "2018-03-22T01:27:58.254406: step 3, loss 1.33194, acc 0.804688\n",
      "2018-03-22T01:27:58.948788: step 4, loss 2.17285, acc 0.75\n",
      "2018-03-22T01:27:59.648297: step 5, loss 1.66585, acc 0.757812\n",
      "2018-03-22T01:28:00.352586: step 6, loss 1.72876, acc 0.757812\n",
      "2018-03-22T01:28:01.072146: step 7, loss 1.29335, acc 0.757812\n",
      "2018-03-22T01:28:01.738604: step 8, loss 1.30583, acc 0.695312\n",
      "2018-03-22T01:28:02.474672: step 9, loss 1.33684, acc 0.71875\n",
      "2018-03-22T01:28:03.149660: step 10, loss 1.71213, acc 0.609375\n",
      "2018-03-22T01:28:03.867800: step 11, loss 1.40424, acc 0.695312\n",
      "2018-03-22T01:28:04.618619: step 12, loss 1.08471, acc 0.742188\n",
      "2018-03-22T01:28:05.281300: step 13, loss 1.17411, acc 0.710938\n",
      "2018-03-22T01:28:05.933996: step 14, loss 0.915626, acc 0.773438\n",
      "2018-03-22T01:28:06.604457: step 15, loss 0.92349, acc 0.820312\n",
      "2018-03-22T01:28:07.289829: step 16, loss 1.22867, acc 0.820312\n",
      "2018-03-22T01:28:07.978127: step 17, loss 1.29122, acc 0.828125\n",
      "2018-03-22T01:28:08.646247: step 18, loss 1.07697, acc 0.820312\n",
      "2018-03-22T01:28:09.309630: step 19, loss 1.03202, acc 0.859375\n",
      "2018-03-22T01:28:09.520550: step 20, loss 1.62769, acc 0.815789\n",
      "2018-03-22T01:28:10.203450: step 21, loss 0.915918, acc 0.804688\n",
      "2018-03-22T01:28:10.861719: step 22, loss 0.83852, acc 0.828125\n",
      "2018-03-22T01:28:11.525856: step 23, loss 0.737865, acc 0.84375\n",
      "2018-03-22T01:28:12.295207: step 24, loss 0.652841, acc 0.882812\n",
      "2018-03-22T01:28:13.080942: step 25, loss 0.972153, acc 0.773438\n",
      "2018-03-22T01:28:13.799212: step 26, loss 0.705182, acc 0.859375\n",
      "2018-03-22T01:28:14.518075: step 27, loss 0.622937, acc 0.890625\n",
      "2018-03-22T01:28:15.326656: step 28, loss 0.512817, acc 0.890625\n",
      "2018-03-22T01:28:16.127479: step 29, loss 0.651802, acc 0.84375\n",
      "2018-03-22T01:28:16.904179: step 30, loss 0.51999, acc 0.882812\n",
      "2018-03-22T01:28:17.648430: step 31, loss 0.552099, acc 0.90625\n",
      "2018-03-22T01:28:18.359845: step 32, loss 0.492873, acc 0.90625\n",
      "2018-03-22T01:28:19.067890: step 33, loss 0.552918, acc 0.90625\n",
      "2018-03-22T01:28:19.798384: step 34, loss 0.743159, acc 0.867188\n",
      "2018-03-22T01:28:20.604611: step 35, loss 0.615566, acc 0.898438\n",
      "2018-03-22T01:28:21.320127: step 36, loss 0.601693, acc 0.890625\n",
      "2018-03-22T01:28:22.048574: step 37, loss 0.517027, acc 0.929688\n",
      "2018-03-22T01:28:22.768421: step 38, loss 0.444384, acc 0.898438\n",
      "2018-03-22T01:28:23.483967: step 39, loss 0.5094, acc 0.882812\n",
      "2018-03-22T01:28:23.724209: step 40, loss 0.401482, acc 0.894737\n",
      "2018-03-22T01:28:24.505352: step 41, loss 0.569235, acc 0.914062\n",
      "2018-03-22T01:28:25.264602: step 42, loss 0.43668, acc 0.921875\n",
      "2018-03-22T01:28:26.136088: step 43, loss 0.501518, acc 0.890625\n",
      "2018-03-22T01:28:26.940144: step 44, loss 0.350823, acc 0.921875\n",
      "2018-03-22T01:28:27.696902: step 45, loss 0.482142, acc 0.875\n",
      "2018-03-22T01:28:28.573435: step 46, loss 0.354031, acc 0.929688\n",
      "2018-03-22T01:28:29.330178: step 47, loss 0.480936, acc 0.898438\n",
      "2018-03-22T01:28:30.057657: step 48, loss 0.473827, acc 0.921875\n",
      "2018-03-22T01:28:30.776352: step 49, loss 0.420914, acc 0.921875\n",
      "2018-03-22T01:28:31.496087: step 50, loss 0.45701, acc 0.90625\n",
      "2018-03-22T01:28:32.231822: step 51, loss 0.571546, acc 0.859375\n",
      "2018-03-22T01:28:32.949275: step 52, loss 0.375308, acc 0.929688\n",
      "2018-03-22T01:28:33.791735: step 53, loss 0.575861, acc 0.90625\n",
      "2018-03-22T01:28:34.550092: step 54, loss 0.338081, acc 0.960938\n",
      "2018-03-22T01:28:35.290502: step 55, loss 0.375322, acc 0.945312\n",
      "2018-03-22T01:28:36.009900: step 56, loss 0.493794, acc 0.90625\n",
      "2018-03-22T01:28:36.733233: step 57, loss 0.327295, acc 0.953125\n",
      "2018-03-22T01:28:37.451970: step 58, loss 0.424677, acc 0.9375\n",
      "2018-03-22T01:28:38.161620: step 59, loss 0.298235, acc 0.9375\n",
      "2018-03-22T01:28:38.381596: step 60, loss 0.52146, acc 0.921053\n",
      "2018-03-22T01:28:39.098475: step 61, loss 0.502482, acc 0.914062\n",
      "2018-03-22T01:28:39.804121: step 62, loss 0.384049, acc 0.921875\n",
      "2018-03-22T01:28:40.507749: step 63, loss 0.305543, acc 0.945312\n",
      "2018-03-22T01:28:41.210415: step 64, loss 0.339596, acc 0.9375\n",
      "2018-03-22T01:28:41.924205: step 65, loss 0.427389, acc 0.929688\n",
      "2018-03-22T01:28:42.629851: step 66, loss 0.369512, acc 0.9375\n",
      "2018-03-22T01:28:43.335564: step 67, loss 0.559449, acc 0.90625\n",
      "2018-03-22T01:28:44.054202: step 68, loss 0.330908, acc 0.953125\n",
      "2018-03-22T01:28:44.761447: step 69, loss 0.339727, acc 0.9375\n",
      "2018-03-22T01:28:45.486042: step 70, loss 0.287741, acc 0.9375\n",
      "2018-03-22T01:28:46.198959: step 71, loss 0.265499, acc 0.984375\n",
      "2018-03-22T01:28:46.889172: step 72, loss 0.275323, acc 0.953125\n",
      "2018-03-22T01:28:47.600595: step 73, loss 0.391939, acc 0.921875\n",
      "2018-03-22T01:28:48.309993: step 74, loss 0.279058, acc 0.976562\n",
      "2018-03-22T01:28:49.013610: step 75, loss 0.558929, acc 0.890625\n",
      "2018-03-22T01:28:49.719504: step 76, loss 0.379708, acc 0.921875\n",
      "2018-03-22T01:28:50.431736: step 77, loss 0.356297, acc 0.953125\n",
      "2018-03-22T01:28:51.134858: step 78, loss 0.468133, acc 0.953125\n",
      "2018-03-22T01:28:51.836940: step 79, loss 0.28388, acc 0.96875\n",
      "2018-03-22T01:28:52.060072: step 80, loss 0.445527, acc 0.894737\n",
      "2018-03-22T01:28:52.766731: step 81, loss 0.349018, acc 0.9375\n",
      "2018-03-22T01:28:53.473557: step 82, loss 0.378425, acc 0.921875\n",
      "2018-03-22T01:28:54.187357: step 83, loss 0.317015, acc 0.9375\n",
      "2018-03-22T01:28:54.899232: step 84, loss 0.261984, acc 0.96875\n",
      "2018-03-22T01:28:55.613496: step 85, loss 0.345945, acc 0.9375\n",
      "2018-03-22T01:28:56.322476: step 86, loss 0.293225, acc 0.953125\n",
      "2018-03-22T01:28:57.038436: step 87, loss 0.318541, acc 0.953125\n",
      "2018-03-22T01:28:57.753823: step 88, loss 0.432171, acc 0.9375\n",
      "2018-03-22T01:28:58.483886: step 89, loss 0.329217, acc 0.96875\n",
      "2018-03-22T01:28:59.229852: step 90, loss 0.352826, acc 0.945312\n",
      "2018-03-22T01:28:59.968482: step 91, loss 0.365355, acc 0.929688\n",
      "2018-03-22T01:29:00.705657: step 92, loss 0.297718, acc 0.976562\n",
      "2018-03-22T01:29:01.454292: step 93, loss 0.316128, acc 0.953125\n",
      "2018-03-22T01:29:02.190220: step 94, loss 0.252827, acc 0.953125\n",
      "2018-03-22T01:29:02.893712: step 95, loss 0.311829, acc 0.945312\n",
      "2018-03-22T01:29:03.594978: step 96, loss 0.304349, acc 0.945312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-22T01:29:04.308082: step 97, loss 0.314651, acc 0.953125\n",
      "2018-03-22T01:29:05.014744: step 98, loss 0.371238, acc 0.960938\n",
      "2018-03-22T01:29:05.713451: step 99, loss 0.293817, acc 0.9375\n",
      "2018-03-22T01:29:05.943018: step 100, loss 0.216244, acc 0.973684\n",
      "\n",
      "Evaluation:\n",
      "2018-03-22T01:29:07.110987: step 100, loss 0.335885, acc 0.961102\n",
      "\n",
      "Saved model checkpoint to /Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel/checkpoints/model-100\n",
      "\n",
      "2018-03-22T01:29:07.995605: step 101, loss 0.243398, acc 0.960938\n",
      "2018-03-22T01:29:08.698035: step 102, loss 0.322938, acc 0.945312\n",
      "2018-03-22T01:29:09.418756: step 103, loss 0.31147, acc 0.960938\n",
      "2018-03-22T01:29:10.151963: step 104, loss 0.279677, acc 0.976562\n",
      "2018-03-22T01:29:10.888718: step 105, loss 0.178388, acc 1\n",
      "2018-03-22T01:29:11.619729: step 106, loss 0.276532, acc 0.960938\n",
      "2018-03-22T01:29:12.348253: step 107, loss 0.31854, acc 0.9375\n",
      "2018-03-22T01:29:13.081931: step 108, loss 0.256074, acc 0.96875\n",
      "2018-03-22T01:29:13.807489: step 109, loss 0.325355, acc 0.960938\n",
      "2018-03-22T01:29:14.539761: step 110, loss 0.304019, acc 0.953125\n",
      "2018-03-22T01:29:15.281823: step 111, loss 0.295835, acc 0.953125\n",
      "2018-03-22T01:29:16.017688: step 112, loss 0.244495, acc 0.96875\n",
      "2018-03-22T01:29:16.740172: step 113, loss 0.205862, acc 0.984375\n",
      "2018-03-22T01:29:17.475677: step 114, loss 0.214372, acc 0.976562\n",
      "2018-03-22T01:29:18.206180: step 115, loss 0.233874, acc 0.96875\n",
      "2018-03-22T01:29:18.935951: step 116, loss 0.277701, acc 0.960938\n",
      "2018-03-22T01:29:19.666413: step 117, loss 0.325377, acc 0.945312\n",
      "2018-03-22T01:29:20.399863: step 118, loss 0.343895, acc 0.945312\n",
      "2018-03-22T01:29:21.129517: step 119, loss 0.224054, acc 0.96875\n",
      "2018-03-22T01:29:21.367620: step 120, loss 0.298999, acc 0.973684\n",
      "2018-03-22T01:29:22.103946: step 121, loss 0.259663, acc 0.984375\n",
      "2018-03-22T01:29:22.845609: step 122, loss 0.285367, acc 0.96875\n",
      "2018-03-22T01:29:23.577572: step 123, loss 0.235418, acc 0.96875\n",
      "2018-03-22T01:29:24.308441: step 124, loss 0.297434, acc 0.945312\n",
      "2018-03-22T01:29:25.038002: step 125, loss 0.271817, acc 0.945312\n",
      "2018-03-22T01:29:25.776641: step 126, loss 0.251528, acc 0.960938\n",
      "2018-03-22T01:29:26.533901: step 127, loss 0.275319, acc 0.96875\n",
      "2018-03-22T01:29:27.294968: step 128, loss 0.379097, acc 0.960938\n",
      "2018-03-22T01:29:28.054865: step 129, loss 0.228243, acc 0.96875\n",
      "2018-03-22T01:29:28.822672: step 130, loss 0.31038, acc 0.953125\n",
      "2018-03-22T01:29:29.592017: step 131, loss 0.282358, acc 0.976562\n",
      "2018-03-22T01:29:30.357456: step 132, loss 0.221941, acc 0.96875\n",
      "2018-03-22T01:29:31.106626: step 133, loss 0.285304, acc 0.953125\n",
      "2018-03-22T01:29:31.845461: step 134, loss 0.202932, acc 0.992188\n",
      "2018-03-22T01:29:32.581272: step 135, loss 0.221536, acc 0.984375\n",
      "2018-03-22T01:29:33.312876: step 136, loss 0.236764, acc 0.976562\n",
      "2018-03-22T01:29:34.045825: step 137, loss 0.264869, acc 0.976562\n",
      "2018-03-22T01:29:34.772720: step 138, loss 0.216567, acc 0.976562\n",
      "2018-03-22T01:29:35.504396: step 139, loss 0.239623, acc 0.984375\n",
      "2018-03-22T01:29:35.747474: step 140, loss 0.179903, acc 1\n",
      "2018-03-22T01:29:36.478162: step 141, loss 0.363765, acc 0.984375\n",
      "2018-03-22T01:29:37.213965: step 142, loss 0.242922, acc 0.960938\n",
      "2018-03-22T01:29:37.946693: step 143, loss 0.269727, acc 0.960938\n",
      "2018-03-22T01:29:38.668413: step 144, loss 0.219124, acc 0.96875\n",
      "2018-03-22T01:29:39.400918: step 145, loss 0.29122, acc 0.953125\n",
      "2018-03-22T01:29:40.147175: step 146, loss 0.243422, acc 0.960938\n",
      "2018-03-22T01:29:40.875368: step 147, loss 0.204214, acc 0.992188\n",
      "2018-03-22T01:29:41.623525: step 148, loss 0.223004, acc 0.976562\n",
      "2018-03-22T01:29:42.381648: step 149, loss 0.187035, acc 0.992188\n",
      "2018-03-22T01:29:43.150544: step 150, loss 0.24374, acc 0.96875\n",
      "2018-03-22T01:29:43.904193: step 151, loss 0.271788, acc 0.96875\n",
      "2018-03-22T01:29:44.671208: step 152, loss 0.209802, acc 0.984375\n",
      "2018-03-22T01:29:45.472209: step 153, loss 0.199852, acc 0.984375\n",
      "2018-03-22T01:29:46.233869: step 154, loss 0.246099, acc 0.96875\n",
      "2018-03-22T01:29:46.990506: step 155, loss 0.246213, acc 0.96875\n",
      "2018-03-22T01:29:47.728369: step 156, loss 0.272192, acc 0.96875\n",
      "2018-03-22T01:29:48.466884: step 157, loss 0.204375, acc 0.984375\n",
      "2018-03-22T01:29:49.203919: step 158, loss 0.191941, acc 0.984375\n",
      "2018-03-22T01:29:49.939373: step 159, loss 0.251058, acc 0.960938\n",
      "2018-03-22T01:29:50.182366: step 160, loss 0.227745, acc 0.973684\n",
      "2018-03-22T01:29:50.918127: step 161, loss 0.194062, acc 0.992188\n",
      "2018-03-22T01:29:51.637674: step 162, loss 0.244621, acc 0.960938\n",
      "2018-03-22T01:29:52.367109: step 163, loss 0.174079, acc 1\n",
      "2018-03-22T01:29:53.101332: step 164, loss 0.227131, acc 0.984375\n",
      "2018-03-22T01:29:53.837187: step 165, loss 0.210645, acc 0.96875\n",
      "2018-03-22T01:29:54.567019: step 166, loss 0.184887, acc 0.992188\n",
      "2018-03-22T01:29:55.301587: step 167, loss 0.194498, acc 0.96875\n",
      "2018-03-22T01:29:56.035540: step 168, loss 0.186038, acc 0.984375\n",
      "2018-03-22T01:29:56.766918: step 169, loss 0.187399, acc 0.992188\n",
      "2018-03-22T01:29:57.493955: step 170, loss 0.273356, acc 0.960938\n",
      "2018-03-22T01:29:58.231574: step 171, loss 0.266614, acc 0.992188\n",
      "2018-03-22T01:29:59.042275: step 172, loss 0.186737, acc 0.976562\n",
      "2018-03-22T01:29:59.816179: step 173, loss 0.173719, acc 0.992188\n",
      "2018-03-22T01:30:00.595413: step 174, loss 0.218875, acc 0.976562\n",
      "2018-03-22T01:30:01.361570: step 175, loss 0.18361, acc 0.984375\n",
      "2018-03-22T01:30:02.130451: step 176, loss 0.228208, acc 0.96875\n",
      "2018-03-22T01:30:02.890249: step 177, loss 0.167106, acc 0.992188\n",
      "2018-03-22T01:30:03.645899: step 178, loss 0.195076, acc 0.976562\n",
      "2018-03-22T01:30:04.400696: step 179, loss 0.25659, acc 0.976562\n",
      "2018-03-22T01:30:04.633666: step 180, loss 0.264551, acc 0.947368\n",
      "2018-03-22T01:30:05.377884: step 181, loss 0.174867, acc 0.992188\n",
      "2018-03-22T01:30:06.114122: step 182, loss 0.234979, acc 0.984375\n",
      "2018-03-22T01:30:06.843450: step 183, loss 0.191249, acc 0.976562\n",
      "2018-03-22T01:30:07.579854: step 184, loss 0.231707, acc 0.953125\n",
      "2018-03-22T01:30:08.310097: step 185, loss 0.166383, acc 1\n",
      "2018-03-22T01:30:09.055206: step 186, loss 0.21031, acc 0.976562\n",
      "2018-03-22T01:30:09.793657: step 187, loss 0.206391, acc 0.96875\n",
      "2018-03-22T01:30:10.521051: step 188, loss 0.22492, acc 0.976562\n",
      "2018-03-22T01:30:11.245984: step 189, loss 0.21194, acc 0.960938\n",
      "2018-03-22T01:30:11.977265: step 190, loss 0.186909, acc 0.984375\n",
      "2018-03-22T01:30:12.714446: step 191, loss 0.171039, acc 1\n",
      "2018-03-22T01:30:13.441430: step 192, loss 0.25863, acc 0.960938\n",
      "2018-03-22T01:30:14.180256: step 193, loss 0.17231, acc 0.992188\n",
      "2018-03-22T01:30:14.927464: step 194, loss 0.176006, acc 0.992188\n",
      "2018-03-22T01:30:15.671754: step 195, loss 0.176806, acc 0.992188\n",
      "2018-03-22T01:30:16.430403: step 196, loss 0.165105, acc 0.992188\n",
      "2018-03-22T01:30:17.192927: step 197, loss 0.165024, acc 0.992188\n",
      "2018-03-22T01:30:17.955434: step 198, loss 0.158977, acc 1\n",
      "2018-03-22T01:30:18.707451: step 199, loss 0.198603, acc 0.984375\n",
      "2018-03-22T01:30:18.953269: step 200, loss 0.246756, acc 0.947368\n",
      "\n",
      "Evaluation:\n",
      "2018-03-22T01:30:20.153014: step 200, loss 0.319521, acc 0.962723\n",
      "\n",
      "Saved model checkpoint to /Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel/checkpoints/model-200\n",
      "\n",
      "2018-03-22T01:30:21.044787: step 201, loss 0.196615, acc 0.984375\n",
      "2018-03-22T01:30:21.798585: step 202, loss 0.184394, acc 0.984375\n",
      "2018-03-22T01:30:22.537527: step 203, loss 0.198416, acc 0.976562\n",
      "2018-03-22T01:30:23.275564: step 204, loss 0.162369, acc 0.992188\n",
      "2018-03-22T01:30:24.000803: step 205, loss 0.215667, acc 0.984375\n",
      "2018-03-22T01:30:24.730449: step 206, loss 0.175448, acc 0.992188\n",
      "2018-03-22T01:30:25.492232: step 207, loss 0.169919, acc 0.992188\n",
      "2018-03-22T01:30:26.297134: step 208, loss 0.29374, acc 0.953125\n",
      "2018-03-22T01:30:27.028495: step 209, loss 0.162855, acc 1\n",
      "2018-03-22T01:30:27.733262: step 210, loss 0.242014, acc 0.960938\n",
      "2018-03-22T01:30:28.445154: step 211, loss 0.160994, acc 1\n",
      "2018-03-22T01:30:29.180441: step 212, loss 0.174158, acc 0.992188\n",
      "2018-03-22T01:30:29.925023: step 213, loss 0.170952, acc 0.992188\n",
      "2018-03-22T01:30:30.660079: step 214, loss 0.177102, acc 0.984375\n",
      "2018-03-22T01:30:31.391411: step 215, loss 0.190486, acc 0.984375\n",
      "2018-03-22T01:30:32.113669: step 216, loss 0.150975, acc 1\n",
      "2018-03-22T01:30:32.851936: step 217, loss 0.172217, acc 0.984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-22T01:30:33.581678: step 218, loss 0.201953, acc 0.984375\n",
      "2018-03-22T01:30:34.323051: step 219, loss 0.166671, acc 0.984375\n",
      "2018-03-22T01:30:34.557014: step 220, loss 0.166984, acc 1\n",
      "2018-03-22T01:30:35.288325: step 221, loss 0.17389, acc 0.992188\n",
      "2018-03-22T01:30:36.017388: step 222, loss 0.148059, acc 1\n",
      "2018-03-22T01:30:36.751951: step 223, loss 0.155127, acc 1\n",
      "2018-03-22T01:30:37.487940: step 224, loss 0.156268, acc 1\n",
      "2018-03-22T01:30:38.198669: step 225, loss 0.165946, acc 0.992188\n",
      "2018-03-22T01:30:38.901994: step 226, loss 0.16134, acc 1\n",
      "2018-03-22T01:30:39.603417: step 227, loss 0.182117, acc 0.976562\n",
      "2018-03-22T01:30:40.322687: step 228, loss 0.182586, acc 0.984375\n",
      "2018-03-22T01:30:41.039166: step 229, loss 0.153145, acc 1\n",
      "2018-03-22T01:30:41.755150: step 230, loss 0.156016, acc 1\n",
      "2018-03-22T01:30:42.475201: step 231, loss 0.156835, acc 0.992188\n",
      "2018-03-22T01:30:43.188298: step 232, loss 0.211199, acc 0.96875\n",
      "2018-03-22T01:30:43.890664: step 233, loss 0.151911, acc 1\n",
      "2018-03-22T01:30:44.607875: step 234, loss 0.177387, acc 0.984375\n",
      "2018-03-22T01:30:45.313989: step 235, loss 0.167955, acc 0.992188\n",
      "2018-03-22T01:30:46.027742: step 236, loss 0.16481, acc 0.992188\n",
      "2018-03-22T01:30:46.733660: step 237, loss 0.152851, acc 1\n",
      "2018-03-22T01:30:47.449773: step 238, loss 0.150487, acc 1\n",
      "2018-03-22T01:30:48.161591: step 239, loss 0.147466, acc 1\n",
      "2018-03-22T01:30:48.384530: step 240, loss 0.143884, acc 1\n",
      "2018-03-22T01:30:49.099813: step 241, loss 0.166423, acc 0.992188\n",
      "2018-03-22T01:30:49.802330: step 242, loss 0.177509, acc 0.984375\n",
      "2018-03-22T01:30:50.506449: step 243, loss 0.164068, acc 0.984375\n",
      "2018-03-22T01:30:51.213867: step 244, loss 0.145913, acc 1\n",
      "2018-03-22T01:30:51.950868: step 245, loss 0.148479, acc 0.992188\n",
      "2018-03-22T01:30:52.674914: step 246, loss 0.158998, acc 0.992188\n",
      "2018-03-22T01:30:53.407610: step 247, loss 0.167913, acc 0.992188\n",
      "2018-03-22T01:30:54.148391: step 248, loss 0.153391, acc 0.992188\n",
      "2018-03-22T01:30:54.871823: step 249, loss 0.194753, acc 0.984375\n",
      "2018-03-22T01:30:55.605865: step 250, loss 0.149982, acc 1\n",
      "2018-03-22T01:30:56.312713: step 251, loss 0.146347, acc 1\n",
      "2018-03-22T01:30:57.035340: step 252, loss 0.191309, acc 0.984375\n",
      "2018-03-22T01:30:57.743853: step 253, loss 0.145784, acc 1\n",
      "2018-03-22T01:30:58.453493: step 254, loss 0.147866, acc 0.992188\n",
      "2018-03-22T01:30:59.174310: step 255, loss 0.178396, acc 0.992188\n",
      "2018-03-22T01:30:59.877712: step 256, loss 0.164087, acc 0.992188\n",
      "2018-03-22T01:31:00.589935: step 257, loss 0.172501, acc 0.984375\n",
      "2018-03-22T01:31:01.298759: step 258, loss 0.144007, acc 1\n",
      "2018-03-22T01:31:02.000955: step 259, loss 0.174385, acc 0.984375\n",
      "2018-03-22T01:31:02.233916: step 260, loss 0.137402, acc 1\n",
      "2018-03-22T01:31:02.944946: step 261, loss 0.142023, acc 1\n",
      "2018-03-22T01:31:03.645032: step 262, loss 0.147842, acc 0.992188\n",
      "2018-03-22T01:31:04.355725: step 263, loss 0.152083, acc 0.992188\n",
      "2018-03-22T01:31:05.062771: step 264, loss 0.146391, acc 0.992188\n",
      "2018-03-22T01:31:05.766095: step 265, loss 0.142205, acc 1\n",
      "2018-03-22T01:31:06.473248: step 266, loss 0.148896, acc 1\n",
      "2018-03-22T01:31:07.177649: step 267, loss 0.142214, acc 0.992188\n",
      "2018-03-22T01:31:07.877340: step 268, loss 0.154738, acc 0.992188\n",
      "2018-03-22T01:31:08.581871: step 269, loss 0.160385, acc 0.984375\n",
      "2018-03-22T01:31:09.286941: step 270, loss 0.158196, acc 0.984375\n",
      "2018-03-22T01:31:09.993693: step 271, loss 0.142772, acc 0.992188\n",
      "2018-03-22T01:31:10.698851: step 272, loss 0.158214, acc 0.992188\n",
      "2018-03-22T01:31:11.408586: step 273, loss 0.147846, acc 0.992188\n",
      "2018-03-22T01:31:12.114607: step 274, loss 0.155281, acc 0.992188\n",
      "2018-03-22T01:31:12.819269: step 275, loss 0.139903, acc 1\n",
      "2018-03-22T01:31:13.526562: step 276, loss 0.185834, acc 0.976562\n",
      "2018-03-22T01:31:14.235005: step 277, loss 0.187029, acc 0.984375\n",
      "2018-03-22T01:31:14.941834: step 278, loss 0.150205, acc 0.992188\n",
      "2018-03-22T01:31:15.647896: step 279, loss 0.15127, acc 0.984375\n",
      "2018-03-22T01:31:15.876876: step 280, loss 0.148993, acc 1\n",
      "2018-03-22T01:31:16.579370: step 281, loss 0.185714, acc 0.984375\n",
      "2018-03-22T01:31:17.296251: step 282, loss 0.136748, acc 1\n",
      "2018-03-22T01:31:17.999009: step 283, loss 0.142192, acc 0.992188\n",
      "2018-03-22T01:31:18.703666: step 284, loss 0.156758, acc 0.984375\n",
      "2018-03-22T01:31:19.414363: step 285, loss 0.137607, acc 1\n",
      "2018-03-22T01:31:20.115525: step 286, loss 0.178701, acc 0.976562\n",
      "2018-03-22T01:31:20.818055: step 287, loss 0.136253, acc 1\n",
      "2018-03-22T01:31:21.523181: step 288, loss 0.14298, acc 1\n",
      "2018-03-22T01:31:22.227862: step 289, loss 0.13442, acc 1\n",
      "2018-03-22T01:31:22.946250: step 290, loss 0.155181, acc 0.992188\n",
      "2018-03-22T01:31:23.653542: step 291, loss 0.155778, acc 0.992188\n",
      "2018-03-22T01:31:24.366842: step 292, loss 0.1383, acc 1\n",
      "2018-03-22T01:31:25.077992: step 293, loss 0.132409, acc 1\n",
      "2018-03-22T01:31:25.789398: step 294, loss 0.162445, acc 0.992188\n",
      "2018-03-22T01:31:26.493735: step 295, loss 0.149041, acc 0.992188\n",
      "2018-03-22T01:31:27.225194: step 296, loss 0.167486, acc 0.984375\n",
      "2018-03-22T01:31:27.936163: step 297, loss 0.151366, acc 1\n",
      "2018-03-22T01:31:28.655299: step 298, loss 0.152499, acc 0.992188\n",
      "2018-03-22T01:31:29.384342: step 299, loss 0.145283, acc 0.992188\n",
      "2018-03-22T01:31:29.694796: step 300, loss 0.133744, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-03-22T01:31:30.904550: step 300, loss 0.295509, acc 0.964344\n",
      "\n",
      "Saved model checkpoint to /Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel/checkpoints/model-300\n",
      "\n",
      "2018-03-22T01:31:31.713553: step 301, loss 0.129957, acc 1\n",
      "2018-03-22T01:31:32.401910: step 302, loss 0.16148, acc 0.984375\n",
      "2018-03-22T01:31:33.083244: step 303, loss 0.142462, acc 0.992188\n",
      "2018-03-22T01:31:33.759063: step 304, loss 0.17636, acc 0.992188\n",
      "2018-03-22T01:31:34.444041: step 305, loss 0.132808, acc 1\n",
      "2018-03-22T01:31:35.135654: step 306, loss 0.135834, acc 1\n",
      "2018-03-22T01:31:35.839769: step 307, loss 0.165578, acc 0.984375\n",
      "2018-03-22T01:31:36.551314: step 308, loss 0.165768, acc 0.984375\n",
      "2018-03-22T01:31:37.260756: step 309, loss 0.168478, acc 0.984375\n",
      "2018-03-22T01:31:37.967973: step 310, loss 0.164442, acc 0.976562\n",
      "2018-03-22T01:31:38.670366: step 311, loss 0.17649, acc 0.984375\n",
      "2018-03-22T01:31:39.378538: step 312, loss 0.147525, acc 0.976562\n",
      "2018-03-22T01:31:40.088468: step 313, loss 0.153309, acc 0.984375\n",
      "2018-03-22T01:31:40.794553: step 314, loss 0.14089, acc 0.992188\n",
      "2018-03-22T01:31:41.502516: step 315, loss 0.13641, acc 0.992188\n",
      "2018-03-22T01:31:42.208982: step 316, loss 0.155237, acc 0.984375\n",
      "2018-03-22T01:31:42.919223: step 317, loss 0.147052, acc 0.984375\n",
      "2018-03-22T01:31:43.627473: step 318, loss 0.180476, acc 0.976562\n",
      "2018-03-22T01:31:44.340476: step 319, loss 0.148942, acc 0.992188\n",
      "2018-03-22T01:31:44.568526: step 320, loss 0.127364, acc 1\n",
      "2018-03-22T01:31:45.295367: step 321, loss 0.124066, acc 1\n",
      "2018-03-22T01:31:46.005215: step 322, loss 0.159618, acc 0.984375\n",
      "2018-03-22T01:31:46.704082: step 323, loss 0.13408, acc 0.992188\n",
      "2018-03-22T01:31:47.420866: step 324, loss 0.123529, acc 1\n",
      "2018-03-22T01:31:48.126534: step 325, loss 0.138397, acc 0.984375\n",
      "2018-03-22T01:31:48.832536: step 326, loss 0.134699, acc 0.992188\n",
      "2018-03-22T01:31:49.538994: step 327, loss 0.125618, acc 1\n",
      "2018-03-22T01:31:50.244094: step 328, loss 0.159416, acc 0.992188\n",
      "2018-03-22T01:31:50.923771: step 329, loss 0.142677, acc 0.992188\n",
      "2018-03-22T01:31:51.605788: step 330, loss 0.130519, acc 1\n",
      "2018-03-22T01:31:52.292217: step 331, loss 0.130206, acc 0.992188\n",
      "2018-03-22T01:31:52.972880: step 332, loss 0.121644, acc 1\n",
      "2018-03-22T01:31:53.657157: step 333, loss 0.160375, acc 0.984375\n",
      "2018-03-22T01:31:54.356928: step 334, loss 0.153466, acc 0.984375\n",
      "2018-03-22T01:31:55.071997: step 335, loss 0.141288, acc 0.984375\n",
      "2018-03-22T01:31:55.780777: step 336, loss 0.125235, acc 1\n",
      "2018-03-22T01:31:56.486754: step 337, loss 0.121362, acc 1\n",
      "2018-03-22T01:31:57.201451: step 338, loss 0.128925, acc 0.992188\n",
      "2018-03-22T01:31:57.909917: step 339, loss 0.135059, acc 0.992188\n",
      "2018-03-22T01:31:58.138122: step 340, loss 0.119781, acc 1\n",
      "2018-03-22T01:31:58.889130: step 341, loss 0.122314, acc 1\n",
      "2018-03-22T01:31:59.591469: step 342, loss 0.122404, acc 1\n",
      "2018-03-22T01:32:00.319448: step 343, loss 0.140408, acc 0.992188\n",
      "2018-03-22T01:32:01.041286: step 344, loss 0.131846, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-22T01:32:01.748404: step 345, loss 0.124975, acc 0.992188\n",
      "2018-03-22T01:32:02.453361: step 346, loss 0.119001, acc 1\n",
      "2018-03-22T01:32:03.169544: step 347, loss 0.127783, acc 0.992188\n",
      "2018-03-22T01:32:03.886140: step 348, loss 0.136033, acc 0.984375\n",
      "2018-03-22T01:32:04.589316: step 349, loss 0.126499, acc 0.992188\n",
      "2018-03-22T01:32:05.286482: step 350, loss 0.120986, acc 1\n",
      "2018-03-22T01:32:05.995867: step 351, loss 0.121003, acc 1\n",
      "2018-03-22T01:32:06.700432: step 352, loss 0.130979, acc 0.992188\n",
      "2018-03-22T01:32:07.382962: step 353, loss 0.124511, acc 1\n",
      "2018-03-22T01:32:08.075501: step 354, loss 0.120544, acc 1\n",
      "2018-03-22T01:32:08.755046: step 355, loss 0.117884, acc 1\n",
      "2018-03-22T01:32:09.446656: step 356, loss 0.120241, acc 1\n",
      "2018-03-22T01:32:10.137315: step 357, loss 0.1448, acc 0.992188\n",
      "2018-03-22T01:32:10.811729: step 358, loss 0.1162, acc 1\n",
      "2018-03-22T01:32:11.494488: step 359, loss 0.130989, acc 0.984375\n",
      "2018-03-22T01:32:11.709227: step 360, loss 0.113202, acc 1\n",
      "2018-03-22T01:32:12.404302: step 361, loss 0.120874, acc 0.992188\n",
      "2018-03-22T01:32:13.117540: step 362, loss 0.133759, acc 0.984375\n",
      "2018-03-22T01:32:13.820438: step 363, loss 0.129422, acc 0.992188\n",
      "2018-03-22T01:32:14.526235: step 364, loss 0.129551, acc 0.992188\n",
      "2018-03-22T01:32:15.239134: step 365, loss 0.116395, acc 1\n",
      "2018-03-22T01:32:15.942736: step 366, loss 0.133924, acc 0.984375\n",
      "2018-03-22T01:32:16.641092: step 367, loss 0.118561, acc 1\n",
      "2018-03-22T01:32:17.358031: step 368, loss 0.119452, acc 0.992188\n",
      "2018-03-22T01:32:18.064992: step 369, loss 0.122299, acc 0.992188\n",
      "2018-03-22T01:32:18.761718: step 370, loss 0.135603, acc 0.992188\n",
      "2018-03-22T01:32:19.470700: step 371, loss 0.118962, acc 1\n",
      "2018-03-22T01:32:20.185306: step 372, loss 0.115589, acc 1\n",
      "2018-03-22T01:32:20.904326: step 373, loss 0.148666, acc 0.992188\n",
      "2018-03-22T01:32:21.605096: step 374, loss 0.127185, acc 0.992188\n",
      "2018-03-22T01:32:22.294119: step 375, loss 0.119799, acc 1\n",
      "2018-03-22T01:32:22.984679: step 376, loss 0.122126, acc 0.984375\n",
      "2018-03-22T01:32:23.664397: step 377, loss 0.116723, acc 1\n",
      "2018-03-22T01:32:24.348164: step 378, loss 0.122482, acc 1\n",
      "2018-03-22T01:32:25.032549: step 379, loss 0.119896, acc 0.992188\n",
      "2018-03-22T01:32:25.251024: step 380, loss 0.109254, acc 1\n",
      "2018-03-22T01:32:25.936189: step 381, loss 0.117829, acc 1\n",
      "2018-03-22T01:32:26.621412: step 382, loss 0.116465, acc 0.992188\n",
      "2018-03-22T01:32:27.331473: step 383, loss 0.109794, acc 1\n",
      "2018-03-22T01:32:28.037313: step 384, loss 0.110696, acc 1\n",
      "2018-03-22T01:32:28.761658: step 385, loss 0.110648, acc 1\n",
      "2018-03-22T01:32:29.473547: step 386, loss 0.129484, acc 0.984375\n",
      "2018-03-22T01:32:30.190785: step 387, loss 0.129539, acc 0.992188\n",
      "2018-03-22T01:32:30.896947: step 388, loss 0.107384, acc 1\n",
      "2018-03-22T01:32:31.604665: step 389, loss 0.108381, acc 1\n",
      "2018-03-22T01:32:32.317633: step 390, loss 0.10836, acc 1\n",
      "2018-03-22T01:32:33.016131: step 391, loss 0.112656, acc 0.992188\n",
      "2018-03-22T01:32:33.712575: step 392, loss 0.115788, acc 0.992188\n",
      "2018-03-22T01:32:34.418273: step 393, loss 0.140367, acc 0.992188\n",
      "2018-03-22T01:32:35.125993: step 394, loss 0.120078, acc 0.992188\n",
      "2018-03-22T01:32:35.833190: step 395, loss 0.107685, acc 1\n",
      "2018-03-22T01:32:36.536832: step 396, loss 0.111121, acc 1\n",
      "2018-03-22T01:32:37.241321: step 397, loss 0.11473, acc 1\n",
      "2018-03-22T01:32:37.945618: step 398, loss 0.106123, acc 1\n",
      "2018-03-22T01:32:38.638103: step 399, loss 0.108499, acc 1\n",
      "2018-03-22T01:32:38.861819: step 400, loss 0.117787, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-03-22T01:32:39.914262: step 400, loss 0.227191, acc 0.974068\n",
      "\n",
      "Saved model checkpoint to /Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel/checkpoints/model-400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "      log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            num_classes=y_train.shape[1],\n",
    "            vocab_size=len(vocab_processor.vocabulary_),\n",
    "            embedding_size=FLAGS.embedding_dim,\n",
    "            filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "            num_filters=FLAGS.num_filters,\n",
    "            l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "\n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", \"cnnmodel\"))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "\n",
    "        # Write vocabulary\n",
    "        vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, accuracy = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "\n",
    "        # Generate batches\n",
    "        batches = data_helpers.batch_iter(\n",
    "            list(zip(x_train, y_train)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % FLAGS.evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % FLAGS.checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testDataDir = \"/Users/SamZhang/Documents/Capstone/dataset/small/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.flags.DEFINE_string(\"positive_test_data_file\", testDataDir + \"/spam/SMS_test.spam\", \"Data source for the positive data.\")\n",
    "tf.flags.DEFINE_string(\"negative_test_data_file\", testDataDir + \"/ham/SMS_test.ham\", \"Data source for the negative data.\")\n",
    "\n",
    "# Eval Parameters\n",
    "tf.flags.DEFINE_string(\"checkpoint_dir\", \"/Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel/checkpoints/\", \"Checkpoint directory from training run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "BATCH_SIZE=128\n",
      "CHECKPOINT_DIR=/Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel/checkpoints/\n",
      "CHECKPOINT_EVERY=100\n",
      "DEV_SAMPLE_PERCENTAGE=0.2\n",
      "DROPOUT_KEEP_PROB=0.5\n",
      "EMBEDDING_DIM=128\n",
      "EVALUATE_EVERY=100\n",
      "FILTER_SIZES=3,4,5\n",
      "L2_REG_LAMBDA=0.1\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "NEGATIVE_DATA_FILE=/Users/SamZhang/Documents/Capstone/dataset/small/train/ham/SMS_train.ham\n",
      "NEGATIVE_TEST_DATA_FILE=/Users/SamZhang/Documents/Capstone/dataset/small/test/ham/SMS_test.ham\n",
      "NUM_CHECKPOINTS=5\n",
      "NUM_EPOCHS=20\n",
      "NUM_FILTERS=256\n",
      "POSITIVE_DATA_FILE=/Users/SamZhang/Documents/Capstone/dataset/small/train/spam/SMS_train.spam\n",
      "POSITIVE_TEST_DATA_FILE=/Users/SamZhang/Documents/Capstone/dataset/small/test/spam/SMS_test.spam\n",
      "\n"
     ]
    }
   ],
   "source": [
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07732584351 rodger burns msg we tried to call you re your reply to our sms for a free nokia mobile free camcorder please call now 08000930705 for delivery tomorrow 1\n"
     ]
    }
   ],
   "source": [
    "x_raw, y_test = data_helpers.load_data_and_labels(FLAGS.positive_test_data_file, FLAGS.negative_test_data_file)\n",
    "y_test = np.argmax(y_test, axis=1) #ham = 0, spam = 1\n",
    "print(x_raw[0], y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_path = os.path.join(FLAGS.checkpoint_dir, \"..\", \"vocab\")\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor.restore(vocab_path)\n",
    "x_test = np.array(list(vocab_processor.transform(x_raw)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint_file = tf.train.latest_checkpoint(FLAGS.checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel/checkpoints/model-400\n",
      "Total number of test examples: 2487\n",
      "Accuracy: 0.990752\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "      log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        # Load the saved meta graph and restore variables\n",
    "        saver = tf.train.import_meta_graph(\"{}.meta\".format(checkpoint_file))\n",
    "        saver.restore(sess, checkpoint_file)\n",
    "\n",
    "        # Get the placeholders from the graph by name\n",
    "        input_x = graph.get_operation_by_name(\"input_x\").outputs[0]\n",
    "        # input_y = graph.get_operation_by_name(\"input_y\").outputs[0]\n",
    "        dropout_keep_prob = graph.get_operation_by_name(\"dropout_keep_prob\").outputs[0]\n",
    "\n",
    "        # Tensors we want to evaluate\n",
    "        predictions = graph.get_operation_by_name(\"output/predictions\").outputs[0]\n",
    "\n",
    "        # Generate batches for one epoch\n",
    "        batches = data_helpers.batch_iter(list(x_test), FLAGS.batch_size, 1, shuffle=False)\n",
    "\n",
    "        # Collect the predictions here\n",
    "        all_predictions = []\n",
    "\n",
    "        for x_test_batch in batches:\n",
    "            batch_predictions = sess.run(predictions, {input_x: x_test_batch, dropout_keep_prob: 1.0})\n",
    "            all_predictions = np.concatenate([all_predictions, batch_predictions])\n",
    "\n",
    "# Print accuracy if y_test is defined\n",
    "if y_test is not None:\n",
    "    correct_predictions = float(sum(all_predictions == y_test))\n",
    "    print(\"Total number of test examples: {}\".format(len(y_test)))\n",
    "    print(\"Accuracy: {:g}\".format(correct_predictions/float(len(y_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "column_stack() takes 1 positional argument but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-4c19159d9f8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Save the evaluation to a csv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'prediction'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'label'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpredictions_human_readable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mout_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"..\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"prediction.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Saving evaluation to {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: column_stack() takes 1 positional argument but 3 were given"
     ]
    }
   ],
   "source": [
    "# Save the evaluation to a csv\n",
    "title = np.column_stack(('text', 'prediction', 'label'))\n",
    "predictions_human_readable = np.column_stack((np.array(x_raw), all_predictions, y_test))\n",
    "out_path = os.path.join(FLAGS.checkpoint_dir, \"..\", \"prediction.csv\")\n",
    "print(\"Saving evaluation to {0}\".format(out_path))\n",
    "\n",
    "with open(out_path, 'w') as f:\n",
    "    csv.writer(f).writerows(title)\n",
    "    csv.writer(f).writerows(predictions_human_readable)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
