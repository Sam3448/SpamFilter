{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import data_helpers\n",
    "from text_cnn import TextCNN\n",
    "from tensorflow.contrib import learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = 'sms'\n",
    "trainingSection = 9\n",
    "\n",
    "trainDataDir = '/Users/SamZhang/Documents/Capstone/dataset/' + dataset + '/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading params\n",
    "tf.flags.DEFINE_float(\"dev_sample_percentage\", .2, \"Percentage of the training data to use for validation\")\n",
    "tf.flags.DEFINE_string(\"positive_data_file\", trainDataDir + \"/spam/\" + str(trainingSection) + '/' + dataset + \"_train.spam\", \"Data source for the positive data.\")\n",
    "tf.flags.DEFINE_string(\"negative_data_file\", trainDataDir + \"/ham/\" + str(trainingSection) + '/' + dataset + \"_train.ham\", \"Data source for the negative data.\")\n",
    "\n",
    "# Model Hyperparameters\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 256, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.1, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 128, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 40, \"Number of training epochs (default: 200)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 50, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 50, \"Save model after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "BATCH_SIZE=128\n",
      "CHECKPOINT_EVERY=50\n",
      "DEV_SAMPLE_PERCENTAGE=0.2\n",
      "DROPOUT_KEEP_PROB=0.5\n",
      "EMBEDDING_DIM=128\n",
      "EVALUATE_EVERY=50\n",
      "FILTER_SIZES=3,4,5\n",
      "L2_REG_LAMBDA=0.1\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "NEGATIVE_DATA_FILE=/Users/SamZhang/Documents/Capstone/dataset/sms/train/ham/1/sms_train.ham\n",
      "NUM_CHECKPOINTS=5\n",
      "NUM_EPOCHS=40\n",
      "NUM_FILTERS=256\n",
      "POSITIVE_DATA_FILE=/Users/SamZhang/Documents/Capstone/dataset/sms/train/spam/1/sms_train.spam\n",
      "\n"
     ]
    }
   ],
   "source": [
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def processData():\n",
    "    # Data Preparation\n",
    "    # ==================================================\n",
    "\n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    x_text, y = data_helpers.load_data_and_labels(FLAGS.positive_data_file, FLAGS.negative_data_file)\n",
    "    print(x_text[0])\n",
    "\n",
    "    # Build vocabulary\n",
    "    max_document_length = max([len(x.split(\" \")) for x in x_text])\n",
    "    vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "    x = np.array(list(vocab_processor.fit_transform(x_text)))\n",
    "\n",
    "    # Randomly shuffle data\n",
    "    np.random.seed(10)\n",
    "    shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "    x_shuffled = x[shuffle_indices]\n",
    "    y_shuffled = y[shuffle_indices]\n",
    "\n",
    "    # Split train/test set\n",
    "    # TODO: This is very crude, should use cross-validation\n",
    "    dev_sample_index = -1 * int(FLAGS.dev_sample_percentage * float(len(y)))\n",
    "    x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "    y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "\n",
    "    del x, y, x_shuffled, y_shuffled\n",
    "\n",
    "    print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
    "    print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))\n",
    "    \n",
    "    return x_train, y_train, x_dev, y_dev, vocab_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry question \\( std txt rate \\) t c 's apply 08452810075over18 's\n",
      "Vocabulary Size: 2319\n",
      "Train/Dev split: 447/111\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_dev, y_dev, vocab_processor = processData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to /Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel/1\n",
      "\n",
      "2018-03-24T23:37:18.855135: step 1, loss 1.43738, acc 0.679688\n",
      "2018-03-24T23:37:19.355093: step 2, loss 1.23517, acc 0.859375\n",
      "2018-03-24T23:37:19.892052: step 3, loss 1.88217, acc 0.835938\n",
      "2018-03-24T23:37:20.247510: step 4, loss 2.06262, acc 0.809524\n",
      "2018-03-24T23:37:20.782299: step 5, loss 1.21392, acc 0.882812\n",
      "2018-03-24T23:37:21.299592: step 6, loss 0.997419, acc 0.859375\n",
      "2018-03-24T23:37:21.786420: step 7, loss 0.697593, acc 0.84375\n",
      "2018-03-24T23:37:22.045849: step 8, loss 0.643734, acc 0.857143\n",
      "2018-03-24T23:37:22.554519: step 9, loss 0.810707, acc 0.796875\n",
      "2018-03-24T23:37:23.061821: step 10, loss 0.969857, acc 0.789062\n",
      "2018-03-24T23:37:23.566916: step 11, loss 0.762322, acc 0.835938\n",
      "2018-03-24T23:37:23.822542: step 12, loss 0.301284, acc 0.920635\n",
      "2018-03-24T23:37:24.323405: step 13, loss 0.436849, acc 0.90625\n",
      "2018-03-24T23:37:24.839657: step 14, loss 0.667774, acc 0.890625\n",
      "2018-03-24T23:37:25.357087: step 15, loss 0.641058, acc 0.90625\n",
      "2018-03-24T23:37:25.628790: step 16, loss 0.461369, acc 0.936508\n",
      "2018-03-24T23:37:26.157633: step 17, loss 0.649167, acc 0.914062\n",
      "2018-03-24T23:37:26.700028: step 18, loss 0.493927, acc 0.914062\n",
      "2018-03-24T23:37:27.246052: step 19, loss 0.740784, acc 0.875\n",
      "2018-03-24T23:37:27.529742: step 20, loss 0.407785, acc 0.936508\n",
      "2018-03-24T23:37:28.131583: step 21, loss 0.590471, acc 0.90625\n",
      "2018-03-24T23:37:28.708414: step 22, loss 0.450286, acc 0.921875\n",
      "2018-03-24T23:37:29.270645: step 23, loss 0.306461, acc 0.945312\n",
      "2018-03-24T23:37:29.564714: step 24, loss 0.349403, acc 0.952381\n",
      "2018-03-24T23:37:30.164506: step 25, loss 0.466958, acc 0.914062\n",
      "2018-03-24T23:37:30.783757: step 26, loss 0.336072, acc 0.9375\n",
      "2018-03-24T23:37:31.337877: step 27, loss 0.429742, acc 0.921875\n",
      "2018-03-24T23:37:31.684382: step 28, loss 0.254456, acc 0.968254\n",
      "2018-03-24T23:37:32.277697: step 29, loss 0.292061, acc 0.96875\n",
      "2018-03-24T23:37:32.823700: step 30, loss 0.290525, acc 0.960938\n",
      "2018-03-24T23:37:33.461433: step 31, loss 0.329522, acc 0.945312\n",
      "2018-03-24T23:37:33.823014: step 32, loss 0.252028, acc 0.984127\n",
      "2018-03-24T23:37:34.440917: step 33, loss 0.276283, acc 0.960938\n",
      "2018-03-24T23:37:35.015353: step 34, loss 0.289843, acc 0.976562\n",
      "2018-03-24T23:37:35.646431: step 35, loss 0.333229, acc 0.929688\n",
      "2018-03-24T23:37:35.996361: step 36, loss 0.274621, acc 0.968254\n",
      "2018-03-24T23:37:36.697037: step 37, loss 0.355828, acc 0.929688\n",
      "2018-03-24T23:37:37.311067: step 38, loss 0.261734, acc 0.984375\n",
      "2018-03-24T23:37:37.902985: step 39, loss 0.242634, acc 0.96875\n",
      "2018-03-24T23:37:38.201216: step 40, loss 0.325157, acc 0.952381\n",
      "2018-03-24T23:37:38.796009: step 41, loss 0.232973, acc 0.976562\n",
      "2018-03-24T23:37:39.394224: step 42, loss 0.274782, acc 0.984375\n",
      "2018-03-24T23:37:39.970753: step 43, loss 0.287268, acc 0.960938\n",
      "2018-03-24T23:37:40.276953: step 44, loss 0.307476, acc 0.968254\n",
      "2018-03-24T23:37:40.872832: step 45, loss 0.214071, acc 0.984375\n",
      "2018-03-24T23:37:41.503101: step 46, loss 0.2106, acc 0.992188\n",
      "2018-03-24T23:37:42.192540: step 47, loss 0.333648, acc 0.96875\n",
      "2018-03-24T23:37:42.552015: step 48, loss 0.200946, acc 1\n",
      "2018-03-24T23:37:43.217767: step 49, loss 0.203106, acc 1\n",
      "2018-03-24T23:37:43.911421: step 50, loss 0.232479, acc 0.976562\n",
      "\n",
      "Evaluation:\n",
      "2018-03-24T23:37:44.134109: step 50, loss 0.461572, acc 0.891892\n",
      "\n",
      "Saved model checkpoint to /Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel/1/checkpoints/model-50\n",
      "\n",
      "2018-03-24T23:37:44.971851: step 51, loss 0.273149, acc 0.976562\n",
      "2018-03-24T23:37:45.334528: step 52, loss 0.223593, acc 0.984127\n",
      "2018-03-24T23:37:45.975289: step 53, loss 0.198944, acc 0.992188\n",
      "2018-03-24T23:37:46.633282: step 54, loss 0.294844, acc 0.960938\n",
      "2018-03-24T23:37:47.280257: step 55, loss 0.187024, acc 1\n",
      "2018-03-24T23:37:47.649196: step 56, loss 0.22592, acc 0.968254\n",
      "2018-03-24T23:37:48.342695: step 57, loss 0.192887, acc 0.992188\n",
      "2018-03-24T23:37:49.091278: step 58, loss 0.209261, acc 0.984375\n",
      "2018-03-24T23:37:49.748256: step 59, loss 0.192474, acc 1\n",
      "2018-03-24T23:37:50.071651: step 60, loss 0.188424, acc 1\n",
      "2018-03-24T23:37:50.789471: step 61, loss 0.181577, acc 1\n",
      "2018-03-24T23:37:51.486733: step 62, loss 0.187825, acc 1\n",
      "2018-03-24T23:37:52.187988: step 63, loss 0.243825, acc 0.96875\n",
      "2018-03-24T23:37:52.579983: step 64, loss 0.185755, acc 1\n",
      "2018-03-24T23:37:53.295004: step 65, loss 0.2059, acc 0.992188\n",
      "2018-03-24T23:37:54.010539: step 66, loss 0.274086, acc 0.976562\n",
      "2018-03-24T23:37:54.708453: step 67, loss 0.197745, acc 0.992188\n",
      "2018-03-24T23:37:55.065363: step 68, loss 0.179923, acc 1\n",
      "2018-03-24T23:37:55.802106: step 69, loss 0.189124, acc 0.992188\n",
      "2018-03-24T23:37:56.571691: step 70, loss 0.18221, acc 1\n",
      "2018-03-24T23:37:57.307959: step 71, loss 0.186329, acc 1\n",
      "2018-03-24T23:37:57.631023: step 72, loss 0.183645, acc 1\n",
      "2018-03-24T23:37:58.342832: step 73, loss 0.180721, acc 1\n",
      "2018-03-24T23:37:59.062796: step 74, loss 0.182514, acc 1\n",
      "2018-03-24T23:37:59.784079: step 75, loss 0.262747, acc 0.976562\n",
      "2018-03-24T23:38:00.172768: step 76, loss 0.177519, acc 1\n",
      "2018-03-24T23:38:00.926063: step 77, loss 0.176426, acc 1\n",
      "2018-03-24T23:38:01.648160: step 78, loss 0.179788, acc 1\n",
      "2018-03-24T23:38:02.269138: step 79, loss 0.1843, acc 1\n",
      "2018-03-24T23:38:02.585868: step 80, loss 0.198637, acc 0.984127\n",
      "2018-03-24T23:38:03.187628: step 81, loss 0.18619, acc 0.992188\n",
      "2018-03-24T23:38:03.898201: step 82, loss 0.192037, acc 0.992188\n",
      "2018-03-24T23:38:04.590945: step 83, loss 0.185565, acc 1\n",
      "2018-03-24T23:38:04.922200: step 84, loss 0.326094, acc 0.968254\n",
      "2018-03-24T23:38:05.560290: step 85, loss 0.185, acc 0.992188\n",
      "2018-03-24T23:38:06.259969: step 86, loss 0.176417, acc 1\n",
      "2018-03-24T23:38:07.042410: step 87, loss 0.195755, acc 0.992188\n",
      "2018-03-24T23:38:07.432275: step 88, loss 0.250012, acc 0.952381\n",
      "2018-03-24T23:38:08.124705: step 89, loss 0.2038, acc 0.976562\n",
      "2018-03-24T23:38:08.842128: step 90, loss 0.206987, acc 0.992188\n",
      "2018-03-24T23:38:09.503110: step 91, loss 0.228676, acc 0.976562\n",
      "2018-03-24T23:38:09.836229: step 92, loss 0.186971, acc 0.984127\n",
      "2018-03-24T23:38:10.451682: step 93, loss 0.208831, acc 0.992188\n",
      "2018-03-24T23:38:11.064841: step 94, loss 0.198156, acc 0.992188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-24T23:38:11.676342: step 95, loss 0.17304, acc 1\n",
      "2018-03-24T23:38:12.001708: step 96, loss 0.172564, acc 1\n",
      "2018-03-24T23:38:12.607495: step 97, loss 0.202138, acc 0.984375\n",
      "2018-03-24T23:38:13.230533: step 98, loss 0.182131, acc 0.992188\n",
      "2018-03-24T23:38:13.817959: step 99, loss 0.198488, acc 0.984375\n",
      "2018-03-24T23:38:14.119422: step 100, loss 0.169627, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-03-24T23:38:14.296917: step 100, loss 0.383976, acc 0.918919\n",
      "\n",
      "Saved model checkpoint to /Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel/1/checkpoints/model-100\n",
      "\n",
      "2018-03-24T23:38:15.003162: step 101, loss 0.182183, acc 0.992188\n",
      "2018-03-24T23:38:15.569159: step 102, loss 0.191224, acc 0.984375\n",
      "2018-03-24T23:38:16.162722: step 103, loss 0.21582, acc 0.984375\n",
      "2018-03-24T23:38:16.445565: step 104, loss 0.17459, acc 1\n",
      "2018-03-24T23:38:17.028923: step 105, loss 0.196674, acc 0.984375\n",
      "2018-03-24T23:38:17.605087: step 106, loss 0.167013, acc 1\n",
      "2018-03-24T23:38:18.179565: step 107, loss 0.168634, acc 1\n",
      "2018-03-24T23:38:18.506260: step 108, loss 0.168094, acc 1\n",
      "2018-03-24T23:38:19.100294: step 109, loss 0.172162, acc 1\n",
      "2018-03-24T23:38:19.688537: step 110, loss 0.178152, acc 0.992188\n",
      "2018-03-24T23:38:20.284266: step 111, loss 0.169363, acc 1\n",
      "2018-03-24T23:38:20.585516: step 112, loss 0.17631, acc 1\n",
      "2018-03-24T23:38:21.204487: step 113, loss 0.191567, acc 0.984375\n",
      "2018-03-24T23:38:21.801637: step 114, loss 0.16872, acc 1\n",
      "2018-03-24T23:38:22.399154: step 115, loss 0.184875, acc 0.992188\n",
      "2018-03-24T23:38:22.727668: step 116, loss 0.170573, acc 1\n",
      "2018-03-24T23:38:23.343801: step 117, loss 0.174608, acc 0.992188\n",
      "2018-03-24T23:38:23.977220: step 118, loss 0.208867, acc 0.992188\n",
      "2018-03-24T23:38:24.568076: step 119, loss 0.165229, acc 1\n",
      "2018-03-24T23:38:24.888165: step 120, loss 0.189306, acc 0.984127\n",
      "2018-03-24T23:38:25.504075: step 121, loss 0.20309, acc 0.984375\n",
      "2018-03-24T23:38:26.111031: step 122, loss 0.165329, acc 1\n",
      "2018-03-24T23:38:26.704001: step 123, loss 0.164886, acc 1\n",
      "2018-03-24T23:38:27.028622: step 124, loss 0.176684, acc 1\n",
      "2018-03-24T23:38:27.665338: step 125, loss 0.167558, acc 1\n",
      "2018-03-24T23:38:28.230093: step 126, loss 0.185468, acc 0.992188\n",
      "2018-03-24T23:38:28.800884: step 127, loss 0.196777, acc 0.992188\n",
      "2018-03-24T23:38:29.091896: step 128, loss 0.226592, acc 0.968254\n",
      "2018-03-24T23:38:29.704651: step 129, loss 0.16593, acc 1\n",
      "2018-03-24T23:38:30.260556: step 130, loss 0.167841, acc 1\n",
      "2018-03-24T23:38:30.862496: step 131, loss 0.162923, acc 1\n",
      "2018-03-24T23:38:31.166385: step 132, loss 0.164625, acc 1\n",
      "2018-03-24T23:38:31.768190: step 133, loss 0.160988, acc 1\n",
      "2018-03-24T23:38:32.347583: step 134, loss 0.164133, acc 1\n",
      "2018-03-24T23:38:32.980968: step 135, loss 0.165115, acc 1\n",
      "2018-03-24T23:38:33.301542: step 136, loss 0.160693, acc 1\n",
      "2018-03-24T23:38:33.917138: step 137, loss 0.165954, acc 1\n",
      "2018-03-24T23:38:34.541646: step 138, loss 0.159255, acc 1\n",
      "2018-03-24T23:38:35.139153: step 139, loss 0.159196, acc 1\n",
      "2018-03-24T23:38:35.451805: step 140, loss 0.195231, acc 0.984127\n",
      "2018-03-24T23:38:36.069204: step 141, loss 0.159127, acc 1\n",
      "2018-03-24T23:38:36.657297: step 142, loss 0.180258, acc 0.992188\n",
      "2018-03-24T23:38:37.243765: step 143, loss 0.159595, acc 1\n",
      "2018-03-24T23:38:37.555562: step 144, loss 0.15484, acc 1\n",
      "2018-03-24T23:38:38.148369: step 145, loss 0.1638, acc 0.992188\n",
      "2018-03-24T23:38:38.758670: step 146, loss 0.153885, acc 1\n",
      "2018-03-24T23:38:39.355859: step 147, loss 0.157467, acc 1\n",
      "2018-03-24T23:38:39.663017: step 148, loss 0.158847, acc 1\n",
      "2018-03-24T23:38:40.264967: step 149, loss 0.177424, acc 0.984375\n",
      "2018-03-24T23:38:40.888432: step 150, loss 0.153473, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-03-24T23:38:41.061890: step 150, loss 0.435714, acc 0.900901\n",
      "\n",
      "Saved model checkpoint to /Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel/1/checkpoints/model-150\n",
      "\n",
      "2018-03-24T23:38:41.827334: step 151, loss 0.155117, acc 1\n",
      "2018-03-24T23:38:42.153918: step 152, loss 0.163231, acc 1\n",
      "2018-03-24T23:38:42.811724: step 153, loss 0.153275, acc 1\n",
      "2018-03-24T23:38:43.396973: step 154, loss 0.159722, acc 0.992188\n",
      "2018-03-24T23:38:44.018529: step 155, loss 0.152122, acc 1\n",
      "2018-03-24T23:38:44.322977: step 156, loss 0.174625, acc 0.984127\n",
      "2018-03-24T23:38:45.034444: step 157, loss 0.155428, acc 1\n",
      "2018-03-24T23:38:45.631866: step 158, loss 0.151665, acc 1\n",
      "2018-03-24T23:38:46.216939: step 159, loss 0.151566, acc 1\n",
      "2018-03-24T23:38:46.531852: step 160, loss 0.153363, acc 1\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "      log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            num_classes=y_train.shape[1],\n",
    "            vocab_size=len(vocab_processor.vocabulary_),\n",
    "            embedding_size=FLAGS.embedding_dim,\n",
    "            filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "            num_filters=FLAGS.num_filters,\n",
    "            l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "\n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", \"cnnmodel\", str(trainingSection)))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "\n",
    "        # Write vocabulary\n",
    "        vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, accuracy = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "\n",
    "        # Generate batches\n",
    "        batches = data_helpers.batch_iter(\n",
    "            list(zip(x_train, y_train)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % FLAGS.evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % FLAGS.checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testDataDir = \"/Users/SamZhang/Documents/Capstone/dataset/\" + dataset + \"/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.flags.DEFINE_string(\"positive_test_data_file\", testDataDir + \"/spam/\" + str(trainingSection) + '/' + dataset + \"_test.spam\", \"Data source for the positive data.\")\n",
    "tf.flags.DEFINE_string(\"negative_test_data_file\", testDataDir + \"/ham/\" + str(trainingSection) + '/' + dataset + \"_test.ham\", \"Data source for the negative data.\")\n",
    "\n",
    "# Eval Parameters\n",
    "tf.flags.DEFINE_string(\"checkpoint_dir\", \"/Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel/\" + str(trainingSection) + \"/checkpoints/\", \"Checkpoint directory from training run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "BATCH_SIZE=128\n",
      "CHECKPOINT_DIR=/Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel/1/checkpoints/\n",
      "CHECKPOINT_EVERY=50\n",
      "DEV_SAMPLE_PERCENTAGE=0.2\n",
      "DROPOUT_KEEP_PROB=0.5\n",
      "EMBEDDING_DIM=128\n",
      "EVALUATE_EVERY=50\n",
      "FILTER_SIZES=3,4,5\n",
      "L2_REG_LAMBDA=0.1\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "NEGATIVE_DATA_FILE=/Users/SamZhang/Documents/Capstone/dataset/sms/train/ham/1/sms_train.ham\n",
      "NEGATIVE_TEST_DATA_FILE=/Users/SamZhang/Documents/Capstone/dataset/sms/test/ham/1/sms_test.ham\n",
      "NUM_CHECKPOINTS=5\n",
      "NUM_EPOCHS=40\n",
      "NUM_FILTERS=256\n",
      "POSITIVE_DATA_FILE=/Users/SamZhang/Documents/Capstone/dataset/sms/train/spam/1/sms_train.spam\n",
      "POSITIVE_TEST_DATA_FILE=/Users/SamZhang/Documents/Capstone/dataset/sms/test/spam/1/sms_test.spam\n",
      "\n"
     ]
    }
   ],
   "source": [
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freemsg hey there darling it 's been 3 week 's now and no word back ! i 'd like some fun you up for it still \\? tb ok ! xxx std chgs to send , 1 50 to rcv 1\n"
     ]
    }
   ],
   "source": [
    "x_raw, y_test = data_helpers.load_data_and_labels(FLAGS.positive_test_data_file, FLAGS.negative_test_data_file)\n",
    "y_test = np.argmax(y_test, axis=1) #ham = 0, spam = 1\n",
    "print(x_raw[0], y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_path = os.path.join(FLAGS.checkpoint_dir, \"..\", \"vocab\")\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor.restore(vocab_path)\n",
    "x_test = np.array(list(vocab_processor.transform(x_raw)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint_file = tf.train.latest_checkpoint(FLAGS.checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel/1/checkpoints/model-150\n",
      "Total number of test examples: 5016\n",
      "Accuracy: 0.888357\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "      log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        # Load the saved meta graph and restore variables\n",
    "        saver = tf.train.import_meta_graph(\"{}.meta\".format(checkpoint_file))\n",
    "        saver.restore(sess, checkpoint_file)\n",
    "\n",
    "        # Get the placeholders from the graph by name\n",
    "        input_x = graph.get_operation_by_name(\"input_x\").outputs[0]\n",
    "        # input_y = graph.get_operation_by_name(\"input_y\").outputs[0]\n",
    "        dropout_keep_prob = graph.get_operation_by_name(\"dropout_keep_prob\").outputs[0]\n",
    "\n",
    "        # Tensors we want to evaluate\n",
    "        predictions = graph.get_operation_by_name(\"output/predictions\").outputs[0]\n",
    "\n",
    "        # Generate batches for one epoch\n",
    "        batches = data_helpers.batch_iter(list(x_test), FLAGS.batch_size, 1, shuffle=False)\n",
    "\n",
    "        # Collect the predictions here\n",
    "        all_predictions = []\n",
    "\n",
    "        for x_test_batch in batches:\n",
    "            batch_predictions = sess.run(predictions, {input_x: x_test_batch, dropout_keep_prob: 1.0})\n",
    "            all_predictions = np.concatenate([all_predictions, batch_predictions])\n",
    "\n",
    "# Print accuracy if y_test is defined\n",
    "if y_test is not None:\n",
    "    correct_predictions = float(sum(all_predictions == y_test))\n",
    "    print(\"Total number of test examples: {}\".format(len(y_test)))\n",
    "    print(\"Accuracy: {:g}\".format(correct_predictions/float(len(y_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving evaluation to /Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel/1/checkpoints/../prediction.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the evaluation to a csv\n",
    "title = np.column_stack(('text', 'prediction', 'label'))\n",
    "predictions_human_readable = np.column_stack((np.array(x_raw), all_predictions, y_test))\n",
    "out_path = os.path.join(FLAGS.checkpoint_dir, \"..\", \"prediction.csv\")\n",
    "print(\"Saving evaluation to {0}\".format(out_path))\n",
    "\n",
    "with open(out_path, 'w') as f:\n",
    "    csv.writer(f).writerows(title)\n",
    "    csv.writer(f).writerows(predictions_human_readable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation and Noise Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112 0 4344 560\n",
      "accuracy : 0.888 \n",
      "precision : 1.000 \n",
      "recall : 0.167 \n",
      "f1 : 0.286 \n",
      "\n",
      "111 0 4344 561\n",
      "accuracy : 0.888 \n",
      "precision : 1.000 \n",
      "recall : 0.165 \n",
      "f1 : 0.284 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import ES_interface as esi\n",
    "\n",
    "esi.metric(dataset + '_' + str(trainingSection), out_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
