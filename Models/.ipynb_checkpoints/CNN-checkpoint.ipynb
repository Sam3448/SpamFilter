{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import data_helpers\n",
    "from text_cnn import TextCNN\n",
    "from tensorflow.contrib import learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = 'ling'\n",
    "trainDataDir = '/Users/SamZhang/Documents/Capstone/dataset/' + dataset + '/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading params\n",
    "tf.flags.DEFINE_float(\"dev_sample_percentage\", .2, \"Percentage of the training data to use for validation\")\n",
    "tf.flags.DEFINE_string(\"positive_data_file\", trainDataDir + \"/spam/\" + dataset + \"_train.spam\", \"Data source for the positive data.\")\n",
    "tf.flags.DEFINE_string(\"negative_data_file\", trainDataDir + \"/ham/\" + dataset + \"_train.ham\", \"Data source for the negative data.\")\n",
    "\n",
    "# Model Hyperparameters\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 256, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.1, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 128, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 10, \"Number of training epochs (default: 200)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "BATCH_SIZE=128\n",
      "CHECKPOINT_EVERY=100\n",
      "DEV_SAMPLE_PERCENTAGE=0.2\n",
      "DROPOUT_KEEP_PROB=0.5\n",
      "EMBEDDING_DIM=128\n",
      "EVALUATE_EVERY=100\n",
      "FILTER_SIZES=3,4,5\n",
      "L2_REG_LAMBDA=0.1\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "NEGATIVE_DATA_FILE=/Users/SamZhang/Documents/Capstone/dataset/ling/train/ham/ling_train.ham\n",
      "NUM_CHECKPOINTS=5\n",
      "NUM_EPOCHS=20\n",
      "NUM_FILTERS=256\n",
      "POSITIVE_DATA_FILE=/Users/SamZhang/Documents/Capstone/dataset/ling/train/spam/ling_train.spam\n",
      "\n"
     ]
    }
   ],
   "source": [
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def processData():\n",
    "    # Data Preparation\n",
    "    # ==================================================\n",
    "\n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    x_text, y = data_helpers.load_data_and_labels(FLAGS.positive_data_file, FLAGS.negative_data_file)\n",
    "    print(x_text[0])\n",
    "\n",
    "    # Build vocabulary\n",
    "    max_document_length = max([len(x.split(\" \")) for x in x_text])\n",
    "    vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "    x = np.array(list(vocab_processor.fit_transform(x_text)))\n",
    "\n",
    "    # Randomly shuffle data\n",
    "    np.random.seed(10)\n",
    "    shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "    x_shuffled = x[shuffle_indices]\n",
    "    y_shuffled = y[shuffle_indices]\n",
    "\n",
    "    # Split train/test set\n",
    "    # TODO: This is very crude, should use cross-validation\n",
    "    dev_sample_index = -1 * int(FLAGS.dev_sample_percentage * float(len(y)))\n",
    "    x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "    y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "\n",
    "    del x, y, x_shuffled, y_shuffled\n",
    "\n",
    "    print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
    "    print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))\n",
    "    \n",
    "    return x_train, y_train, x_dev, y_dev, vocab_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "great part time summer job ! display box credit application need place small owner operate store area here 1 introduce yourself store owner manager 2 our 90 effective script tell little display box save customer hundred dollar , draw card business , 5 0 15 0 every app send 3 spot counter , place box , nothing need , need name address company send commission check compensaation 10 every box place become representative earn commission 10 each application store course much profitable plan , pay month small effort call 1 888 703 5390 code 3 24 hours receive detail ! ! removed our mailing list , type b2998 hotmail com \\( \\) area \\( remove \\) subject area e mail send\n",
      "Vocabulary Size: 38556\n",
      "Train/Dev split: 1312/327\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_dev, y_dev, vocab_processor = processData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to /Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel\n",
      "\n",
      "2018-03-23T21:17:37.463146: step 1, loss 1.80779, acc 0.617188\n",
      "2018-03-23T21:17:55.066948: step 2, loss 2.21903, acc 0.671875\n",
      "2018-03-23T21:18:12.902975: step 3, loss 2.49038, acc 0.648438\n",
      "2018-03-23T21:18:29.403907: step 4, loss 1.7162, acc 0.640625\n",
      "2018-03-23T21:18:46.759254: step 5, loss 1.97431, acc 0.5625\n",
      "2018-03-23T21:19:03.047730: step 6, loss 2.00799, acc 0.585938\n",
      "2018-03-23T21:19:18.523413: step 7, loss 1.81705, acc 0.640625\n",
      "2018-03-23T21:19:34.230636: step 8, loss 1.99881, acc 0.710938\n",
      "2018-03-23T21:19:49.950724: step 9, loss 2.1808, acc 0.65625\n",
      "2018-03-23T21:20:06.550528: step 10, loss 1.91196, acc 0.734375\n",
      "2018-03-23T21:20:10.507718: step 11, loss 2.16872, acc 0.65625\n",
      "2018-03-23T21:20:26.612276: step 12, loss 1.60956, acc 0.757812\n",
      "2018-03-23T21:20:41.992911: step 13, loss 1.5736, acc 0.710938\n",
      "2018-03-23T21:20:58.050489: step 14, loss 1.35188, acc 0.78125\n",
      "2018-03-23T21:21:14.399757: step 15, loss 1.38412, acc 0.78125\n",
      "2018-03-23T21:21:30.974686: step 16, loss 1.63117, acc 0.671875\n",
      "2018-03-23T21:21:46.478226: step 17, loss 1.33607, acc 0.703125\n",
      "2018-03-23T21:22:02.730749: step 18, loss 1.40651, acc 0.71875\n",
      "2018-03-23T21:22:19.815616: step 19, loss 1.34685, acc 0.695312\n",
      "2018-03-23T21:22:36.858480: step 20, loss 1.22302, acc 0.765625\n",
      "2018-03-23T21:22:52.748053: step 21, loss 1.06715, acc 0.789062\n",
      "2018-03-23T21:22:56.746323: step 22, loss 1.4549, acc 0.71875\n",
      "2018-03-23T21:23:12.512794: step 23, loss 0.97167, acc 0.789062\n",
      "2018-03-23T21:23:28.894732: step 24, loss 1.04358, acc 0.78125\n",
      "2018-03-23T21:23:44.923920: step 25, loss 0.825259, acc 0.84375\n",
      "2018-03-23T21:24:00.482372: step 26, loss 0.836083, acc 0.8125\n",
      "2018-03-23T21:24:16.340376: step 27, loss 0.805287, acc 0.875\n",
      "2018-03-23T21:24:32.022853: step 28, loss 0.984749, acc 0.765625\n",
      "2018-03-23T21:24:48.215063: step 29, loss 0.868728, acc 0.804688\n",
      "2018-03-23T21:25:04.071171: step 30, loss 1.03995, acc 0.796875\n",
      "2018-03-23T21:25:19.916830: step 31, loss 1.18491, acc 0.804688\n",
      "2018-03-23T21:25:36.030695: step 32, loss 0.959242, acc 0.773438\n",
      "2018-03-23T21:25:40.140234: step 33, loss 0.925899, acc 0.8125\n",
      "2018-03-23T21:25:56.056281: step 34, loss 0.585322, acc 0.867188\n",
      "2018-03-23T21:26:11.775708: step 35, loss 0.558844, acc 0.882812\n",
      "2018-03-23T21:26:28.149161: step 36, loss 0.750043, acc 0.859375\n",
      "2018-03-23T21:26:44.936552: step 37, loss 0.824338, acc 0.796875\n",
      "2018-03-23T21:27:03.148765: step 38, loss 0.932991, acc 0.757812\n",
      "2018-03-23T21:27:19.045260: step 39, loss 0.622028, acc 0.875\n",
      "2018-03-23T21:27:34.966051: step 40, loss 0.753606, acc 0.804688\n",
      "2018-03-23T21:27:50.824523: step 41, loss 0.727761, acc 0.84375\n",
      "2018-03-23T21:28:06.388254: step 42, loss 0.604669, acc 0.867188\n",
      "2018-03-23T21:28:22.556673: step 43, loss 0.742541, acc 0.84375\n",
      "2018-03-23T21:28:26.692864: step 44, loss 1.06451, acc 0.8125\n",
      "2018-03-23T21:28:43.359942: step 45, loss 0.582164, acc 0.882812\n",
      "2018-03-23T21:28:59.201347: step 46, loss 0.642334, acc 0.867188\n",
      "2018-03-23T21:29:15.721299: step 47, loss 0.639517, acc 0.882812\n",
      "2018-03-23T21:29:32.478761: step 48, loss 0.531805, acc 0.851562\n",
      "2018-03-23T21:29:50.083325: step 49, loss 0.668677, acc 0.867188\n",
      "2018-03-23T21:30:09.110614: step 50, loss 0.776287, acc 0.8125\n",
      "2018-03-23T21:30:27.988341: step 51, loss 0.546804, acc 0.851562\n",
      "2018-03-23T21:30:46.864786: step 52, loss 0.7057, acc 0.875\n",
      "2018-03-23T21:31:04.469141: step 53, loss 0.63003, acc 0.835938\n",
      "2018-03-23T21:31:23.799037: step 54, loss 0.630209, acc 0.898438\n",
      "2018-03-23T21:31:28.447803: step 55, loss 0.670356, acc 0.84375\n",
      "2018-03-23T21:31:45.991527: step 56, loss 0.541146, acc 0.890625\n",
      "2018-03-23T21:32:03.189789: step 57, loss 0.434926, acc 0.898438\n",
      "2018-03-23T21:32:20.622602: step 58, loss 0.593949, acc 0.859375\n",
      "2018-03-23T21:32:38.955056: step 59, loss 0.555989, acc 0.890625\n",
      "2018-03-23T21:32:56.938804: step 60, loss 0.503101, acc 0.914062\n",
      "2018-03-23T21:33:14.268841: step 61, loss 0.603153, acc 0.882812\n",
      "2018-03-23T21:33:33.550596: step 62, loss 0.480081, acc 0.929688\n",
      "2018-03-23T21:33:52.750891: step 63, loss 0.339556, acc 0.953125\n",
      "2018-03-23T21:34:11.287649: step 64, loss 0.527132, acc 0.898438\n",
      "2018-03-23T21:34:28.956335: step 65, loss 0.519419, acc 0.914062\n",
      "2018-03-23T21:34:33.883276: step 66, loss 0.209942, acc 0.96875\n",
      "2018-03-23T21:34:51.632156: step 67, loss 0.653864, acc 0.898438\n",
      "2018-03-23T21:35:09.453985: step 68, loss 0.522598, acc 0.90625\n",
      "2018-03-23T21:35:27.254401: step 69, loss 0.358624, acc 0.945312\n",
      "2018-03-23T21:35:44.377971: step 70, loss 0.522095, acc 0.875\n",
      "2018-03-23T21:36:01.849140: step 71, loss 0.445122, acc 0.921875\n",
      "2018-03-23T21:36:19.366358: step 72, loss 0.341386, acc 0.914062\n",
      "2018-03-23T21:36:36.084450: step 73, loss 0.422667, acc 0.914062\n",
      "2018-03-23T21:36:52.904882: step 74, loss 0.528395, acc 0.882812\n",
      "2018-03-23T21:37:12.251279: step 75, loss 0.505356, acc 0.890625\n",
      "2018-03-23T21:37:31.816353: step 76, loss 0.382101, acc 0.945312\n",
      "2018-03-23T21:37:36.089554: step 77, loss 0.306337, acc 0.90625\n",
      "2018-03-23T21:37:54.489173: step 78, loss 0.408281, acc 0.914062\n",
      "2018-03-23T21:38:12.533837: step 79, loss 0.485004, acc 0.882812\n",
      "2018-03-23T21:38:30.149997: step 80, loss 0.253575, acc 0.96875\n",
      "2018-03-23T21:38:47.368131: step 81, loss 0.31523, acc 0.953125\n",
      "2018-03-23T21:39:05.587128: step 82, loss 0.280682, acc 0.953125\n",
      "2018-03-23T21:39:23.610983: step 83, loss 0.286657, acc 0.96875\n",
      "2018-03-23T21:39:41.019541: step 84, loss 0.242834, acc 0.96875\n",
      "2018-03-23T21:39:58.652815: step 85, loss 0.365164, acc 0.953125\n",
      "2018-03-23T21:40:17.650671: step 86, loss 0.40992, acc 0.921875\n",
      "2018-03-23T21:40:37.469248: step 87, loss 0.435042, acc 0.929688\n",
      "2018-03-23T21:40:42.162609: step 88, loss 0.323539, acc 0.9375\n",
      "2018-03-23T21:41:02.570803: step 89, loss 0.263886, acc 0.96875\n",
      "2018-03-23T21:41:20.989576: step 90, loss 0.354934, acc 0.945312\n",
      "2018-03-23T21:41:38.507706: step 91, loss 0.353957, acc 0.945312\n",
      "2018-03-23T21:41:55.816505: step 92, loss 0.449549, acc 0.898438\n",
      "2018-03-23T21:42:12.816296: step 93, loss 0.209212, acc 0.976562\n",
      "2018-03-23T21:42:31.653243: step 94, loss 0.324852, acc 0.9375\n",
      "2018-03-23T21:42:50.310867: step 95, loss 0.285155, acc 0.953125\n",
      "2018-03-23T21:43:08.674293: step 96, loss 0.349339, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-23T21:43:28.190493: step 97, loss 0.280208, acc 0.976562\n",
      "2018-03-23T21:43:46.476792: step 98, loss 0.407327, acc 0.921875\n",
      "2018-03-23T21:43:51.274205: step 99, loss 0.187846, acc 1\n",
      "2018-03-23T21:44:09.938397: step 100, loss 0.288667, acc 0.960938\n",
      "\n",
      "Evaluation:\n",
      "2018-03-23T21:44:28.288742: step 100, loss 0.264947, acc 0.95107\n",
      "\n",
      "Saved model checkpoint to /Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel/checkpoints/model-100\n",
      "\n",
      "2018-03-23T21:44:47.062839: step 101, loss 0.292106, acc 0.9375\n",
      "2018-03-23T21:45:05.010181: step 102, loss 0.293564, acc 0.96875\n",
      "2018-03-23T21:45:22.578455: step 103, loss 0.25475, acc 0.984375\n",
      "2018-03-23T21:45:40.281813: step 104, loss 0.300933, acc 0.976562\n",
      "2018-03-23T21:45:57.674952: step 105, loss 0.380933, acc 0.945312\n",
      "2018-03-23T21:46:14.820264: step 106, loss 0.222981, acc 0.976562\n",
      "2018-03-23T21:46:32.065160: step 107, loss 0.348977, acc 0.953125\n",
      "2018-03-23T21:46:48.965546: step 108, loss 0.307084, acc 0.945312\n",
      "2018-03-23T21:47:05.750595: step 109, loss 0.251579, acc 0.96875\n",
      "2018-03-23T21:47:09.956445: step 110, loss 0.274522, acc 0.96875\n",
      "2018-03-23T21:47:26.572100: step 111, loss 0.226759, acc 0.976562\n",
      "2018-03-23T21:47:43.087503: step 112, loss 0.233028, acc 0.984375\n",
      "2018-03-23T21:47:59.676644: step 113, loss 0.262176, acc 0.953125\n",
      "2018-03-23T21:48:16.203792: step 114, loss 0.208731, acc 0.976562\n",
      "2018-03-23T21:48:34.177381: step 115, loss 0.279365, acc 0.96875\n",
      "2018-03-23T21:48:53.530460: step 116, loss 0.200288, acc 0.976562\n",
      "2018-03-23T21:49:12.600393: step 117, loss 0.351674, acc 0.960938\n",
      "2018-03-23T21:49:31.043312: step 118, loss 0.344711, acc 0.9375\n",
      "2018-03-23T21:49:49.116945: step 119, loss 0.33981, acc 0.953125\n",
      "2018-03-23T21:50:06.776823: step 120, loss 0.257751, acc 0.976562\n",
      "2018-03-23T21:50:11.388111: step 121, loss 0.179387, acc 1\n",
      "2018-03-23T21:50:30.154251: step 122, loss 0.242092, acc 0.960938\n",
      "2018-03-23T21:50:48.473857: step 123, loss 0.2379, acc 0.96875\n",
      "2018-03-23T21:51:06.276749: step 124, loss 0.302645, acc 0.953125\n",
      "2018-03-23T21:51:24.172622: step 125, loss 0.275305, acc 0.9375\n",
      "2018-03-23T21:51:41.267816: step 126, loss 0.221125, acc 0.960938\n",
      "2018-03-23T21:51:57.907915: step 127, loss 0.262756, acc 0.976562\n",
      "2018-03-23T21:52:15.080706: step 128, loss 0.219353, acc 0.984375\n",
      "2018-03-23T21:52:32.253349: step 129, loss 0.274347, acc 0.96875\n",
      "2018-03-23T21:52:49.315317: step 130, loss 0.189164, acc 0.992188\n",
      "2018-03-23T21:53:06.032932: step 131, loss 0.24109, acc 0.976562\n",
      "2018-03-23T21:53:10.233062: step 132, loss 0.269249, acc 0.96875\n",
      "2018-03-23T21:53:27.424746: step 133, loss 0.237052, acc 0.96875\n",
      "2018-03-23T21:53:44.777825: step 134, loss 0.33627, acc 0.976562\n",
      "2018-03-23T21:54:01.901178: step 135, loss 0.264176, acc 0.96875\n",
      "2018-03-23T21:54:20.490581: step 136, loss 0.207169, acc 0.984375\n",
      "2018-03-23T21:54:38.954486: step 137, loss 0.17488, acc 0.992188\n",
      "2018-03-23T21:54:56.272797: step 138, loss 0.251669, acc 0.976562\n",
      "2018-03-23T21:55:13.342980: step 139, loss 0.193314, acc 0.984375\n",
      "2018-03-23T21:55:30.312178: step 140, loss 0.292929, acc 0.945312\n",
      "2018-03-23T21:55:46.834254: step 141, loss 0.239162, acc 0.960938\n",
      "2018-03-23T21:56:03.369823: step 142, loss 0.25064, acc 0.945312\n",
      "2018-03-23T21:56:07.933514: step 143, loss 0.307137, acc 0.96875\n",
      "2018-03-23T21:56:26.064463: step 144, loss 0.241895, acc 0.953125\n",
      "2018-03-23T21:56:43.024070: step 145, loss 0.282954, acc 0.96875\n",
      "2018-03-23T21:56:59.620033: step 146, loss 0.249176, acc 0.96875\n",
      "2018-03-23T21:57:16.316443: step 147, loss 0.205739, acc 0.976562\n",
      "2018-03-23T21:57:32.919883: step 148, loss 0.236666, acc 0.976562\n",
      "2018-03-23T21:57:49.516591: step 149, loss 0.178544, acc 0.984375\n",
      "2018-03-23T21:58:06.227297: step 150, loss 0.314119, acc 0.953125\n",
      "2018-03-23T21:58:22.887942: step 151, loss 0.273941, acc 0.960938\n",
      "2018-03-23T21:58:39.495368: step 152, loss 0.238126, acc 0.960938\n",
      "2018-03-23T21:58:56.150292: step 153, loss 0.198828, acc 0.984375\n",
      "2018-03-23T21:59:00.348881: step 154, loss 0.212423, acc 0.96875\n",
      "2018-03-23T21:59:17.367916: step 155, loss 0.166704, acc 1\n",
      "2018-03-23T21:59:34.670740: step 156, loss 0.243853, acc 0.976562\n",
      "2018-03-23T21:59:52.160729: step 157, loss 0.265279, acc 0.960938\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "      log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            num_classes=y_train.shape[1],\n",
    "            vocab_size=len(vocab_processor.vocabulary_),\n",
    "            embedding_size=FLAGS.embedding_dim,\n",
    "            filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "            num_filters=FLAGS.num_filters,\n",
    "            l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "\n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", \"cnnmodel\"))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "\n",
    "        # Write vocabulary\n",
    "        vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, accuracy = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "\n",
    "        # Generate batches\n",
    "        batches = data_helpers.batch_iter(\n",
    "            list(zip(x_train, y_train)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % FLAGS.evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % FLAGS.checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testDataDir = \"/Users/SamZhang/Documents/Capstone/dataset/\" + dataset + \"/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.flags.DEFINE_string(\"positive_test_data_file\", testDataDir + \"/spam/\" + dataset + \"_test.spam\", \"Data source for the positive data.\")\n",
    "tf.flags.DEFINE_string(\"negative_test_data_file\", testDataDir + \"/ham/\" + dataset + \"_test.ham\", \"Data source for the negative data.\")\n",
    "\n",
    "# Eval Parameters\n",
    "tf.flags.DEFINE_string(\"checkpoint_dir\", \"/Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel/checkpoints/\", \"Checkpoint directory from training run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "BATCH_SIZE=128\n",
      "CHECKPOINT_DIR=/Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel/checkpoints/\n",
      "CHECKPOINT_EVERY=100\n",
      "DEV_SAMPLE_PERCENTAGE=0.2\n",
      "DROPOUT_KEEP_PROB=0.5\n",
      "EMBEDDING_DIM=128\n",
      "EVALUATE_EVERY=100\n",
      "FILTER_SIZES=3,4,5\n",
      "L2_REG_LAMBDA=0.1\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "NEGATIVE_DATA_FILE=/Users/SamZhang/Documents/Capstone/dataset/small/train/ham/SMS_train.ham\n",
      "NEGATIVE_TEST_DATA_FILE=/Users/SamZhang/Documents/Capstone/dataset/small/test/ham/SMS_test.ham\n",
      "NUM_CHECKPOINTS=5\n",
      "NUM_EPOCHS=20\n",
      "NUM_FILTERS=256\n",
      "POSITIVE_DATA_FILE=/Users/SamZhang/Documents/Capstone/dataset/small/train/spam/SMS_train.spam\n",
      "POSITIVE_TEST_DATA_FILE=/Users/SamZhang/Documents/Capstone/dataset/small/test/spam/SMS_test.spam\n",
      "\n"
     ]
    }
   ],
   "source": [
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07732584351 rodger burns msg we tried to call you re your reply to our sms for a free nokia mobile free camcorder please call now 08000930705 for delivery tomorrow 1\n"
     ]
    }
   ],
   "source": [
    "x_raw, y_test = data_helpers.load_data_and_labels(FLAGS.positive_test_data_file, FLAGS.negative_test_data_file)\n",
    "y_test = np.argmax(y_test, axis=1) #ham = 0, spam = 1\n",
    "print(x_raw[0], y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_path = os.path.join(FLAGS.checkpoint_dir, \"..\", \"vocab\")\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor.restore(vocab_path)\n",
    "x_test = np.array(list(vocab_processor.transform(x_raw)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint_file = tf.train.latest_checkpoint(FLAGS.checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel/checkpoints/model-400\n",
      "Total number of test examples: 2487\n",
      "Accuracy: 0.990752\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "      log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        # Load the saved meta graph and restore variables\n",
    "        saver = tf.train.import_meta_graph(\"{}.meta\".format(checkpoint_file))\n",
    "        saver.restore(sess, checkpoint_file)\n",
    "\n",
    "        # Get the placeholders from the graph by name\n",
    "        input_x = graph.get_operation_by_name(\"input_x\").outputs[0]\n",
    "        # input_y = graph.get_operation_by_name(\"input_y\").outputs[0]\n",
    "        dropout_keep_prob = graph.get_operation_by_name(\"dropout_keep_prob\").outputs[0]\n",
    "\n",
    "        # Tensors we want to evaluate\n",
    "        predictions = graph.get_operation_by_name(\"output/predictions\").outputs[0]\n",
    "\n",
    "        # Generate batches for one epoch\n",
    "        batches = data_helpers.batch_iter(list(x_test), FLAGS.batch_size, 1, shuffle=False)\n",
    "\n",
    "        # Collect the predictions here\n",
    "        all_predictions = []\n",
    "\n",
    "        for x_test_batch in batches:\n",
    "            batch_predictions = sess.run(predictions, {input_x: x_test_batch, dropout_keep_prob: 1.0})\n",
    "            all_predictions = np.concatenate([all_predictions, batch_predictions])\n",
    "\n",
    "# Print accuracy if y_test is defined\n",
    "if y_test is not None:\n",
    "    correct_predictions = float(sum(all_predictions == y_test))\n",
    "    print(\"Total number of test examples: {}\".format(len(y_test)))\n",
    "    print(\"Accuracy: {:g}\".format(correct_predictions/float(len(y_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving evaluation to /Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel/checkpoints/../prediction.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the evaluation to a csv\n",
    "title = np.column_stack(('text', 'prediction', 'label'))\n",
    "predictions_human_readable = np.column_stack((np.array(x_raw), all_predictions, y_test))\n",
    "out_path = os.path.join(FLAGS.checkpoint_dir, \"..\", \"prediction.csv\")\n",
    "print(\"Saving evaluation to {0}\".format(out_path))\n",
    "\n",
    "with open(out_path, 'w') as f:\n",
    "    csv.writer(f).writerows(title)\n",
    "    csv.writer(f).writerows(predictions_human_readable)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
