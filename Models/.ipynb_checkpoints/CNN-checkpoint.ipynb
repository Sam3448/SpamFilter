{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import data_helpers\n",
    "from text_cnn import TextCNN\n",
    "from tensorflow.contrib import learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainDataDir = \"/Users/SamZhang/Documents/Capstone/dataset/small/train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading params\n",
    "tf.flags.DEFINE_float(\"dev_sample_percentage\", .2, \"Percentage of the training data to use for validation\")\n",
    "tf.flags.DEFINE_string(\"positive_data_file\", trainDataDir + \"/ham/SMS_train.ham\", \"Data source for the positive data.\")\n",
    "tf.flags.DEFINE_string(\"negative_data_file\", trainDataDir + \"/spam/SMS_train.spam\", \"Data source for the negative data.\")\n",
    "\n",
    "# Model Hyperparameters\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 256, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.1, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 128, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 20, \"Number of training epochs (default: 200)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "BATCH_SIZE=128\n",
      "CHECKPOINT_EVERY=100\n",
      "DEV_SAMPLE_PERCENTAGE=0.2\n",
      "DROPOUT_KEEP_PROB=0.5\n",
      "EMBEDDING_DIM=128\n",
      "EVALUATE_EVERY=100\n",
      "FILTER_SIZES=3,4,5\n",
      "L2_REG_LAMBDA=0.1\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "NEGATIVE_DATA_FILE=/Users/SamZhang/Documents/Capstone/dataset/small/train/spam/SMS_train.spam\n",
      "NUM_CHECKPOINTS=5\n",
      "NUM_EPOCHS=20\n",
      "NUM_FILTERS=256\n",
      "POSITIVE_DATA_FILE=/Users/SamZhang/Documents/Capstone/dataset/small/train/ham/SMS_train.ham\n",
      "\n"
     ]
    }
   ],
   "source": [
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def processData():\n",
    "    # Data Preparation\n",
    "    # ==================================================\n",
    "\n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    x_text, y = data_helpers.load_data_and_labels(FLAGS.positive_data_file, FLAGS.negative_data_file)\n",
    "    print(x_text[0])\n",
    "\n",
    "    # Build vocabulary\n",
    "    max_document_length = max([len(x.split(\" \")) for x in x_text])\n",
    "    vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "    x = np.array(list(vocab_processor.fit_transform(x_text)))\n",
    "\n",
    "    # Randomly shuffle data\n",
    "    np.random.seed(10)\n",
    "    shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "    x_shuffled = x[shuffle_indices]\n",
    "    y_shuffled = y[shuffle_indices]\n",
    "\n",
    "    # Split train/test set\n",
    "    # TODO: This is very crude, should use cross-validation\n",
    "    dev_sample_index = -1 * int(FLAGS.dev_sample_percentage * float(len(y)))\n",
    "    x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "    y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "\n",
    "    del x, y, x_shuffled, y_shuffled\n",
    "\n",
    "    print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
    "    print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))\n",
    "    \n",
    "    return x_train, y_train, x_dev, y_dev, vocab_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "go until jurong point , crazy available only in bugis n great world la e buffet cine there got amore wat\n",
      "Vocabulary Size: 7814\n",
      "Train/Dev split: 3568/892\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_dev, y_dev, vocab_processor = processData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to /Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel\n",
      "\n",
      "2018-03-14T00:30:55.111628: step 1, loss 2.97933, acc 0.429688\n",
      "2018-03-14T00:30:55.926951: step 2, loss 1.04439, acc 0.75\n",
      "2018-03-14T00:30:56.722527: step 3, loss 1.12703, acc 0.84375\n",
      "2018-03-14T00:30:57.527369: step 4, loss 2.04415, acc 0.820312\n",
      "2018-03-14T00:30:58.320835: step 5, loss 1.54365, acc 0.882812\n",
      "2018-03-14T00:30:59.138151: step 6, loss 1.58763, acc 0.84375\n",
      "2018-03-14T00:30:59.954256: step 7, loss 1.5108, acc 0.882812\n",
      "2018-03-14T00:31:00.798254: step 8, loss 1.53613, acc 0.84375\n",
      "2018-03-14T00:31:01.641976: step 9, loss 1.02943, acc 0.890625\n",
      "2018-03-14T00:31:02.508858: step 10, loss 0.551608, acc 0.914062\n",
      "2018-03-14T00:31:03.441712: step 11, loss 0.907921, acc 0.828125\n",
      "2018-03-14T00:31:04.374509: step 12, loss 0.721142, acc 0.820312\n",
      "2018-03-14T00:31:05.302378: step 13, loss 0.886713, acc 0.8125\n",
      "2018-03-14T00:31:06.223134: step 14, loss 1.13898, acc 0.742188\n",
      "2018-03-14T00:31:07.154160: step 15, loss 0.974022, acc 0.765625\n",
      "2018-03-14T00:31:08.075892: step 16, loss 0.970513, acc 0.78125\n",
      "2018-03-14T00:31:08.991350: step 17, loss 0.919341, acc 0.796875\n",
      "2018-03-14T00:31:09.922212: step 18, loss 0.756659, acc 0.867188\n",
      "2018-03-14T00:31:10.839759: step 19, loss 0.995288, acc 0.835938\n",
      "2018-03-14T00:31:11.779510: step 20, loss 0.737608, acc 0.882812\n",
      "2018-03-14T00:31:12.726683: step 21, loss 0.406143, acc 0.921875\n",
      "2018-03-14T00:31:13.689632: step 22, loss 0.689396, acc 0.867188\n",
      "2018-03-14T00:31:14.638106: step 23, loss 0.769137, acc 0.898438\n",
      "2018-03-14T00:31:15.600990: step 24, loss 0.805525, acc 0.8125\n",
      "2018-03-14T00:31:16.546149: step 25, loss 0.815615, acc 0.890625\n",
      "2018-03-14T00:31:17.492461: step 26, loss 0.824137, acc 0.851562\n",
      "2018-03-14T00:31:18.444178: step 27, loss 0.757568, acc 0.898438\n",
      "2018-03-14T00:31:19.313864: step 28, loss 0.645803, acc 0.883929\n",
      "2018-03-14T00:31:20.283973: step 29, loss 0.539766, acc 0.875\n",
      "2018-03-14T00:31:21.251194: step 30, loss 0.507948, acc 0.90625\n",
      "2018-03-14T00:31:22.206650: step 31, loss 0.49192, acc 0.921875\n",
      "2018-03-14T00:31:23.167769: step 32, loss 0.527502, acc 0.914062\n",
      "2018-03-14T00:31:24.171690: step 33, loss 0.551435, acc 0.890625\n",
      "2018-03-14T00:31:25.153512: step 34, loss 0.565251, acc 0.921875\n",
      "2018-03-14T00:31:26.138112: step 35, loss 0.701297, acc 0.875\n",
      "2018-03-14T00:31:27.117887: step 36, loss 0.743078, acc 0.875\n",
      "2018-03-14T00:31:28.102109: step 37, loss 0.657269, acc 0.882812\n",
      "2018-03-14T00:31:29.071713: step 38, loss 0.444573, acc 0.960938\n",
      "2018-03-14T00:31:30.040617: step 39, loss 0.476248, acc 0.898438\n",
      "2018-03-14T00:31:31.009989: step 40, loss 0.453584, acc 0.914062\n",
      "2018-03-14T00:31:31.982629: step 41, loss 0.405766, acc 0.9375\n",
      "2018-03-14T00:31:32.953146: step 42, loss 0.585902, acc 0.890625\n",
      "2018-03-14T00:31:33.921729: step 43, loss 0.504577, acc 0.90625\n",
      "2018-03-14T00:31:34.900764: step 44, loss 0.490113, acc 0.929688\n",
      "2018-03-14T00:31:35.884949: step 45, loss 0.424115, acc 0.914062\n",
      "2018-03-14T00:31:36.883705: step 46, loss 0.265553, acc 0.96875\n",
      "2018-03-14T00:31:37.864089: step 47, loss 0.408768, acc 0.945312\n",
      "2018-03-14T00:31:38.840076: step 48, loss 0.399507, acc 0.9375\n",
      "2018-03-14T00:31:39.827898: step 49, loss 0.456373, acc 0.898438\n",
      "2018-03-14T00:31:40.807751: step 50, loss 0.349149, acc 0.921875\n",
      "2018-03-14T00:31:41.791688: step 51, loss 0.463284, acc 0.90625\n",
      "2018-03-14T00:31:42.813080: step 52, loss 0.377346, acc 0.921875\n",
      "2018-03-14T00:31:43.840594: step 53, loss 0.325116, acc 0.960938\n",
      "2018-03-14T00:31:44.863527: step 54, loss 0.689559, acc 0.90625\n",
      "2018-03-14T00:31:45.912835: step 55, loss 0.396229, acc 0.960938\n",
      "2018-03-14T00:31:46.804685: step 56, loss 0.513081, acc 0.9375\n",
      "2018-03-14T00:31:47.814060: step 57, loss 0.447026, acc 0.9375\n",
      "2018-03-14T00:31:49.013425: step 58, loss 0.407246, acc 0.960938\n",
      "2018-03-14T00:31:50.029104: step 59, loss 0.358104, acc 0.9375\n",
      "2018-03-14T00:31:51.070297: step 60, loss 0.470128, acc 0.90625\n",
      "2018-03-14T00:31:52.097849: step 61, loss 0.455126, acc 0.929688\n",
      "2018-03-14T00:31:53.102044: step 62, loss 0.301126, acc 0.953125\n",
      "2018-03-14T00:31:54.110977: step 63, loss 0.467466, acc 0.929688\n",
      "2018-03-14T00:31:55.127926: step 64, loss 0.573752, acc 0.867188\n",
      "2018-03-14T00:31:56.142446: step 65, loss 0.41148, acc 0.945312\n",
      "2018-03-14T00:31:57.157134: step 66, loss 0.591249, acc 0.90625\n",
      "2018-03-14T00:31:58.182172: step 67, loss 0.345006, acc 0.96875\n",
      "2018-03-14T00:31:59.172682: step 68, loss 0.572273, acc 0.859375\n",
      "2018-03-14T00:32:00.167615: step 69, loss 0.45696, acc 0.898438\n",
      "2018-03-14T00:32:01.147461: step 70, loss 0.287837, acc 0.945312\n",
      "2018-03-14T00:32:02.132915: step 71, loss 0.454861, acc 0.914062\n",
      "2018-03-14T00:32:03.112615: step 72, loss 0.477957, acc 0.9375\n",
      "2018-03-14T00:32:04.093385: step 73, loss 0.458833, acc 0.945312\n",
      "2018-03-14T00:32:05.073437: step 74, loss 0.239462, acc 0.976562\n",
      "2018-03-14T00:32:06.063579: step 75, loss 0.368109, acc 0.9375\n",
      "2018-03-14T00:32:07.048963: step 76, loss 0.43987, acc 0.945312\n",
      "2018-03-14T00:32:07.999372: step 77, loss 0.292023, acc 0.960938\n",
      "2018-03-14T00:32:08.943265: step 78, loss 0.348512, acc 0.953125\n",
      "2018-03-14T00:32:09.894320: step 79, loss 0.234571, acc 0.976562\n",
      "2018-03-14T00:32:10.857028: step 80, loss 0.350782, acc 0.9375\n",
      "2018-03-14T00:32:11.832440: step 81, loss 0.282129, acc 0.960938\n",
      "2018-03-14T00:32:12.808313: step 82, loss 0.428534, acc 0.921875\n",
      "2018-03-14T00:32:13.796987: step 83, loss 0.53117, acc 0.9375\n",
      "2018-03-14T00:32:14.666129: step 84, loss 0.508292, acc 0.928571\n",
      "2018-03-14T00:32:15.669306: step 85, loss 0.257399, acc 0.953125\n",
      "2018-03-14T00:32:16.631280: step 86, loss 0.332944, acc 0.945312\n",
      "2018-03-14T00:32:17.581266: step 87, loss 0.363293, acc 0.953125\n",
      "2018-03-14T00:32:18.522940: step 88, loss 0.225824, acc 0.976562\n",
      "2018-03-14T00:32:19.472702: step 89, loss 0.258367, acc 0.976562\n",
      "2018-03-14T00:32:20.436932: step 90, loss 0.281642, acc 0.945312\n",
      "2018-03-14T00:32:21.404433: step 91, loss 0.360382, acc 0.953125\n",
      "2018-03-14T00:32:22.359564: step 92, loss 0.350124, acc 0.945312\n",
      "2018-03-14T00:32:23.308511: step 93, loss 0.228726, acc 0.992188\n",
      "2018-03-14T00:32:24.259911: step 94, loss 0.382498, acc 0.953125\n",
      "2018-03-14T00:32:25.205579: step 95, loss 0.306793, acc 0.9375\n",
      "2018-03-14T00:32:26.161241: step 96, loss 0.26963, acc 0.945312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-14T00:32:27.109770: step 97, loss 0.29227, acc 0.96875\n",
      "2018-03-14T00:32:28.049977: step 98, loss 0.239095, acc 0.96875\n",
      "2018-03-14T00:32:28.990825: step 99, loss 0.305023, acc 0.960938\n",
      "2018-03-14T00:32:29.943627: step 100, loss 0.225496, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2018-03-14T00:32:32.156879: step 100, loss 0.292407, acc 0.965247\n",
      "\n",
      "Saved model checkpoint to /Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel/checkpoints/model-100\n",
      "\n",
      "2018-03-14T00:32:33.210652: step 101, loss 0.302558, acc 0.953125\n",
      "2018-03-14T00:32:34.123287: step 102, loss 0.260448, acc 0.953125\n",
      "2018-03-14T00:32:35.040295: step 103, loss 0.316657, acc 0.953125\n",
      "2018-03-14T00:32:35.953376: step 104, loss 0.380362, acc 0.9375\n",
      "2018-03-14T00:32:36.876896: step 105, loss 0.308211, acc 0.953125\n",
      "2018-03-14T00:32:37.800747: step 106, loss 0.53046, acc 0.914062\n",
      "2018-03-14T00:32:38.721098: step 107, loss 0.362547, acc 0.9375\n",
      "2018-03-14T00:32:39.652591: step 108, loss 0.201542, acc 0.984375\n",
      "2018-03-14T00:32:40.628726: step 109, loss 0.271104, acc 0.960938\n",
      "2018-03-14T00:32:41.618737: step 110, loss 0.264636, acc 0.960938\n",
      "2018-03-14T00:32:42.613840: step 111, loss 0.344666, acc 0.945312\n",
      "2018-03-14T00:32:43.463409: step 112, loss 0.287504, acc 0.964286\n",
      "2018-03-14T00:32:44.426433: step 113, loss 0.233923, acc 0.976562\n",
      "2018-03-14T00:32:45.384976: step 114, loss 0.299484, acc 0.945312\n",
      "2018-03-14T00:32:46.329781: step 115, loss 0.34936, acc 0.945312\n",
      "2018-03-14T00:32:47.276207: step 116, loss 0.28283, acc 0.953125\n",
      "2018-03-14T00:32:48.186772: step 117, loss 0.315768, acc 0.96875\n",
      "2018-03-14T00:32:49.103928: step 118, loss 0.341435, acc 0.953125\n",
      "2018-03-14T00:32:50.017099: step 119, loss 0.23775, acc 0.984375\n",
      "2018-03-14T00:32:50.930189: step 120, loss 0.20873, acc 0.976562\n",
      "2018-03-14T00:32:51.843762: step 121, loss 0.263569, acc 0.960938\n",
      "2018-03-14T00:32:52.771360: step 122, loss 0.190328, acc 0.992188\n",
      "2018-03-14T00:32:53.685939: step 123, loss 0.231099, acc 0.984375\n",
      "2018-03-14T00:32:54.617702: step 124, loss 0.254564, acc 0.960938\n",
      "2018-03-14T00:32:55.564279: step 125, loss 0.195673, acc 0.984375\n",
      "2018-03-14T00:32:56.511142: step 126, loss 0.206839, acc 0.992188\n",
      "2018-03-14T00:32:57.452949: step 127, loss 0.320912, acc 0.96875\n",
      "2018-03-14T00:32:58.395667: step 128, loss 0.218369, acc 0.992188\n",
      "2018-03-14T00:32:59.321988: step 129, loss 0.268311, acc 0.960938\n",
      "2018-03-14T00:33:00.242230: step 130, loss 0.243122, acc 0.96875\n",
      "2018-03-14T00:33:01.164580: step 131, loss 0.246027, acc 0.976562\n",
      "2018-03-14T00:33:02.078218: step 132, loss 0.193005, acc 0.984375\n",
      "2018-03-14T00:33:02.995845: step 133, loss 0.185796, acc 1\n",
      "2018-03-14T00:33:03.912447: step 134, loss 0.205294, acc 0.984375\n",
      "2018-03-14T00:33:04.831316: step 135, loss 0.190139, acc 0.984375\n",
      "2018-03-14T00:33:05.754612: step 136, loss 0.175782, acc 1\n",
      "2018-03-14T00:33:06.680245: step 137, loss 0.224694, acc 0.976562\n",
      "2018-03-14T00:33:07.610413: step 138, loss 0.253607, acc 0.96875\n",
      "2018-03-14T00:33:08.559712: step 139, loss 0.386254, acc 0.9375\n",
      "2018-03-14T00:33:09.398407: step 140, loss 0.279361, acc 0.955357\n",
      "2018-03-14T00:33:10.347444: step 141, loss 0.233409, acc 0.96875\n",
      "2018-03-14T00:33:11.297041: step 142, loss 0.316708, acc 0.9375\n",
      "2018-03-14T00:33:12.207446: step 143, loss 0.197621, acc 0.984375\n",
      "2018-03-14T00:33:13.119820: step 144, loss 0.307914, acc 0.96875\n",
      "2018-03-14T00:33:14.058428: step 145, loss 0.175857, acc 1\n",
      "2018-03-14T00:33:14.972260: step 146, loss 0.180747, acc 0.992188\n",
      "2018-03-14T00:33:15.899564: step 147, loss 0.190458, acc 0.992188\n",
      "2018-03-14T00:33:16.825864: step 148, loss 0.325439, acc 0.960938\n",
      "2018-03-14T00:33:17.778058: step 149, loss 0.18707, acc 0.992188\n",
      "2018-03-14T00:33:18.690593: step 150, loss 0.203069, acc 0.984375\n",
      "2018-03-14T00:33:19.629102: step 151, loss 0.241136, acc 0.976562\n",
      "2018-03-14T00:33:20.542138: step 152, loss 0.163073, acc 1\n",
      "2018-03-14T00:33:21.486191: step 153, loss 0.190696, acc 0.984375\n",
      "2018-03-14T00:33:22.430899: step 154, loss 0.221962, acc 0.976562\n",
      "2018-03-14T00:33:23.374109: step 155, loss 0.176993, acc 0.992188\n",
      "2018-03-14T00:33:24.320426: step 156, loss 0.174173, acc 0.992188\n",
      "2018-03-14T00:33:25.292464: step 157, loss 0.213885, acc 0.96875\n",
      "2018-03-14T00:33:26.207551: step 158, loss 0.252536, acc 0.976562\n",
      "2018-03-14T00:33:27.148333: step 159, loss 0.359307, acc 0.953125\n",
      "2018-03-14T00:33:28.074518: step 160, loss 0.205065, acc 0.96875\n",
      "2018-03-14T00:33:28.998930: step 161, loss 0.19111, acc 0.984375\n",
      "2018-03-14T00:33:29.910458: step 162, loss 0.306916, acc 0.96875\n",
      "2018-03-14T00:33:30.833588: step 163, loss 0.342712, acc 0.945312\n",
      "2018-03-14T00:33:31.749520: step 164, loss 0.175954, acc 0.992188\n",
      "2018-03-14T00:33:32.669212: step 165, loss 0.279708, acc 0.953125\n",
      "2018-03-14T00:33:33.617162: step 166, loss 0.222185, acc 0.992188\n",
      "2018-03-14T00:33:34.554898: step 167, loss 0.200454, acc 0.984375\n",
      "2018-03-14T00:33:35.388443: step 168, loss 0.267828, acc 0.955357\n",
      "2018-03-14T00:33:36.339676: step 169, loss 0.254515, acc 0.953125\n",
      "2018-03-14T00:33:37.275291: step 170, loss 0.165311, acc 1\n",
      "2018-03-14T00:33:38.192719: step 171, loss 0.179034, acc 0.984375\n",
      "2018-03-14T00:33:39.101131: step 172, loss 0.184204, acc 0.984375\n",
      "2018-03-14T00:33:40.022128: step 173, loss 0.232152, acc 0.96875\n",
      "2018-03-14T00:33:40.937897: step 174, loss 0.193213, acc 0.992188\n",
      "2018-03-14T00:33:41.860180: step 175, loss 0.245538, acc 0.945312\n",
      "2018-03-14T00:33:42.773912: step 176, loss 0.187196, acc 0.984375\n",
      "2018-03-14T00:33:43.698589: step 177, loss 0.198335, acc 0.96875\n",
      "2018-03-14T00:33:44.627091: step 178, loss 0.316762, acc 0.953125\n",
      "2018-03-14T00:33:45.591082: step 179, loss 0.188596, acc 0.976562\n",
      "2018-03-14T00:33:46.542040: step 180, loss 0.173006, acc 0.992188\n",
      "2018-03-14T00:33:47.495093: step 181, loss 0.19271, acc 0.976562\n",
      "2018-03-14T00:33:48.425424: step 182, loss 0.235498, acc 0.96875\n",
      "2018-03-14T00:33:49.345567: step 183, loss 0.163211, acc 0.992188\n",
      "2018-03-14T00:33:50.262202: step 184, loss 0.248362, acc 0.976562\n",
      "2018-03-14T00:33:51.178706: step 185, loss 0.249291, acc 0.976562\n",
      "2018-03-14T00:33:52.099778: step 186, loss 0.160696, acc 1\n",
      "2018-03-14T00:33:53.017956: step 187, loss 0.180071, acc 0.992188\n",
      "2018-03-14T00:33:53.934263: step 188, loss 0.162542, acc 0.992188\n",
      "2018-03-14T00:33:54.863563: step 189, loss 0.15645, acc 1\n",
      "2018-03-14T00:33:55.785961: step 190, loss 0.207089, acc 0.976562\n",
      "2018-03-14T00:33:56.709900: step 191, loss 0.295862, acc 0.953125\n",
      "2018-03-14T00:33:57.647879: step 192, loss 0.21874, acc 0.96875\n",
      "2018-03-14T00:33:58.601945: step 193, loss 0.213113, acc 0.96875\n",
      "2018-03-14T00:33:59.553232: step 194, loss 0.236338, acc 0.992188\n",
      "2018-03-14T00:34:00.513636: step 195, loss 0.153036, acc 1\n",
      "2018-03-14T00:34:01.344271: step 196, loss 0.177275, acc 0.982143\n",
      "2018-03-14T00:34:02.280543: step 197, loss 0.16643, acc 0.984375\n",
      "2018-03-14T00:34:03.194879: step 198, loss 0.210165, acc 0.984375\n",
      "2018-03-14T00:34:04.112329: step 199, loss 0.189782, acc 0.984375\n",
      "2018-03-14T00:34:05.028987: step 200, loss 0.197347, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2018-03-14T00:34:07.179007: step 200, loss 0.201741, acc 0.982063\n",
      "\n",
      "Saved model checkpoint to /Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel/checkpoints/model-200\n",
      "\n",
      "2018-03-14T00:34:08.221511: step 201, loss 0.154772, acc 1\n",
      "2018-03-14T00:34:09.151998: step 202, loss 0.163154, acc 0.992188\n",
      "2018-03-14T00:34:10.066628: step 203, loss 0.251196, acc 0.984375\n",
      "2018-03-14T00:34:10.992605: step 204, loss 0.168816, acc 0.992188\n",
      "2018-03-14T00:34:11.919061: step 205, loss 0.153928, acc 1\n",
      "2018-03-14T00:34:12.835458: step 206, loss 0.210113, acc 0.976562\n",
      "2018-03-14T00:34:13.763262: step 207, loss 0.20094, acc 0.992188\n",
      "2018-03-14T00:34:14.705751: step 208, loss 0.149246, acc 1\n",
      "2018-03-14T00:34:15.695972: step 209, loss 0.167445, acc 0.992188\n",
      "2018-03-14T00:34:16.644789: step 210, loss 0.183598, acc 0.984375\n",
      "2018-03-14T00:34:17.590383: step 211, loss 0.160017, acc 1\n",
      "2018-03-14T00:34:18.525334: step 212, loss 0.159274, acc 0.992188\n",
      "2018-03-14T00:34:19.447492: step 213, loss 0.216167, acc 0.96875\n",
      "2018-03-14T00:34:20.372709: step 214, loss 0.217532, acc 0.976562\n",
      "2018-03-14T00:34:21.291726: step 215, loss 0.206171, acc 0.976562\n",
      "2018-03-14T00:34:22.229781: step 216, loss 0.241673, acc 0.976562\n",
      "2018-03-14T00:34:23.151891: step 217, loss 0.18118, acc 0.984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-14T00:34:24.077790: step 218, loss 0.167554, acc 0.984375\n",
      "2018-03-14T00:34:25.009891: step 219, loss 0.186807, acc 0.984375\n",
      "2018-03-14T00:34:25.921547: step 220, loss 0.169105, acc 0.992188\n",
      "2018-03-14T00:34:26.847881: step 221, loss 0.147892, acc 1\n",
      "2018-03-14T00:34:27.795545: step 222, loss 0.170601, acc 0.984375\n",
      "2018-03-14T00:34:28.749632: step 223, loss 0.200008, acc 0.96875\n",
      "2018-03-14T00:34:29.576888: step 224, loss 0.185073, acc 0.964286\n",
      "2018-03-14T00:34:30.535774: step 225, loss 0.198051, acc 0.992188\n",
      "2018-03-14T00:34:31.457713: step 226, loss 0.16598, acc 0.984375\n",
      "2018-03-14T00:34:32.378336: step 227, loss 0.154002, acc 0.992188\n",
      "2018-03-14T00:34:33.298881: step 228, loss 0.173334, acc 0.992188\n",
      "2018-03-14T00:34:34.217850: step 229, loss 0.199145, acc 0.976562\n",
      "2018-03-14T00:34:35.133932: step 230, loss 0.184857, acc 0.984375\n",
      "2018-03-14T00:34:36.046623: step 231, loss 0.219331, acc 0.976562\n",
      "2018-03-14T00:34:36.971036: step 232, loss 0.232641, acc 0.976562\n",
      "2018-03-14T00:34:37.896005: step 233, loss 0.168764, acc 0.976562\n",
      "2018-03-14T00:34:38.847612: step 234, loss 0.158461, acc 1\n",
      "2018-03-14T00:34:39.795883: step 235, loss 0.176353, acc 0.992188\n",
      "2018-03-14T00:34:40.742233: step 236, loss 0.159638, acc 0.984375\n",
      "2018-03-14T00:34:41.683729: step 237, loss 0.200626, acc 0.976562\n",
      "2018-03-14T00:34:42.596511: step 238, loss 0.188384, acc 0.976562\n",
      "2018-03-14T00:34:43.528792: step 239, loss 0.16922, acc 0.984375\n",
      "2018-03-14T00:34:44.445288: step 240, loss 0.191482, acc 0.976562\n",
      "2018-03-14T00:34:45.367416: step 241, loss 0.22185, acc 0.984375\n",
      "2018-03-14T00:34:46.280975: step 242, loss 0.144593, acc 1\n",
      "2018-03-14T00:34:47.197754: step 243, loss 0.141448, acc 1\n",
      "2018-03-14T00:34:48.118895: step 244, loss 0.233411, acc 0.984375\n",
      "2018-03-14T00:34:49.035767: step 245, loss 0.141462, acc 1\n",
      "2018-03-14T00:34:49.969792: step 246, loss 0.183819, acc 0.992188\n",
      "2018-03-14T00:34:50.921083: step 247, loss 0.184664, acc 0.984375\n",
      "2018-03-14T00:34:51.874763: step 248, loss 0.171829, acc 0.96875\n",
      "2018-03-14T00:34:52.824133: step 249, loss 0.150346, acc 0.992188\n",
      "2018-03-14T00:34:53.775562: step 250, loss 0.155921, acc 0.992188\n",
      "2018-03-14T00:34:54.701383: step 251, loss 0.152788, acc 0.992188\n",
      "2018-03-14T00:34:55.506887: step 252, loss 0.180754, acc 0.973214\n",
      "2018-03-14T00:34:56.438279: step 253, loss 0.137429, acc 1\n",
      "2018-03-14T00:34:57.393263: step 254, loss 0.13969, acc 1\n",
      "2018-03-14T00:34:58.315076: step 255, loss 0.161774, acc 0.992188\n",
      "2018-03-14T00:34:59.242403: step 256, loss 0.215899, acc 0.976562\n",
      "2018-03-14T00:35:00.169026: step 257, loss 0.141384, acc 1\n",
      "2018-03-14T00:35:01.088881: step 258, loss 0.155276, acc 0.992188\n",
      "2018-03-14T00:35:02.003766: step 259, loss 0.146465, acc 0.992188\n",
      "2018-03-14T00:35:02.954787: step 260, loss 0.162933, acc 0.992188\n",
      "2018-03-14T00:35:03.903206: step 261, loss 0.181915, acc 0.976562\n",
      "2018-03-14T00:35:04.848544: step 262, loss 0.15118, acc 0.992188\n",
      "2018-03-14T00:35:05.802525: step 263, loss 0.171929, acc 0.984375\n",
      "2018-03-14T00:35:06.745164: step 264, loss 0.146364, acc 0.992188\n",
      "2018-03-14T00:35:07.663135: step 265, loss 0.148209, acc 0.992188\n",
      "2018-03-14T00:35:08.575741: step 266, loss 0.134914, acc 1\n",
      "2018-03-14T00:35:09.492424: step 267, loss 0.139284, acc 0.992188\n",
      "2018-03-14T00:35:10.405548: step 268, loss 0.184124, acc 0.976562\n",
      "2018-03-14T00:35:11.326978: step 269, loss 0.161545, acc 0.984375\n",
      "2018-03-14T00:35:12.267337: step 270, loss 0.164021, acc 0.984375\n",
      "2018-03-14T00:35:13.261646: step 271, loss 0.134569, acc 1\n",
      "2018-03-14T00:35:14.222276: step 272, loss 0.137664, acc 0.992188\n",
      "2018-03-14T00:35:15.186722: step 273, loss 0.186771, acc 0.984375\n",
      "2018-03-14T00:35:16.138359: step 274, loss 0.198673, acc 0.976562\n",
      "2018-03-14T00:35:17.085490: step 275, loss 0.144758, acc 0.992188\n",
      "2018-03-14T00:35:18.001611: step 276, loss 0.133659, acc 1\n",
      "2018-03-14T00:35:18.915040: step 277, loss 0.129867, acc 1\n",
      "2018-03-14T00:35:19.850150: step 278, loss 0.170639, acc 0.984375\n",
      "2018-03-14T00:35:20.769706: step 279, loss 0.136985, acc 0.992188\n",
      "2018-03-14T00:35:21.580059: step 280, loss 0.144808, acc 0.991071\n",
      "2018-03-14T00:35:22.526875: step 281, loss 0.139521, acc 0.992188\n",
      "2018-03-14T00:35:23.464511: step 282, loss 0.144763, acc 0.992188\n",
      "2018-03-14T00:35:24.407868: step 283, loss 0.137135, acc 0.992188\n",
      "2018-03-14T00:35:25.380228: step 284, loss 0.164645, acc 0.984375\n",
      "2018-03-14T00:35:26.441083: step 285, loss 0.144981, acc 0.992188\n",
      "2018-03-14T00:35:27.398199: step 286, loss 0.207534, acc 0.976562\n",
      "2018-03-14T00:35:28.342682: step 287, loss 0.143931, acc 0.992188\n",
      "2018-03-14T00:35:29.284901: step 288, loss 0.160471, acc 0.992188\n",
      "2018-03-14T00:35:30.206332: step 289, loss 0.131168, acc 1\n",
      "2018-03-14T00:35:31.139157: step 290, loss 0.146123, acc 0.984375\n",
      "2018-03-14T00:35:32.058972: step 291, loss 0.155515, acc 0.984375\n",
      "2018-03-14T00:35:32.971101: step 292, loss 0.211492, acc 0.96875\n",
      "2018-03-14T00:35:33.880812: step 293, loss 0.146788, acc 0.992188\n",
      "2018-03-14T00:35:34.803601: step 294, loss 0.140098, acc 0.992188\n",
      "2018-03-14T00:35:35.721960: step 295, loss 0.125427, acc 1\n",
      "2018-03-14T00:35:36.670852: step 296, loss 0.131255, acc 1\n",
      "2018-03-14T00:35:37.623190: step 297, loss 0.131044, acc 1\n",
      "2018-03-14T00:35:38.557071: step 298, loss 0.144803, acc 0.992188\n",
      "2018-03-14T00:35:39.501120: step 299, loss 0.148475, acc 0.992188\n",
      "2018-03-14T00:35:40.446509: step 300, loss 0.146227, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2018-03-14T00:35:42.438316: step 300, loss 0.168888, acc 0.984305\n",
      "\n",
      "Saved model checkpoint to /Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel/checkpoints/model-300\n",
      "\n",
      "2018-03-14T00:35:43.499838: step 301, loss 0.150014, acc 0.984375\n",
      "2018-03-14T00:35:44.418289: step 302, loss 0.120029, acc 1\n",
      "2018-03-14T00:35:45.347262: step 303, loss 0.121771, acc 1\n",
      "2018-03-14T00:35:46.263952: step 304, loss 0.128112, acc 1\n",
      "2018-03-14T00:35:47.186988: step 305, loss 0.138684, acc 0.992188\n",
      "2018-03-14T00:35:48.113611: step 306, loss 0.150452, acc 0.992188\n",
      "2018-03-14T00:35:49.064476: step 307, loss 0.125813, acc 1\n",
      "2018-03-14T00:35:49.896601: step 308, loss 0.133889, acc 0.991071\n",
      "2018-03-14T00:35:50.840551: step 309, loss 0.123125, acc 1\n",
      "2018-03-14T00:35:51.799566: step 310, loss 0.160905, acc 0.984375\n",
      "2018-03-14T00:35:52.740738: step 311, loss 0.13396, acc 0.992188\n",
      "2018-03-14T00:35:53.686665: step 312, loss 0.126192, acc 1\n",
      "2018-03-14T00:35:54.610696: step 313, loss 0.139861, acc 0.984375\n",
      "2018-03-14T00:35:55.538289: step 314, loss 0.141604, acc 0.992188\n",
      "2018-03-14T00:35:56.483459: step 315, loss 0.130821, acc 0.992188\n",
      "2018-03-14T00:35:57.411140: step 316, loss 0.16557, acc 0.976562\n",
      "2018-03-14T00:35:58.341028: step 317, loss 0.145047, acc 0.984375\n",
      "2018-03-14T00:35:59.292361: step 318, loss 0.161483, acc 0.992188\n",
      "2018-03-14T00:36:00.257903: step 319, loss 0.118913, acc 1\n",
      "2018-03-14T00:36:01.209701: step 320, loss 0.122534, acc 1\n",
      "2018-03-14T00:36:02.161021: step 321, loss 0.126635, acc 0.992188\n",
      "2018-03-14T00:36:03.134064: step 322, loss 0.127039, acc 0.992188\n",
      "2018-03-14T00:36:04.091614: step 323, loss 0.161601, acc 0.992188\n",
      "2018-03-14T00:36:05.038467: step 324, loss 0.184831, acc 0.96875\n",
      "2018-03-14T00:36:05.958105: step 325, loss 0.162261, acc 0.984375\n",
      "2018-03-14T00:36:06.886323: step 326, loss 0.132768, acc 0.992188\n",
      "2018-03-14T00:36:07.799422: step 327, loss 0.120122, acc 1\n",
      "2018-03-14T00:36:08.720888: step 328, loss 0.144535, acc 0.992188\n",
      "2018-03-14T00:36:09.641999: step 329, loss 0.125386, acc 1\n",
      "2018-03-14T00:36:10.586995: step 330, loss 0.116854, acc 1\n",
      "2018-03-14T00:36:11.550781: step 331, loss 0.174437, acc 0.96875\n",
      "2018-03-14T00:36:12.553816: step 332, loss 0.128493, acc 0.992188\n",
      "2018-03-14T00:36:13.571095: step 333, loss 0.137931, acc 0.984375\n",
      "2018-03-14T00:36:14.556549: step 334, loss 0.128314, acc 0.992188\n",
      "2018-03-14T00:36:15.515260: step 335, loss 0.147946, acc 0.976562\n",
      "2018-03-14T00:36:16.368826: step 336, loss 0.124718, acc 0.991071\n",
      "2018-03-14T00:36:17.319450: step 337, loss 0.111399, acc 1\n",
      "2018-03-14T00:36:18.272263: step 338, loss 0.125047, acc 0.992188\n",
      "2018-03-14T00:36:19.224379: step 339, loss 0.122036, acc 0.992188\n",
      "2018-03-14T00:36:20.214301: step 340, loss 0.115968, acc 0.992188\n",
      "2018-03-14T00:36:21.178689: step 341, loss 0.112214, acc 1\n",
      "2018-03-14T00:36:22.129699: step 342, loss 0.11224, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-14T00:36:23.080204: step 343, loss 0.108815, acc 1\n",
      "2018-03-14T00:36:24.028986: step 344, loss 0.136869, acc 0.984375\n",
      "2018-03-14T00:36:25.033829: step 345, loss 0.115345, acc 1\n",
      "2018-03-14T00:36:25.982877: step 346, loss 0.114621, acc 1\n",
      "2018-03-14T00:36:26.900196: step 347, loss 0.115205, acc 1\n",
      "2018-03-14T00:36:27.811948: step 348, loss 0.117751, acc 0.992188\n",
      "2018-03-14T00:36:28.732524: step 349, loss 0.121854, acc 0.984375\n",
      "2018-03-14T00:36:29.653217: step 350, loss 0.15999, acc 0.984375\n",
      "2018-03-14T00:36:30.593483: step 351, loss 0.126868, acc 0.984375\n",
      "2018-03-14T00:36:31.542551: step 352, loss 0.13266, acc 0.992188\n",
      "2018-03-14T00:36:32.499577: step 353, loss 0.116966, acc 0.992188\n",
      "2018-03-14T00:36:33.444774: step 354, loss 0.151683, acc 0.992188\n",
      "2018-03-14T00:36:34.393016: step 355, loss 0.133307, acc 0.992188\n",
      "2018-03-14T00:36:35.346205: step 356, loss 0.109481, acc 1\n",
      "2018-03-14T00:36:36.289940: step 357, loss 0.110833, acc 1\n",
      "2018-03-14T00:36:37.233065: step 358, loss 0.116781, acc 0.992188\n",
      "2018-03-14T00:36:38.187832: step 359, loss 0.124199, acc 0.992188\n",
      "2018-03-14T00:36:39.140902: step 360, loss 0.107743, acc 1\n",
      "2018-03-14T00:36:40.090746: step 361, loss 0.113101, acc 1\n",
      "2018-03-14T00:36:41.035001: step 362, loss 0.162235, acc 0.976562\n",
      "2018-03-14T00:36:41.950642: step 363, loss 0.11002, acc 1\n",
      "2018-03-14T00:36:42.752786: step 364, loss 0.106279, acc 1\n",
      "2018-03-14T00:36:43.689509: step 365, loss 0.113478, acc 0.992188\n",
      "2018-03-14T00:36:44.606827: step 366, loss 0.112977, acc 0.992188\n",
      "2018-03-14T00:36:45.538287: step 367, loss 0.107053, acc 1\n",
      "2018-03-14T00:36:46.484253: step 368, loss 0.127092, acc 0.992188\n",
      "2018-03-14T00:36:47.431660: step 369, loss 0.105226, acc 1\n",
      "2018-03-14T00:36:48.376103: step 370, loss 0.106157, acc 1\n",
      "2018-03-14T00:36:49.323122: step 371, loss 0.116116, acc 0.992188\n",
      "2018-03-14T00:36:50.267529: step 372, loss 0.103389, acc 1\n",
      "2018-03-14T00:36:51.214626: step 373, loss 0.139716, acc 0.976562\n",
      "2018-03-14T00:36:52.155148: step 374, loss 0.120645, acc 0.992188\n",
      "2018-03-14T00:36:53.104846: step 375, loss 0.121031, acc 0.984375\n",
      "2018-03-14T00:36:54.055624: step 376, loss 0.100888, acc 1\n",
      "2018-03-14T00:36:55.009797: step 377, loss 0.102264, acc 1\n",
      "2018-03-14T00:36:55.963172: step 378, loss 0.106143, acc 1\n",
      "2018-03-14T00:36:56.899538: step 379, loss 0.100105, acc 1\n",
      "2018-03-14T00:36:57.822392: step 380, loss 0.121212, acc 0.984375\n",
      "2018-03-14T00:36:58.739032: step 381, loss 0.107942, acc 1\n",
      "2018-03-14T00:36:59.656470: step 382, loss 0.135746, acc 0.992188\n",
      "2018-03-14T00:37:00.578410: step 383, loss 0.112827, acc 0.992188\n",
      "2018-03-14T00:37:01.530137: step 384, loss 0.105173, acc 1\n",
      "2018-03-14T00:37:02.474725: step 385, loss 0.104759, acc 1\n",
      "2018-03-14T00:37:03.428595: step 386, loss 0.126597, acc 0.984375\n",
      "2018-03-14T00:37:04.374001: step 387, loss 0.133485, acc 0.976562\n",
      "2018-03-14T00:37:05.322286: step 388, loss 0.10883, acc 1\n",
      "2018-03-14T00:37:06.271455: step 389, loss 0.102381, acc 1\n",
      "2018-03-14T00:37:07.222870: step 390, loss 0.104574, acc 1\n",
      "2018-03-14T00:37:08.169581: step 391, loss 0.115406, acc 0.992188\n",
      "2018-03-14T00:37:09.004036: step 392, loss 0.12894, acc 0.991071\n",
      "2018-03-14T00:37:09.960675: step 393, loss 0.118717, acc 0.992188\n",
      "2018-03-14T00:37:10.902529: step 394, loss 0.102881, acc 1\n",
      "2018-03-14T00:37:11.821211: step 395, loss 0.121968, acc 0.992188\n",
      "2018-03-14T00:37:12.737160: step 396, loss 0.140571, acc 0.992188\n",
      "2018-03-14T00:37:13.673821: step 397, loss 0.110355, acc 0.992188\n",
      "2018-03-14T00:37:14.590703: step 398, loss 0.0979476, acc 1\n",
      "2018-03-14T00:37:15.543790: step 399, loss 0.130002, acc 0.984375\n",
      "2018-03-14T00:37:16.510153: step 400, loss 0.0961142, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-03-14T00:37:18.623616: step 400, loss 0.153406, acc 0.984305\n",
      "\n",
      "Saved model checkpoint to /Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel/checkpoints/model-400\n",
      "\n",
      "2018-03-14T00:37:19.717274: step 401, loss 0.0969094, acc 1\n",
      "2018-03-14T00:37:20.699547: step 402, loss 0.100186, acc 1\n",
      "2018-03-14T00:37:21.669770: step 403, loss 0.109794, acc 0.992188\n",
      "2018-03-14T00:37:22.645699: step 404, loss 0.0959125, acc 1\n",
      "2018-03-14T00:37:23.626143: step 405, loss 0.109574, acc 0.992188\n",
      "2018-03-14T00:37:24.595138: step 406, loss 0.0993508, acc 1\n",
      "2018-03-14T00:37:25.580649: step 407, loss 0.102798, acc 0.992188\n",
      "2018-03-14T00:37:26.543563: step 408, loss 0.099893, acc 0.992188\n",
      "2018-03-14T00:37:27.464164: step 409, loss 0.0930813, acc 1\n",
      "2018-03-14T00:37:28.380574: step 410, loss 0.0957144, acc 1\n",
      "2018-03-14T00:37:29.304567: step 411, loss 0.143732, acc 0.992188\n",
      "2018-03-14T00:37:30.219885: step 412, loss 0.0980684, acc 1\n",
      "2018-03-14T00:37:31.149134: step 413, loss 0.101364, acc 0.992188\n",
      "2018-03-14T00:37:32.097941: step 414, loss 0.0946183, acc 1\n",
      "2018-03-14T00:37:33.044642: step 415, loss 0.0955648, acc 1\n",
      "2018-03-14T00:37:33.997796: step 416, loss 0.102747, acc 0.992188\n",
      "2018-03-14T00:37:34.946808: step 417, loss 0.100945, acc 1\n",
      "2018-03-14T00:37:35.894203: step 418, loss 0.0973215, acc 0.992188\n",
      "2018-03-14T00:37:36.859893: step 419, loss 0.0955261, acc 1\n",
      "2018-03-14T00:37:37.686174: step 420, loss 0.109866, acc 0.991071\n",
      "2018-03-14T00:37:38.636125: step 421, loss 0.0917923, acc 1\n",
      "2018-03-14T00:37:39.577734: step 422, loss 0.110209, acc 0.984375\n",
      "2018-03-14T00:37:40.519534: step 423, loss 0.0949021, acc 1\n",
      "2018-03-14T00:37:41.439000: step 424, loss 0.0900768, acc 1\n",
      "2018-03-14T00:37:42.354321: step 425, loss 0.110147, acc 0.984375\n",
      "2018-03-14T00:37:43.268201: step 426, loss 0.104476, acc 0.992188\n",
      "2018-03-14T00:37:44.192542: step 427, loss 0.0913635, acc 1\n",
      "2018-03-14T00:37:45.135352: step 428, loss 0.103698, acc 0.992188\n",
      "2018-03-14T00:37:46.085398: step 429, loss 0.0896758, acc 1\n",
      "2018-03-14T00:37:47.036838: step 430, loss 0.0904994, acc 1\n",
      "2018-03-14T00:37:47.994129: step 431, loss 0.0880745, acc 1\n",
      "2018-03-14T00:37:48.942413: step 432, loss 0.0888553, acc 1\n",
      "2018-03-14T00:37:49.887522: step 433, loss 0.0900379, acc 1\n",
      "2018-03-14T00:37:50.839568: step 434, loss 0.108501, acc 0.992188\n",
      "2018-03-14T00:37:51.790367: step 435, loss 0.0889267, acc 1\n",
      "2018-03-14T00:37:52.744071: step 436, loss 0.0862145, acc 1\n",
      "2018-03-14T00:37:53.690014: step 437, loss 0.132295, acc 0.984375\n",
      "2018-03-14T00:37:54.631885: step 438, loss 0.0998251, acc 1\n",
      "2018-03-14T00:37:55.555326: step 439, loss 0.0993548, acc 0.992188\n",
      "2018-03-14T00:37:56.468215: step 440, loss 0.0976515, acc 0.992188\n",
      "2018-03-14T00:37:57.408921: step 441, loss 0.0915164, acc 1\n",
      "2018-03-14T00:37:58.331818: step 442, loss 0.132509, acc 0.992188\n",
      "2018-03-14T00:37:59.262830: step 443, loss 0.0957914, acc 1\n",
      "2018-03-14T00:38:00.228140: step 444, loss 0.0916218, acc 1\n",
      "2018-03-14T00:38:01.184326: step 445, loss 0.0885109, acc 1\n",
      "2018-03-14T00:38:02.138425: step 446, loss 0.0941996, acc 0.992188\n",
      "2018-03-14T00:38:03.100374: step 447, loss 0.090701, acc 1\n",
      "2018-03-14T00:38:03.935123: step 448, loss 0.0990681, acc 0.991071\n",
      "2018-03-14T00:38:04.905882: step 449, loss 0.0926597, acc 1\n",
      "2018-03-14T00:38:05.843493: step 450, loss 0.105035, acc 0.992188\n",
      "2018-03-14T00:38:06.795441: step 451, loss 0.0833239, acc 1\n",
      "2018-03-14T00:38:07.745904: step 452, loss 0.0901999, acc 1\n",
      "2018-03-14T00:38:08.677055: step 453, loss 0.105111, acc 0.992188\n",
      "2018-03-14T00:38:09.592239: step 454, loss 0.0845801, acc 1\n",
      "2018-03-14T00:38:10.509607: step 455, loss 0.125589, acc 0.992188\n",
      "2018-03-14T00:38:11.426428: step 456, loss 0.0838185, acc 1\n",
      "2018-03-14T00:38:12.351793: step 457, loss 0.112325, acc 0.992188\n",
      "2018-03-14T00:38:13.304523: step 458, loss 0.0861673, acc 1\n",
      "2018-03-14T00:38:14.256163: step 459, loss 0.0853748, acc 1\n",
      "2018-03-14T00:38:15.215357: step 460, loss 0.0974396, acc 0.984375\n",
      "2018-03-14T00:38:16.167354: step 461, loss 0.0960693, acc 0.984375\n",
      "2018-03-14T00:38:17.125368: step 462, loss 0.1168, acc 0.984375\n",
      "2018-03-14T00:38:18.073338: step 463, loss 0.0811909, acc 1\n",
      "2018-03-14T00:38:19.059795: step 464, loss 0.0858061, acc 1\n",
      "2018-03-14T00:38:20.047158: step 465, loss 0.107562, acc 0.992188\n",
      "2018-03-14T00:38:21.045706: step 466, loss 0.0816103, acc 1\n",
      "2018-03-14T00:38:22.018486: step 467, loss 0.0909764, acc 1\n",
      "2018-03-14T00:38:22.967378: step 468, loss 0.100065, acc 0.992188\n",
      "2018-03-14T00:38:23.917372: step 469, loss 0.0883322, acc 1\n",
      "2018-03-14T00:38:24.872710: step 470, loss 0.112303, acc 0.984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-14T00:38:25.834055: step 471, loss 0.0930337, acc 0.992188\n",
      "2018-03-14T00:38:26.783998: step 472, loss 0.0843123, acc 1\n",
      "2018-03-14T00:38:27.696427: step 473, loss 0.0808937, acc 1\n",
      "2018-03-14T00:38:28.609650: step 474, loss 0.0800949, acc 1\n",
      "2018-03-14T00:38:29.526492: step 475, loss 0.0805205, acc 1\n",
      "2018-03-14T00:38:30.335017: step 476, loss 0.0849256, acc 1\n",
      "2018-03-14T00:38:31.265862: step 477, loss 0.0813884, acc 1\n",
      "2018-03-14T00:38:32.218419: step 478, loss 0.0808356, acc 1\n",
      "2018-03-14T00:38:33.171083: step 479, loss 0.0807028, acc 1\n",
      "2018-03-14T00:38:34.121505: step 480, loss 0.0951197, acc 0.992188\n",
      "2018-03-14T00:38:35.068437: step 481, loss 0.0806565, acc 1\n",
      "2018-03-14T00:38:36.014603: step 482, loss 0.08182, acc 1\n",
      "2018-03-14T00:38:36.969846: step 483, loss 0.0822202, acc 1\n",
      "2018-03-14T00:38:37.917935: step 484, loss 0.078914, acc 1\n",
      "2018-03-14T00:38:38.866297: step 485, loss 0.0847366, acc 0.992188\n",
      "2018-03-14T00:38:39.813603: step 486, loss 0.0943559, acc 0.992188\n",
      "2018-03-14T00:38:40.764674: step 487, loss 0.0790561, acc 1\n",
      "2018-03-14T00:38:41.683412: step 488, loss 0.0829872, acc 1\n",
      "2018-03-14T00:38:42.598712: step 489, loss 0.0839457, acc 1\n",
      "2018-03-14T00:38:43.518790: step 490, loss 0.077579, acc 1\n",
      "2018-03-14T00:38:44.450271: step 491, loss 0.0766784, acc 1\n",
      "2018-03-14T00:38:45.387432: step 492, loss 0.0790712, acc 1\n",
      "2018-03-14T00:38:46.333513: step 493, loss 0.0784931, acc 1\n",
      "2018-03-14T00:38:47.285054: step 494, loss 0.0771551, acc 1\n",
      "2018-03-14T00:38:48.232625: step 495, loss 0.074534, acc 1\n",
      "2018-03-14T00:38:49.184272: step 496, loss 0.114972, acc 0.992188\n",
      "2018-03-14T00:38:50.132621: step 497, loss 0.0938398, acc 0.992188\n",
      "2018-03-14T00:38:51.085441: step 498, loss 0.0870052, acc 0.992188\n",
      "2018-03-14T00:38:52.037926: step 499, loss 0.0818319, acc 1\n",
      "2018-03-14T00:38:52.984815: step 500, loss 0.113965, acc 0.976562\n",
      "\n",
      "Evaluation:\n",
      "2018-03-14T00:38:54.992236: step 500, loss 0.12524, acc 0.984305\n",
      "\n",
      "Saved model checkpoint to /Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel/checkpoints/model-500\n",
      "\n",
      "2018-03-14T00:38:56.033144: step 501, loss 0.0965435, acc 0.984375\n",
      "2018-03-14T00:38:56.948599: step 502, loss 0.0956673, acc 0.984375\n",
      "2018-03-14T00:38:57.862130: step 503, loss 0.0762845, acc 1\n",
      "2018-03-14T00:38:58.670251: step 504, loss 0.079208, acc 1\n",
      "2018-03-14T00:38:59.602937: step 505, loss 0.0729451, acc 1\n",
      "2018-03-14T00:39:00.544001: step 506, loss 0.0792076, acc 1\n",
      "2018-03-14T00:39:01.496004: step 507, loss 0.0785793, acc 0.992188\n",
      "2018-03-14T00:39:02.452162: step 508, loss 0.0791115, acc 1\n",
      "2018-03-14T00:39:03.399210: step 509, loss 0.0817778, acc 0.992188\n",
      "2018-03-14T00:39:04.352305: step 510, loss 0.0715741, acc 1\n",
      "2018-03-14T00:39:05.305969: step 511, loss 0.078087, acc 1\n",
      "2018-03-14T00:39:06.253070: step 512, loss 0.0760677, acc 1\n",
      "2018-03-14T00:39:07.199651: step 513, loss 0.0759762, acc 1\n",
      "2018-03-14T00:39:08.145655: step 514, loss 0.0799059, acc 1\n",
      "2018-03-14T00:39:09.102382: step 515, loss 0.0740198, acc 1\n",
      "2018-03-14T00:39:10.032207: step 516, loss 0.0707924, acc 1\n",
      "2018-03-14T00:39:10.947586: step 517, loss 0.105547, acc 0.984375\n",
      "2018-03-14T00:39:11.860522: step 518, loss 0.0735281, acc 1\n",
      "2018-03-14T00:39:12.781450: step 519, loss 0.0695438, acc 1\n",
      "2018-03-14T00:39:13.715090: step 520, loss 0.0734266, acc 1\n",
      "2018-03-14T00:39:14.675678: step 521, loss 0.0709155, acc 1\n",
      "2018-03-14T00:39:15.625081: step 522, loss 0.0700831, acc 1\n",
      "2018-03-14T00:39:16.571583: step 523, loss 0.0745886, acc 1\n",
      "2018-03-14T00:39:17.517527: step 524, loss 0.0754718, acc 1\n",
      "2018-03-14T00:39:18.462833: step 525, loss 0.0725093, acc 1\n",
      "2018-03-14T00:39:19.431848: step 526, loss 0.0686755, acc 1\n",
      "2018-03-14T00:39:20.374590: step 527, loss 0.0753771, acc 1\n",
      "2018-03-14T00:39:21.333649: step 528, loss 0.0731949, acc 1\n",
      "2018-03-14T00:39:22.287115: step 529, loss 0.0691038, acc 1\n",
      "2018-03-14T00:39:23.242324: step 530, loss 0.0729422, acc 1\n",
      "2018-03-14T00:39:24.194791: step 531, loss 0.0674846, acc 1\n",
      "2018-03-14T00:39:25.010152: step 532, loss 0.0832675, acc 0.991071\n",
      "2018-03-14T00:39:25.957604: step 533, loss 0.0920243, acc 0.992188\n",
      "2018-03-14T00:39:26.870439: step 534, loss 0.0794696, acc 0.992188\n",
      "2018-03-14T00:39:27.788450: step 535, loss 0.0681758, acc 1\n",
      "2018-03-14T00:39:28.710342: step 536, loss 0.0754872, acc 0.992188\n",
      "2018-03-14T00:39:29.661781: step 537, loss 0.0884409, acc 0.984375\n",
      "2018-03-14T00:39:30.618009: step 538, loss 0.0704947, acc 1\n",
      "2018-03-14T00:39:31.560872: step 539, loss 0.0671193, acc 1\n",
      "2018-03-14T00:39:32.509481: step 540, loss 0.0709658, acc 1\n",
      "2018-03-14T00:39:33.476090: step 541, loss 0.0712304, acc 1\n",
      "2018-03-14T00:39:34.427073: step 542, loss 0.07328, acc 1\n",
      "2018-03-14T00:39:35.368654: step 543, loss 0.074518, acc 1\n",
      "2018-03-14T00:39:36.313450: step 544, loss 0.0668905, acc 1\n",
      "2018-03-14T00:39:37.271727: step 545, loss 0.0740119, acc 1\n",
      "2018-03-14T00:39:38.218925: step 546, loss 0.0729638, acc 0.992188\n",
      "2018-03-14T00:39:39.168459: step 547, loss 0.0699003, acc 0.992188\n",
      "2018-03-14T00:39:40.113295: step 548, loss 0.0779465, acc 0.992188\n",
      "2018-03-14T00:39:41.034100: step 549, loss 0.0794471, acc 0.992188\n",
      "2018-03-14T00:39:41.955123: step 550, loss 0.0755625, acc 0.992188\n",
      "2018-03-14T00:39:42.873045: step 551, loss 0.0661188, acc 1\n",
      "2018-03-14T00:39:43.811376: step 552, loss 0.0780967, acc 0.992188\n",
      "2018-03-14T00:39:44.735001: step 553, loss 0.0818684, acc 0.992188\n",
      "2018-03-14T00:39:45.718636: step 554, loss 0.0654821, acc 1\n",
      "2018-03-14T00:39:46.659531: step 555, loss 0.0698781, acc 1\n",
      "2018-03-14T00:39:47.604132: step 556, loss 0.0658055, acc 1\n",
      "2018-03-14T00:39:48.574176: step 557, loss 0.0684977, acc 1\n",
      "2018-03-14T00:39:49.522741: step 558, loss 0.0676065, acc 1\n",
      "2018-03-14T00:39:50.476176: step 559, loss 0.0653758, acc 1\n",
      "2018-03-14T00:39:51.311457: step 560, loss 0.0671931, acc 1\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "      log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            num_classes=y_train.shape[1],\n",
    "            vocab_size=len(vocab_processor.vocabulary_),\n",
    "            embedding_size=FLAGS.embedding_dim,\n",
    "            filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "            num_filters=FLAGS.num_filters,\n",
    "            l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "\n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", \"cnnmodel\"))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "\n",
    "        # Write vocabulary\n",
    "        vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, accuracy = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "\n",
    "        # Generate batches\n",
    "        batches = data_helpers.batch_iter(\n",
    "            list(zip(x_train, y_train)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % FLAGS.evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % FLAGS.checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testDataDir = \"/Users/SamZhang/Documents/Capstone/dataset/small/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.flags.DEFINE_string(\"positive_test_data_file\", testDataDir + \"/ham/SMS_test.ham\", \"Data source for the positive data.\")\n",
    "tf.flags.DEFINE_string(\"negative_test_data_file\", testDataDir + \"/spam/SMS_test.spam\", \"Data source for the negative data.\")\n",
    "\n",
    "# Eval Parameters\n",
    "tf.flags.DEFINE_string(\"checkpoint_dir\", \"/Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel/checkpoints/\", \"Checkpoint directory from training run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "BATCH_SIZE=128\n",
      "CHECKPOINT_DIR=/Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel/checkpoints/\n",
      "CHECKPOINT_EVERY=100\n",
      "DEV_SAMPLE_PERCENTAGE=0.2\n",
      "DROPOUT_KEEP_PROB=0.5\n",
      "EMBEDDING_DIM=128\n",
      "EVALUATE_EVERY=100\n",
      "FILTER_SIZES=3,4,5\n",
      "L2_REG_LAMBDA=0.1\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "NEGATIVE_DATA_FILE=/Users/SamZhang/Documents/Capstone/dataset/small/train/spam/SMS_train.spam\n",
      "NEGATIVE_TEST_DATA_FILE=/Users/SamZhang/Documents/Capstone/dataset/small/test/spam/SMS_test.spam\n",
      "NUM_CHECKPOINTS=5\n",
      "NUM_EPOCHS=20\n",
      "NUM_FILTERS=256\n",
      "POSITIVE_DATA_FILE=/Users/SamZhang/Documents/Capstone/dataset/small/train/ham/SMS_train.ham\n",
      "POSITIVE_TEST_DATA_FILE=/Users/SamZhang/Documents/Capstone/dataset/small/test/ham/SMS_test.ham\n",
      "\n"
     ]
    }
   ],
   "source": [
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_raw, y_test = data_helpers.load_data_and_labels(FLAGS.positive_test_data_file, FLAGS.negative_test_data_file)\n",
    "y_test = np.argmax(y_test, axis=1) #ham = 1, spam = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_path = os.path.join(FLAGS.checkpoint_dir, \"..\", \"vocab\")\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor.restore(vocab_path)\n",
    "x_test = np.array(list(vocab_processor.transform(x_raw)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_file = tf.train.latest_checkpoint(FLAGS.checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel/checkpoints/model-500\n",
      "Total number of test examples: 1114\n",
      "Accuracy: 0.964991\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "      log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        # Load the saved meta graph and restore variables\n",
    "        saver = tf.train.import_meta_graph(\"{}.meta\".format(checkpoint_file))\n",
    "        saver.restore(sess, checkpoint_file)\n",
    "\n",
    "        # Get the placeholders from the graph by name\n",
    "        input_x = graph.get_operation_by_name(\"input_x\").outputs[0]\n",
    "        # input_y = graph.get_operation_by_name(\"input_y\").outputs[0]\n",
    "        dropout_keep_prob = graph.get_operation_by_name(\"dropout_keep_prob\").outputs[0]\n",
    "\n",
    "        # Tensors we want to evaluate\n",
    "        predictions = graph.get_operation_by_name(\"output/predictions\").outputs[0]\n",
    "\n",
    "        # Generate batches for one epoch\n",
    "        batches = data_helpers.batch_iter(list(x_test), FLAGS.batch_size, 1, shuffle=False)\n",
    "\n",
    "        # Collect the predictions here\n",
    "        all_predictions = []\n",
    "\n",
    "        for x_test_batch in batches:\n",
    "            batch_predictions = sess.run(predictions, {input_x: x_test_batch, dropout_keep_prob: 1.0})\n",
    "            all_predictions = np.concatenate([all_predictions, batch_predictions])\n",
    "\n",
    "# Print accuracy if y_test is defined\n",
    "if y_test is not None:\n",
    "    correct_predictions = float(sum(all_predictions == y_test))\n",
    "    print(\"Total number of test examples: {}\".format(len(y_test)))\n",
    "    print(\"Accuracy: {:g}\".format(correct_predictions/float(len(y_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving evaluation to /Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel/checkpoints/../prediction.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the evaluation to a csv\n",
    "title = np.column_stack(('text', 'prediction', 'label'))\n",
    "predictions_human_readable = np.column_stack((np.array(x_raw), all_predictions, y_test))\n",
    "out_path = os.path.join(FLAGS.checkpoint_dir, \"..\", \"prediction.csv\")\n",
    "print(\"Saving evaluation to {0}\".format(out_path))\n",
    "\n",
    "with open(out_path, 'w') as f:\n",
    "    csv.writer(f).writerows(title)\n",
    "    csv.writer(f).writerows(predictions_human_readable)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
