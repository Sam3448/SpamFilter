{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import data_helpers\n",
    "from text_cnn import TextCNN\n",
    "from tensorflow.contrib import learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainDataDir = \"/Users/SamZhang/Documents/Capstone/dataset/small/train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading params\n",
    "tf.flags.DEFINE_float(\"dev_sample_percentage\", .2, \"Percentage of the training data to use for validation\")\n",
    "tf.flags.DEFINE_string(\"positive_data_file\", trainDataDir + \"/spam/SMS_train.spam\", \"Data source for the positive data.\")\n",
    "tf.flags.DEFINE_string(\"negative_data_file\", trainDataDir + \"/ham/SMS_train.ham\", \"Data source for the negative data.\")\n",
    "\n",
    "# Model Hyperparameters\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 256, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.1, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 128, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 20, \"Number of training epochs (default: 200)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "BATCH_SIZE=128\n",
      "CHECKPOINT_EVERY=100\n",
      "DEV_SAMPLE_PERCENTAGE=0.2\n",
      "DROPOUT_KEEP_PROB=0.5\n",
      "EMBEDDING_DIM=128\n",
      "EVALUATE_EVERY=100\n",
      "FILTER_SIZES=3,4,5\n",
      "L2_REG_LAMBDA=0.1\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "NEGATIVE_DATA_FILE=/Users/SamZhang/Documents/Capstone/dataset/small/train/ham/SMS_train.ham\n",
      "NUM_CHECKPOINTS=5\n",
      "NUM_EPOCHS=20\n",
      "NUM_FILTERS=256\n",
      "POSITIVE_DATA_FILE=/Users/SamZhang/Documents/Capstone/dataset/small/train/spam/SMS_train.spam\n",
      "\n"
     ]
    }
   ],
   "source": [
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def processData():\n",
    "    # Data Preparation\n",
    "    # ==================================================\n",
    "\n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    x_text, y = data_helpers.load_data_and_labels(FLAGS.positive_data_file, FLAGS.negative_data_file)\n",
    "    print(x_text[0])\n",
    "\n",
    "    # Build vocabulary\n",
    "    max_document_length = max([len(x.split(\" \")) for x in x_text])\n",
    "    vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "    x = np.array(list(vocab_processor.fit_transform(x_text)))\n",
    "\n",
    "    # Randomly shuffle data\n",
    "    np.random.seed(10)\n",
    "    shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "    x_shuffled = x[shuffle_indices]\n",
    "    y_shuffled = y[shuffle_indices]\n",
    "\n",
    "    # Split train/test set\n",
    "    # TODO: This is very crude, should use cross-validation\n",
    "    dev_sample_index = -1 * int(FLAGS.dev_sample_percentage * float(len(y)))\n",
    "    x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "    y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "\n",
    "    del x, y, x_shuffled, y_shuffled\n",
    "\n",
    "    print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
    "    print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))\n",
    "    \n",
    "    return x_train, y_train, x_dev, y_dev, vocab_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry question \\( std txt rate \\) t c 's apply 08452810075over18 's\n",
      "Vocabulary Size: 7814\n",
      "Train/Dev split: 3568/892\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_dev, y_dev, vocab_processor = processData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to /Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel\n",
      "\n",
      "2018-03-15T17:46:28.326569: step 1, loss 3.89652, acc 0.359375\n",
      "2018-03-15T17:46:29.189320: step 2, loss 1.26011, acc 0.71875\n",
      "2018-03-15T17:46:30.059922: step 3, loss 1.14055, acc 0.835938\n",
      "2018-03-15T17:46:30.956020: step 4, loss 0.905671, acc 0.914062\n",
      "2018-03-15T17:46:31.827990: step 5, loss 1.87267, acc 0.851562\n",
      "2018-03-15T17:46:32.707851: step 6, loss 1.57418, acc 0.882812\n",
      "2018-03-15T17:46:33.566748: step 7, loss 1.62063, acc 0.875\n",
      "2018-03-15T17:46:34.438635: step 8, loss 1.90887, acc 0.867188\n",
      "2018-03-15T17:46:35.300821: step 9, loss 2.87425, acc 0.78125\n",
      "2018-03-15T17:46:36.180018: step 10, loss 1.78917, acc 0.859375\n",
      "2018-03-15T17:46:37.094990: step 11, loss 1.18347, acc 0.875\n",
      "2018-03-15T17:46:38.032597: step 12, loss 0.719, acc 0.890625\n",
      "2018-03-15T17:46:38.955674: step 13, loss 0.923639, acc 0.796875\n",
      "2018-03-15T17:46:39.945135: step 14, loss 0.705334, acc 0.867188\n",
      "2018-03-15T17:46:40.975761: step 15, loss 0.937315, acc 0.78125\n",
      "2018-03-15T17:46:41.958540: step 16, loss 1.3582, acc 0.726562\n",
      "2018-03-15T17:46:42.912474: step 17, loss 0.987266, acc 0.789062\n",
      "2018-03-15T17:46:43.865562: step 18, loss 1.21211, acc 0.789062\n",
      "2018-03-15T17:46:44.847139: step 19, loss 0.836361, acc 0.820312\n",
      "2018-03-15T17:46:45.817375: step 20, loss 0.825066, acc 0.789062\n",
      "2018-03-15T17:46:46.847939: step 21, loss 1.00076, acc 0.78125\n",
      "2018-03-15T17:46:47.835516: step 22, loss 0.516798, acc 0.90625\n",
      "2018-03-15T17:46:48.788292: step 23, loss 0.808084, acc 0.867188\n",
      "2018-03-15T17:46:49.893092: step 24, loss 0.674666, acc 0.882812\n",
      "2018-03-15T17:46:50.879549: step 25, loss 0.895266, acc 0.875\n",
      "2018-03-15T17:46:51.896168: step 26, loss 0.85574, acc 0.890625\n",
      "2018-03-15T17:46:52.965347: step 27, loss 1.10871, acc 0.820312\n",
      "2018-03-15T17:46:53.833575: step 28, loss 1.03969, acc 0.857143\n",
      "2018-03-15T17:46:54.827086: step 29, loss 0.399834, acc 0.9375\n",
      "2018-03-15T17:46:55.811261: step 30, loss 0.637062, acc 0.90625\n",
      "2018-03-15T17:46:56.785339: step 31, loss 0.62629, acc 0.875\n",
      "2018-03-15T17:46:57.771879: step 32, loss 0.536213, acc 0.921875\n",
      "2018-03-15T17:46:58.773344: step 33, loss 0.529204, acc 0.945312\n",
      "2018-03-15T17:46:59.904550: step 34, loss 0.564019, acc 0.882812\n",
      "2018-03-15T17:47:00.916923: step 35, loss 0.615425, acc 0.882812\n",
      "2018-03-15T17:47:01.950575: step 36, loss 0.406274, acc 0.921875\n",
      "2018-03-15T17:47:02.983659: step 37, loss 0.592506, acc 0.898438\n",
      "2018-03-15T17:47:04.038059: step 38, loss 0.481542, acc 0.929688\n",
      "2018-03-15T17:47:05.110410: step 39, loss 0.686234, acc 0.84375\n",
      "2018-03-15T17:47:06.127470: step 40, loss 0.468476, acc 0.929688\n",
      "2018-03-15T17:47:07.133053: step 41, loss 0.461552, acc 0.914062\n",
      "2018-03-15T17:47:08.128709: step 42, loss 0.571642, acc 0.867188\n",
      "2018-03-15T17:47:09.119518: step 43, loss 0.312324, acc 0.953125\n",
      "2018-03-15T17:47:10.140713: step 44, loss 0.462929, acc 0.9375\n",
      "2018-03-15T17:47:11.136192: step 45, loss 0.441039, acc 0.921875\n",
      "2018-03-15T17:47:12.138086: step 46, loss 0.465788, acc 0.898438\n",
      "2018-03-15T17:47:13.139408: step 47, loss 0.742609, acc 0.882812\n",
      "2018-03-15T17:47:14.129980: step 48, loss 0.658592, acc 0.914062\n",
      "2018-03-15T17:47:15.207523: step 49, loss 0.523837, acc 0.921875\n",
      "2018-03-15T17:47:16.216639: step 50, loss 0.380561, acc 0.929688\n",
      "2018-03-15T17:47:17.240287: step 51, loss 0.336586, acc 0.960938\n",
      "2018-03-15T17:47:18.264653: step 52, loss 0.654324, acc 0.90625\n",
      "2018-03-15T17:47:19.306024: step 53, loss 0.318073, acc 0.96875\n",
      "2018-03-15T17:47:20.353165: step 54, loss 0.286713, acc 0.96875\n",
      "2018-03-15T17:47:21.388666: step 55, loss 0.453756, acc 0.921875\n",
      "2018-03-15T17:47:22.298987: step 56, loss 0.338058, acc 0.9375\n",
      "2018-03-15T17:47:23.389833: step 57, loss 0.366966, acc 0.9375\n",
      "2018-03-15T17:47:24.442078: step 58, loss 0.368379, acc 0.914062\n",
      "2018-03-15T17:47:25.477718: step 59, loss 0.371685, acc 0.929688\n",
      "2018-03-15T17:47:26.545952: step 60, loss 0.338598, acc 0.960938\n",
      "2018-03-15T17:47:27.648453: step 61, loss 0.450006, acc 0.921875\n",
      "2018-03-15T17:47:28.685477: step 62, loss 0.312444, acc 0.976562\n",
      "2018-03-15T17:47:29.766629: step 63, loss 0.407603, acc 0.960938\n",
      "2018-03-15T17:47:30.822127: step 64, loss 0.339504, acc 0.9375\n",
      "2018-03-15T17:47:31.872078: step 65, loss 0.475784, acc 0.898438\n",
      "2018-03-15T17:47:32.934302: step 66, loss 0.392756, acc 0.96875\n",
      "2018-03-15T17:47:33.976921: step 67, loss 0.340086, acc 0.96875\n",
      "2018-03-15T17:47:34.993025: step 68, loss 0.353703, acc 0.953125\n",
      "2018-03-15T17:47:35.993769: step 69, loss 0.410146, acc 0.921875\n",
      "2018-03-15T17:47:37.017425: step 70, loss 0.636004, acc 0.929688\n",
      "2018-03-15T17:47:38.035688: step 71, loss 0.393663, acc 0.929688\n",
      "2018-03-15T17:47:39.081603: step 72, loss 0.319187, acc 0.960938\n",
      "2018-03-15T17:47:40.107578: step 73, loss 0.515555, acc 0.945312\n",
      "2018-03-15T17:47:41.121281: step 74, loss 0.429309, acc 0.9375\n",
      "2018-03-15T17:47:42.122436: step 75, loss 0.341319, acc 0.9375\n",
      "2018-03-15T17:47:43.118148: step 76, loss 0.404185, acc 0.9375\n",
      "2018-03-15T17:47:44.121048: step 77, loss 0.421728, acc 0.929688\n",
      "2018-03-15T17:47:45.172033: step 78, loss 0.36672, acc 0.953125\n",
      "2018-03-15T17:47:46.192771: step 79, loss 0.386256, acc 0.929688\n",
      "2018-03-15T17:47:47.224171: step 80, loss 0.326151, acc 0.960938\n",
      "2018-03-15T17:47:48.216395: step 81, loss 0.340464, acc 0.9375\n",
      "2018-03-15T17:47:49.172893: step 82, loss 0.264477, acc 0.96875\n",
      "2018-03-15T17:47:50.162466: step 83, loss 0.274097, acc 0.960938\n",
      "2018-03-15T17:47:51.019773: step 84, loss 0.36908, acc 0.946429\n",
      "2018-03-15T17:47:52.006740: step 85, loss 0.361125, acc 0.960938\n",
      "2018-03-15T17:47:52.998869: step 86, loss 0.416074, acc 0.9375\n",
      "2018-03-15T17:47:54.010344: step 87, loss 0.350019, acc 0.921875\n",
      "2018-03-15T17:47:54.995438: step 88, loss 0.321073, acc 0.976562\n",
      "2018-03-15T17:47:56.014041: step 89, loss 0.471784, acc 0.921875\n",
      "2018-03-15T17:47:57.010013: step 90, loss 0.420484, acc 0.945312\n",
      "2018-03-15T17:47:57.957360: step 91, loss 0.217346, acc 0.992188\n",
      "2018-03-15T17:47:58.898911: step 92, loss 0.270232, acc 0.976562\n",
      "2018-03-15T17:47:59.863895: step 93, loss 0.33182, acc 0.96875\n",
      "2018-03-15T17:48:00.828330: step 94, loss 0.226266, acc 0.984375\n",
      "2018-03-15T17:48:01.776973: step 95, loss 0.210361, acc 0.984375\n",
      "2018-03-15T17:48:02.737350: step 96, loss 0.285089, acc 0.976562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-15T17:48:03.683413: step 97, loss 0.330531, acc 0.96875\n",
      "2018-03-15T17:48:04.629976: step 98, loss 0.362217, acc 0.953125\n",
      "2018-03-15T17:48:05.577202: step 99, loss 0.322987, acc 0.9375\n",
      "2018-03-15T17:48:06.519368: step 100, loss 0.257919, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2018-03-15T17:48:08.847431: step 100, loss 0.308056, acc 0.969731\n",
      "\n",
      "Saved model checkpoint to /Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel/checkpoints/model-100\n",
      "\n",
      "2018-03-15T17:48:09.944949: step 101, loss 0.279902, acc 0.953125\n",
      "2018-03-15T17:48:10.895282: step 102, loss 0.384854, acc 0.96875\n",
      "2018-03-15T17:48:11.843925: step 103, loss 0.233529, acc 0.96875\n",
      "2018-03-15T17:48:12.801791: step 104, loss 0.232146, acc 0.96875\n",
      "2018-03-15T17:48:14.009841: step 105, loss 0.443532, acc 0.960938\n",
      "2018-03-15T17:48:15.067472: step 106, loss 0.36805, acc 0.945312\n",
      "2018-03-15T17:48:16.284688: step 107, loss 0.234471, acc 0.96875\n",
      "2018-03-15T17:48:17.549798: step 108, loss 0.285291, acc 0.976562\n",
      "2018-03-15T17:48:18.811334: step 109, loss 0.241324, acc 0.96875\n",
      "2018-03-15T17:48:19.813602: step 110, loss 0.232344, acc 0.984375\n",
      "2018-03-15T17:48:20.847540: step 111, loss 0.280351, acc 0.960938\n",
      "2018-03-15T17:48:21.703800: step 112, loss 0.211921, acc 0.982143\n",
      "2018-03-15T17:48:22.680492: step 113, loss 0.289828, acc 0.960938\n",
      "2018-03-15T17:48:23.714460: step 114, loss 0.237811, acc 0.96875\n",
      "2018-03-15T17:48:24.670908: step 115, loss 0.229176, acc 0.976562\n",
      "2018-03-15T17:48:25.654643: step 116, loss 0.264879, acc 0.96875\n",
      "2018-03-15T17:48:26.613615: step 117, loss 0.255785, acc 0.976562\n",
      "2018-03-15T17:48:27.593305: step 118, loss 0.211539, acc 0.984375\n",
      "2018-03-15T17:48:28.562626: step 119, loss 0.292483, acc 0.96875\n",
      "2018-03-15T17:48:29.554122: step 120, loss 0.267082, acc 0.96875\n",
      "2018-03-15T17:48:30.516687: step 121, loss 0.22499, acc 0.976562\n",
      "2018-03-15T17:48:31.496858: step 122, loss 0.295044, acc 0.960938\n",
      "2018-03-15T17:48:32.491754: step 123, loss 0.26308, acc 0.953125\n",
      "2018-03-15T17:48:33.483176: step 124, loss 0.277186, acc 0.960938\n",
      "2018-03-15T17:48:34.442411: step 125, loss 0.26447, acc 0.96875\n",
      "2018-03-15T17:48:35.427369: step 126, loss 0.210619, acc 0.984375\n",
      "2018-03-15T17:48:36.403945: step 127, loss 0.442733, acc 0.953125\n",
      "2018-03-15T17:48:37.367622: step 128, loss 0.262567, acc 0.960938\n",
      "2018-03-15T17:48:38.334102: step 129, loss 0.198475, acc 0.992188\n",
      "2018-03-15T17:48:39.332362: step 130, loss 0.224835, acc 0.976562\n",
      "2018-03-15T17:48:40.477974: step 131, loss 0.251828, acc 0.96875\n",
      "2018-03-15T17:48:41.439611: step 132, loss 0.210125, acc 0.984375\n",
      "2018-03-15T17:48:42.399430: step 133, loss 0.387564, acc 0.945312\n",
      "2018-03-15T17:48:43.360412: step 134, loss 0.255858, acc 0.984375\n",
      "2018-03-15T17:48:44.421947: step 135, loss 0.413125, acc 0.953125\n",
      "2018-03-15T17:48:45.403578: step 136, loss 0.223369, acc 0.984375\n",
      "2018-03-15T17:48:46.407870: step 137, loss 0.242508, acc 0.96875\n",
      "2018-03-15T17:48:47.439629: step 138, loss 0.269932, acc 0.960938\n",
      "2018-03-15T17:48:48.491741: step 139, loss 0.226181, acc 0.96875\n",
      "2018-03-15T17:48:49.443939: step 140, loss 0.241068, acc 0.982143\n",
      "2018-03-15T17:48:50.484773: step 141, loss 0.196448, acc 0.992188\n",
      "2018-03-15T17:48:51.576690: step 142, loss 0.218286, acc 0.984375\n",
      "2018-03-15T17:48:52.553457: step 143, loss 0.205424, acc 0.984375\n",
      "2018-03-15T17:48:53.579315: step 144, loss 0.294511, acc 0.953125\n",
      "2018-03-15T17:48:54.555675: step 145, loss 0.20982, acc 0.96875\n",
      "2018-03-15T17:48:55.540935: step 146, loss 0.193951, acc 0.992188\n",
      "2018-03-15T17:48:56.524242: step 147, loss 0.244944, acc 0.96875\n",
      "2018-03-15T17:48:57.494945: step 148, loss 0.330811, acc 0.945312\n",
      "2018-03-15T17:48:58.455767: step 149, loss 0.247722, acc 0.960938\n",
      "2018-03-15T17:48:59.440132: step 150, loss 0.221505, acc 0.984375\n",
      "2018-03-15T17:49:00.384925: step 151, loss 0.229675, acc 0.960938\n",
      "2018-03-15T17:49:01.361915: step 152, loss 0.264662, acc 0.96875\n",
      "2018-03-15T17:49:02.361683: step 153, loss 0.24947, acc 0.976562\n",
      "2018-03-15T17:49:03.339653: step 154, loss 0.353779, acc 0.945312\n",
      "2018-03-15T17:49:04.350875: step 155, loss 0.194293, acc 0.992188\n",
      "2018-03-15T17:49:05.448766: step 156, loss 0.247419, acc 0.96875\n",
      "2018-03-15T17:49:06.415441: step 157, loss 0.246482, acc 0.96875\n",
      "2018-03-15T17:49:07.385729: step 158, loss 0.205884, acc 0.976562\n",
      "2018-03-15T17:49:08.391440: step 159, loss 0.316381, acc 0.960938\n",
      "2018-03-15T17:49:09.388449: step 160, loss 0.209407, acc 0.984375\n",
      "2018-03-15T17:49:10.484448: step 161, loss 0.237143, acc 0.976562\n",
      "2018-03-15T17:49:11.548741: step 162, loss 0.250126, acc 0.96875\n",
      "2018-03-15T17:49:12.595704: step 163, loss 0.176399, acc 1\n",
      "2018-03-15T17:49:13.702063: step 164, loss 0.21452, acc 0.96875\n",
      "2018-03-15T17:49:14.848495: step 165, loss 0.198139, acc 0.984375\n",
      "2018-03-15T17:49:15.979727: step 166, loss 0.324067, acc 0.984375\n",
      "2018-03-15T17:49:16.983927: step 167, loss 0.21716, acc 0.976562\n",
      "2018-03-15T17:49:17.836697: step 168, loss 0.226367, acc 0.982143\n",
      "2018-03-15T17:49:18.840764: step 169, loss 0.24559, acc 0.976562\n",
      "2018-03-15T17:49:19.843030: step 170, loss 0.235219, acc 0.984375\n",
      "2018-03-15T17:49:20.850998: step 171, loss 0.175119, acc 1\n",
      "2018-03-15T17:49:21.888095: step 172, loss 0.196821, acc 0.984375\n",
      "2018-03-15T17:49:22.875095: step 173, loss 0.220426, acc 0.96875\n",
      "2018-03-15T17:49:23.880384: step 174, loss 0.411918, acc 0.960938\n",
      "2018-03-15T17:49:24.939408: step 175, loss 0.206338, acc 0.976562\n",
      "2018-03-15T17:49:26.051976: step 176, loss 0.204054, acc 0.992188\n",
      "2018-03-15T17:49:27.051115: step 177, loss 0.256444, acc 0.96875\n",
      "2018-03-15T17:49:28.037707: step 178, loss 0.211897, acc 0.976562\n",
      "2018-03-15T17:49:29.010327: step 179, loss 0.184597, acc 0.992188\n",
      "2018-03-15T17:49:29.969996: step 180, loss 0.209545, acc 0.984375\n",
      "2018-03-15T17:49:30.942920: step 181, loss 0.18473, acc 0.992188\n",
      "2018-03-15T17:49:31.880082: step 182, loss 0.188124, acc 0.992188\n",
      "2018-03-15T17:49:32.829711: step 183, loss 0.198349, acc 0.992188\n",
      "2018-03-15T17:49:33.751784: step 184, loss 0.212647, acc 0.984375\n",
      "2018-03-15T17:49:34.683367: step 185, loss 0.205378, acc 0.984375\n",
      "2018-03-15T17:49:35.612633: step 186, loss 0.195129, acc 0.984375\n",
      "2018-03-15T17:49:36.587817: step 187, loss 0.235175, acc 0.984375\n",
      "2018-03-15T17:49:37.570238: step 188, loss 0.254573, acc 0.96875\n",
      "2018-03-15T17:49:38.541839: step 189, loss 0.216732, acc 0.976562\n",
      "2018-03-15T17:49:39.494013: step 190, loss 0.298075, acc 0.976562\n",
      "2018-03-15T17:49:40.483123: step 191, loss 0.205273, acc 0.976562\n",
      "2018-03-15T17:49:41.445854: step 192, loss 0.185741, acc 0.992188\n",
      "2018-03-15T17:49:42.428299: step 193, loss 0.181436, acc 0.984375\n",
      "2018-03-15T17:49:43.544662: step 194, loss 0.225504, acc 0.984375\n",
      "2018-03-15T17:49:44.562231: step 195, loss 0.223262, acc 0.976562\n",
      "2018-03-15T17:49:45.442757: step 196, loss 0.184915, acc 0.991071\n",
      "2018-03-15T17:49:46.445396: step 197, loss 0.212371, acc 0.976562\n",
      "2018-03-15T17:49:47.418664: step 198, loss 0.209458, acc 0.976562\n",
      "2018-03-15T17:49:48.368208: step 199, loss 0.208371, acc 0.984375\n",
      "2018-03-15T17:49:49.354007: step 200, loss 0.184376, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-03-15T17:49:51.626976: step 200, loss 0.272441, acc 0.9787\n",
      "\n",
      "Saved model checkpoint to /Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel/checkpoints/model-200\n",
      "\n",
      "2018-03-15T17:49:52.711157: step 201, loss 0.233689, acc 0.976562\n",
      "2018-03-15T17:49:53.682087: step 202, loss 0.171429, acc 0.992188\n",
      "2018-03-15T17:49:54.632867: step 203, loss 0.181619, acc 0.984375\n",
      "2018-03-15T17:49:55.600136: step 204, loss 0.176653, acc 0.992188\n",
      "2018-03-15T17:49:56.559057: step 205, loss 0.240387, acc 0.960938\n",
      "2018-03-15T17:49:57.531305: step 206, loss 0.195874, acc 0.984375\n",
      "2018-03-15T17:49:58.480717: step 207, loss 0.173427, acc 1\n",
      "2018-03-15T17:49:59.442546: step 208, loss 0.260454, acc 0.96875\n",
      "2018-03-15T17:50:00.393844: step 209, loss 0.194774, acc 0.984375\n",
      "2018-03-15T17:50:01.345195: step 210, loss 0.229387, acc 0.96875\n",
      "2018-03-15T17:50:02.330016: step 211, loss 0.280983, acc 0.96875\n",
      "2018-03-15T17:50:03.321743: step 212, loss 0.158762, acc 1\n",
      "2018-03-15T17:50:04.336675: step 213, loss 0.175071, acc 0.992188\n",
      "2018-03-15T17:50:05.350146: step 214, loss 0.214498, acc 0.984375\n",
      "2018-03-15T17:50:06.388020: step 215, loss 0.162669, acc 1\n",
      "2018-03-15T17:50:07.387222: step 216, loss 0.220877, acc 0.976562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-15T17:50:08.341658: step 217, loss 0.191203, acc 0.984375\n",
      "2018-03-15T17:50:09.309278: step 218, loss 0.167548, acc 0.992188\n",
      "2018-03-15T17:50:10.268578: step 219, loss 0.160504, acc 1\n",
      "2018-03-15T17:50:11.210428: step 220, loss 0.17248, acc 0.992188\n",
      "2018-03-15T17:50:12.151756: step 221, loss 0.203864, acc 0.96875\n",
      "2018-03-15T17:50:13.102264: step 222, loss 0.15715, acc 1\n",
      "2018-03-15T17:50:14.056976: step 223, loss 0.20358, acc 0.976562\n",
      "2018-03-15T17:50:14.888307: step 224, loss 0.189547, acc 0.982143\n",
      "2018-03-15T17:50:15.871752: step 225, loss 0.185083, acc 0.984375\n",
      "2018-03-15T17:50:16.845629: step 226, loss 0.19873, acc 0.984375\n",
      "2018-03-15T17:50:17.805668: step 227, loss 0.190343, acc 0.992188\n",
      "2018-03-15T17:50:18.774465: step 228, loss 0.176944, acc 0.992188\n",
      "2018-03-15T17:50:19.760785: step 229, loss 0.169672, acc 0.992188\n",
      "2018-03-15T17:50:20.786222: step 230, loss 0.179228, acc 0.992188\n",
      "2018-03-15T17:50:21.914691: step 231, loss 0.1802, acc 0.984375\n",
      "2018-03-15T17:50:22.889826: step 232, loss 0.1575, acc 1\n",
      "2018-03-15T17:50:23.873653: step 233, loss 0.199616, acc 0.976562\n",
      "2018-03-15T17:50:24.839837: step 234, loss 0.202601, acc 0.976562\n",
      "2018-03-15T17:50:25.801784: step 235, loss 0.171947, acc 0.992188\n",
      "2018-03-15T17:50:26.786540: step 236, loss 0.161958, acc 1\n",
      "2018-03-15T17:50:27.733533: step 237, loss 0.195042, acc 0.976562\n",
      "2018-03-15T17:50:28.670472: step 238, loss 0.162282, acc 1\n",
      "2018-03-15T17:50:29.634578: step 239, loss 0.239521, acc 0.984375\n",
      "2018-03-15T17:50:30.577745: step 240, loss 0.165873, acc 1\n",
      "2018-03-15T17:50:31.630795: step 241, loss 0.167447, acc 0.992188\n",
      "2018-03-15T17:50:32.623013: step 242, loss 0.166955, acc 0.992188\n",
      "2018-03-15T17:50:33.694085: step 243, loss 0.229739, acc 0.976562\n",
      "2018-03-15T17:50:34.766081: step 244, loss 0.168758, acc 0.992188\n",
      "2018-03-15T17:50:35.953809: step 245, loss 0.219329, acc 0.984375\n",
      "2018-03-15T17:50:36.991038: step 246, loss 0.213296, acc 0.976562\n",
      "2018-03-15T17:50:38.011854: step 247, loss 0.189179, acc 0.96875\n",
      "2018-03-15T17:50:38.992825: step 248, loss 0.152134, acc 1\n",
      "2018-03-15T17:50:40.023510: step 249, loss 0.15207, acc 1\n",
      "2018-03-15T17:50:41.017128: step 250, loss 0.160753, acc 0.992188\n",
      "2018-03-15T17:50:42.045900: step 251, loss 0.184858, acc 0.976562\n",
      "2018-03-15T17:50:42.895461: step 252, loss 0.148851, acc 1\n",
      "2018-03-15T17:50:43.863736: step 253, loss 0.14932, acc 1\n",
      "2018-03-15T17:50:44.833472: step 254, loss 0.151188, acc 1\n",
      "2018-03-15T17:50:45.786266: step 255, loss 0.15985, acc 0.992188\n",
      "2018-03-15T17:50:46.768527: step 256, loss 0.165981, acc 0.992188\n",
      "2018-03-15T17:50:47.768432: step 257, loss 0.17965, acc 0.984375\n",
      "2018-03-15T17:50:48.744160: step 258, loss 0.186521, acc 0.992188\n",
      "2018-03-15T17:50:49.769055: step 259, loss 0.209811, acc 0.984375\n",
      "2018-03-15T17:50:50.768143: step 260, loss 0.210333, acc 0.976562\n",
      "2018-03-15T17:50:51.745762: step 261, loss 0.164714, acc 0.992188\n",
      "2018-03-15T17:50:52.792789: step 262, loss 0.152273, acc 1\n",
      "2018-03-15T17:50:53.793884: step 263, loss 0.161826, acc 0.992188\n",
      "2018-03-15T17:50:54.767977: step 264, loss 0.189822, acc 0.984375\n",
      "2018-03-15T17:50:55.723533: step 265, loss 0.151756, acc 1\n",
      "2018-03-15T17:50:56.671435: step 266, loss 0.15271, acc 0.992188\n",
      "2018-03-15T17:50:57.636877: step 267, loss 0.159786, acc 0.984375\n",
      "2018-03-15T17:50:58.607184: step 268, loss 0.215939, acc 0.984375\n",
      "2018-03-15T17:50:59.596215: step 269, loss 0.16366, acc 0.984375\n",
      "2018-03-15T17:51:00.577062: step 270, loss 0.200169, acc 0.984375\n",
      "2018-03-15T17:51:01.583111: step 271, loss 0.183743, acc 0.984375\n",
      "2018-03-15T17:51:02.568255: step 272, loss 0.170226, acc 0.984375\n",
      "2018-03-15T17:51:03.520980: step 273, loss 0.161438, acc 0.992188\n",
      "2018-03-15T17:51:04.486534: step 274, loss 0.156093, acc 1\n",
      "2018-03-15T17:51:05.443703: step 275, loss 0.141964, acc 1\n",
      "2018-03-15T17:51:06.382002: step 276, loss 0.143891, acc 1\n",
      "2018-03-15T17:51:07.342421: step 277, loss 0.159872, acc 0.992188\n",
      "2018-03-15T17:51:08.313956: step 278, loss 0.194401, acc 0.976562\n",
      "2018-03-15T17:51:09.256340: step 279, loss 0.152264, acc 0.992188\n",
      "2018-03-15T17:51:10.087674: step 280, loss 0.156085, acc 1\n",
      "2018-03-15T17:51:11.219113: step 281, loss 0.153576, acc 0.992188\n",
      "2018-03-15T17:51:12.346905: step 282, loss 0.143793, acc 1\n",
      "2018-03-15T17:51:13.362228: step 283, loss 0.254168, acc 0.976562\n",
      "2018-03-15T17:51:14.365436: step 284, loss 0.147495, acc 0.992188\n",
      "2018-03-15T17:51:15.502055: step 285, loss 0.173044, acc 0.984375\n",
      "2018-03-15T17:51:16.511068: step 286, loss 0.162382, acc 0.984375\n",
      "2018-03-15T17:51:17.502104: step 287, loss 0.155572, acc 0.992188\n",
      "2018-03-15T17:51:18.493789: step 288, loss 0.138805, acc 1\n",
      "2018-03-15T17:51:19.486724: step 289, loss 0.150451, acc 0.992188\n",
      "2018-03-15T17:51:20.473305: step 290, loss 0.138537, acc 1\n",
      "2018-03-15T17:51:21.463586: step 291, loss 0.161858, acc 0.984375\n",
      "2018-03-15T17:51:22.486326: step 292, loss 0.148271, acc 1\n",
      "2018-03-15T17:51:23.479195: step 293, loss 0.165293, acc 0.984375\n",
      "2018-03-15T17:51:24.446553: step 294, loss 0.154102, acc 0.992188\n",
      "2018-03-15T17:51:25.403243: step 295, loss 0.13917, acc 1\n",
      "2018-03-15T17:51:26.369051: step 296, loss 0.201555, acc 0.976562\n",
      "2018-03-15T17:51:27.327230: step 297, loss 0.138651, acc 1\n",
      "2018-03-15T17:51:28.288518: step 298, loss 0.190626, acc 0.992188\n",
      "2018-03-15T17:51:29.269313: step 299, loss 0.14807, acc 0.992188\n",
      "2018-03-15T17:51:30.316365: step 300, loss 0.146517, acc 0.992188\n",
      "\n",
      "Evaluation:\n",
      "2018-03-15T17:51:32.646014: step 300, loss 0.248552, acc 0.979821\n",
      "\n",
      "Saved model checkpoint to /Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel/checkpoints/model-300\n",
      "\n",
      "2018-03-15T17:51:33.729986: step 301, loss 0.140504, acc 1\n",
      "2018-03-15T17:51:34.694241: step 302, loss 0.151692, acc 0.984375\n",
      "2018-03-15T17:51:35.650211: step 303, loss 0.169984, acc 0.976562\n",
      "2018-03-15T17:51:36.615121: step 304, loss 0.142974, acc 1\n",
      "2018-03-15T17:51:37.576427: step 305, loss 0.156635, acc 0.992188\n",
      "2018-03-15T17:51:38.529189: step 306, loss 0.158572, acc 0.992188\n",
      "2018-03-15T17:51:39.495993: step 307, loss 0.153763, acc 0.984375\n",
      "2018-03-15T17:51:40.341275: step 308, loss 0.171368, acc 0.982143\n",
      "2018-03-15T17:51:41.305898: step 309, loss 0.1474, acc 1\n",
      "2018-03-15T17:51:42.256303: step 310, loss 0.148179, acc 0.992188\n",
      "2018-03-15T17:51:43.209873: step 311, loss 0.159013, acc 0.992188\n",
      "2018-03-15T17:51:44.350545: step 312, loss 0.142259, acc 0.992188\n",
      "2018-03-15T17:51:45.453396: step 313, loss 0.151703, acc 0.992188\n",
      "2018-03-15T17:51:46.449545: step 314, loss 0.132148, acc 1\n",
      "2018-03-15T17:51:47.455455: step 315, loss 0.131992, acc 1\n",
      "2018-03-15T17:51:48.539534: step 316, loss 0.167085, acc 0.984375\n",
      "2018-03-15T17:51:49.530133: step 317, loss 0.138553, acc 1\n",
      "2018-03-15T17:51:50.619127: step 318, loss 0.135412, acc 1\n",
      "2018-03-15T17:51:51.702342: step 319, loss 0.193466, acc 0.976562\n",
      "2018-03-15T17:51:52.700262: step 320, loss 0.148018, acc 0.984375\n",
      "2018-03-15T17:51:53.788425: step 321, loss 0.139391, acc 1\n",
      "2018-03-15T17:51:54.777642: step 322, loss 0.130105, acc 1\n",
      "2018-03-15T17:51:55.735780: step 323, loss 0.134135, acc 1\n",
      "2018-03-15T17:51:56.714626: step 324, loss 0.144192, acc 0.992188\n",
      "2018-03-15T17:51:57.676685: step 325, loss 0.143506, acc 0.992188\n",
      "2018-03-15T17:51:58.653648: step 326, loss 0.159495, acc 0.984375\n",
      "2018-03-15T17:51:59.621640: step 327, loss 0.152302, acc 0.992188\n",
      "2018-03-15T17:52:00.584506: step 328, loss 0.147556, acc 0.984375\n",
      "2018-03-15T17:52:01.600988: step 329, loss 0.141046, acc 0.992188\n",
      "2018-03-15T17:52:02.630890: step 330, loss 0.151714, acc 0.992188\n",
      "2018-03-15T17:52:03.639856: step 331, loss 0.127346, acc 1\n",
      "2018-03-15T17:52:04.673601: step 332, loss 0.157979, acc 0.984375\n",
      "2018-03-15T17:52:05.749839: step 333, loss 0.13445, acc 0.992188\n",
      "2018-03-15T17:52:06.833758: step 334, loss 0.146232, acc 0.984375\n",
      "2018-03-15T17:52:07.915549: step 335, loss 0.161918, acc 0.984375\n",
      "2018-03-15T17:52:08.844088: step 336, loss 0.126648, acc 1\n",
      "2018-03-15T17:52:09.921942: step 337, loss 0.145328, acc 0.992188\n",
      "2018-03-15T17:52:10.973368: step 338, loss 0.163345, acc 0.992188\n",
      "2018-03-15T17:52:12.001342: step 339, loss 0.135093, acc 1\n",
      "2018-03-15T17:52:13.035492: step 340, loss 0.187475, acc 0.984375\n",
      "2018-03-15T17:52:14.174032: step 341, loss 0.131874, acc 1\n",
      "2018-03-15T17:52:15.211211: step 342, loss 0.13073, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-15T17:52:16.250851: step 343, loss 0.133276, acc 1\n",
      "2018-03-15T17:52:17.300527: step 344, loss 0.128649, acc 1\n",
      "2018-03-15T17:52:18.428066: step 345, loss 0.12953, acc 1\n",
      "2018-03-15T17:52:19.503304: step 346, loss 0.128673, acc 1\n",
      "2018-03-15T17:52:20.576532: step 347, loss 0.125813, acc 1\n",
      "2018-03-15T17:52:21.577730: step 348, loss 0.151708, acc 0.984375\n",
      "2018-03-15T17:52:22.585581: step 349, loss 0.129339, acc 1\n",
      "2018-03-15T17:52:23.646870: step 350, loss 0.136685, acc 0.992188\n",
      "2018-03-15T17:52:24.639136: step 351, loss 0.141615, acc 0.992188\n",
      "2018-03-15T17:52:25.677102: step 352, loss 0.146848, acc 0.992188\n",
      "2018-03-15T17:52:26.685365: step 353, loss 0.131489, acc 1\n",
      "2018-03-15T17:52:27.675339: step 354, loss 0.13246, acc 0.992188\n",
      "2018-03-15T17:52:28.663471: step 355, loss 0.125443, acc 1\n",
      "2018-03-15T17:52:29.656338: step 356, loss 0.122022, acc 1\n",
      "2018-03-15T17:52:30.653042: step 357, loss 0.14591, acc 0.992188\n",
      "2018-03-15T17:52:31.631935: step 358, loss 0.134307, acc 1\n",
      "2018-03-15T17:52:32.620624: step 359, loss 0.128838, acc 1\n",
      "2018-03-15T17:52:33.595102: step 360, loss 0.125225, acc 1\n",
      "2018-03-15T17:52:34.548247: step 361, loss 0.160529, acc 0.984375\n",
      "2018-03-15T17:52:35.591355: step 362, loss 0.150602, acc 0.992188\n",
      "2018-03-15T17:52:36.668732: step 363, loss 0.143231, acc 0.992188\n",
      "2018-03-15T17:52:37.546889: step 364, loss 0.121183, acc 1\n",
      "2018-03-15T17:52:38.592941: step 365, loss 0.12474, acc 1\n",
      "2018-03-15T17:52:39.630011: step 366, loss 0.12189, acc 1\n",
      "2018-03-15T17:52:40.714083: step 367, loss 0.158542, acc 0.992188\n",
      "2018-03-15T17:52:41.779559: step 368, loss 0.121177, acc 1\n",
      "2018-03-15T17:52:42.807354: step 369, loss 0.123374, acc 1\n",
      "2018-03-15T17:52:43.781684: step 370, loss 0.128259, acc 1\n",
      "2018-03-15T17:52:44.812087: step 371, loss 0.145328, acc 0.984375\n",
      "2018-03-15T17:52:45.851145: step 372, loss 0.118066, acc 1\n",
      "2018-03-15T17:52:46.849259: step 373, loss 0.157269, acc 0.992188\n",
      "2018-03-15T17:52:47.870968: step 374, loss 0.131774, acc 0.992188\n",
      "2018-03-15T17:52:48.888681: step 375, loss 0.129918, acc 0.984375\n",
      "2018-03-15T17:52:49.924378: step 376, loss 0.12049, acc 1\n",
      "2018-03-15T17:52:50.942486: step 377, loss 0.11919, acc 1\n",
      "2018-03-15T17:52:52.054070: step 378, loss 0.153183, acc 0.976562\n",
      "2018-03-15T17:52:53.076212: step 379, loss 0.122183, acc 1\n",
      "2018-03-15T17:52:54.113068: step 380, loss 0.132419, acc 0.984375\n",
      "2018-03-15T17:52:55.152720: step 381, loss 0.119083, acc 1\n",
      "2018-03-15T17:52:56.134408: step 382, loss 0.121344, acc 0.992188\n",
      "2018-03-15T17:52:57.187925: step 383, loss 0.126933, acc 1\n",
      "2018-03-15T17:52:58.172337: step 384, loss 0.124165, acc 1\n",
      "2018-03-15T17:52:59.133812: step 385, loss 0.122636, acc 0.992188\n",
      "2018-03-15T17:53:00.091610: step 386, loss 0.129105, acc 0.992188\n",
      "2018-03-15T17:53:01.070274: step 387, loss 0.149635, acc 0.984375\n",
      "2018-03-15T17:53:02.088082: step 388, loss 0.129417, acc 0.992188\n",
      "2018-03-15T17:53:03.075466: step 389, loss 0.114097, acc 1\n",
      "2018-03-15T17:53:04.115755: step 390, loss 0.127241, acc 0.992188\n",
      "2018-03-15T17:53:05.125311: step 391, loss 0.13473, acc 0.984375\n",
      "2018-03-15T17:53:06.000445: step 392, loss 0.114624, acc 1\n",
      "2018-03-15T17:53:06.973904: step 393, loss 0.140337, acc 0.992188\n",
      "2018-03-15T17:53:07.961551: step 394, loss 0.112621, acc 1\n",
      "2018-03-15T17:53:08.977789: step 395, loss 0.114159, acc 1\n",
      "2018-03-15T17:53:09.945557: step 396, loss 0.134461, acc 0.992188\n",
      "2018-03-15T17:53:10.929152: step 397, loss 0.116371, acc 1\n",
      "2018-03-15T17:53:11.889378: step 398, loss 0.115388, acc 1\n",
      "2018-03-15T17:53:12.973307: step 399, loss 0.138787, acc 0.992188\n",
      "2018-03-15T17:53:14.046721: step 400, loss 0.115933, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-03-15T17:53:16.643436: step 400, loss 0.224916, acc 0.979821\n",
      "\n",
      "Saved model checkpoint to /Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel/checkpoints/model-400\n",
      "\n",
      "2018-03-15T17:53:17.985347: step 401, loss 0.122516, acc 1\n",
      "2018-03-15T17:53:19.105373: step 402, loss 0.140548, acc 0.992188\n",
      "2018-03-15T17:53:20.218619: step 403, loss 0.113134, acc 1\n",
      "2018-03-15T17:53:21.334588: step 404, loss 0.113736, acc 1\n",
      "2018-03-15T17:53:22.470145: step 405, loss 0.114112, acc 1\n",
      "2018-03-15T17:53:23.521926: step 406, loss 0.135718, acc 0.984375\n",
      "2018-03-15T17:53:24.517289: step 407, loss 0.111109, acc 1\n",
      "2018-03-15T17:53:25.547618: step 408, loss 0.115715, acc 0.992188\n",
      "2018-03-15T17:53:26.575609: step 409, loss 0.111701, acc 1\n",
      "2018-03-15T17:53:27.582413: step 410, loss 0.113946, acc 1\n",
      "2018-03-15T17:53:28.614969: step 411, loss 0.142517, acc 0.984375\n",
      "2018-03-15T17:53:29.694116: step 412, loss 0.121573, acc 0.992188\n",
      "2018-03-15T17:53:30.735090: step 413, loss 0.109916, acc 1\n",
      "2018-03-15T17:53:31.744391: step 414, loss 0.119287, acc 0.992188\n",
      "2018-03-15T17:53:32.773732: step 415, loss 0.129945, acc 0.984375\n",
      "2018-03-15T17:53:33.846357: step 416, loss 0.107993, acc 1\n",
      "2018-03-15T17:53:34.882897: step 417, loss 0.109705, acc 1\n",
      "2018-03-15T17:53:35.905308: step 418, loss 0.136181, acc 0.992188\n",
      "2018-03-15T17:53:37.015288: step 419, loss 0.109055, acc 1\n",
      "2018-03-15T17:53:37.972301: step 420, loss 0.117937, acc 0.991071\n",
      "2018-03-15T17:53:38.980660: step 421, loss 0.111111, acc 1\n",
      "2018-03-15T17:53:39.971590: step 422, loss 0.111343, acc 1\n",
      "2018-03-15T17:53:41.008998: step 423, loss 0.109637, acc 1\n",
      "2018-03-15T17:53:42.034901: step 424, loss 0.150243, acc 0.976562\n",
      "2018-03-15T17:53:43.223326: step 425, loss 0.111536, acc 1\n",
      "2018-03-15T17:53:44.301582: step 426, loss 0.143192, acc 0.992188\n",
      "2018-03-15T17:53:45.452668: step 427, loss 0.119703, acc 0.992188\n",
      "2018-03-15T17:53:46.638827: step 428, loss 0.107359, acc 1\n",
      "2018-03-15T17:53:47.742560: step 429, loss 0.108986, acc 1\n",
      "2018-03-15T17:53:48.868897: step 430, loss 0.107377, acc 1\n",
      "2018-03-15T17:53:50.009885: step 431, loss 0.107379, acc 1\n",
      "2018-03-15T17:53:51.109760: step 432, loss 0.123525, acc 0.992188\n",
      "2018-03-15T17:53:52.127744: step 433, loss 0.111255, acc 1\n",
      "2018-03-15T17:53:53.168035: step 434, loss 0.104816, acc 1\n",
      "2018-03-15T17:53:54.243682: step 435, loss 0.117315, acc 0.984375\n",
      "2018-03-15T17:53:55.235825: step 436, loss 0.113055, acc 0.992188\n",
      "2018-03-15T17:53:56.222698: step 437, loss 0.114456, acc 1\n",
      "2018-03-15T17:53:57.198601: step 438, loss 0.108107, acc 1\n",
      "2018-03-15T17:53:58.203489: step 439, loss 0.144117, acc 0.984375\n",
      "2018-03-15T17:53:59.224562: step 440, loss 0.101755, acc 1\n",
      "2018-03-15T17:54:00.282192: step 441, loss 0.127745, acc 0.992188\n",
      "2018-03-15T17:54:01.308743: step 442, loss 0.108406, acc 1\n",
      "2018-03-15T17:54:02.324272: step 443, loss 0.106408, acc 1\n",
      "2018-03-15T17:54:03.361354: step 444, loss 0.104519, acc 1\n",
      "2018-03-15T17:54:04.414654: step 445, loss 0.112363, acc 1\n",
      "2018-03-15T17:54:05.428968: step 446, loss 0.104003, acc 1\n",
      "2018-03-15T17:54:06.613294: step 447, loss 0.112198, acc 0.992188\n",
      "2018-03-15T17:54:07.665814: step 448, loss 0.113122, acc 0.991071\n",
      "2018-03-15T17:54:08.784525: step 449, loss 0.102468, acc 1\n",
      "2018-03-15T17:54:09.807630: step 450, loss 0.131562, acc 0.984375\n",
      "2018-03-15T17:54:10.850070: step 451, loss 0.107638, acc 1\n",
      "2018-03-15T17:54:11.997642: step 452, loss 0.124315, acc 0.992188\n",
      "2018-03-15T17:54:13.089222: step 453, loss 0.111609, acc 0.992188\n",
      "2018-03-15T17:54:14.155124: step 454, loss 0.103639, acc 1\n",
      "2018-03-15T17:54:15.336581: step 455, loss 0.116394, acc 0.992188\n",
      "2018-03-15T17:54:16.373410: step 456, loss 0.114166, acc 1\n",
      "2018-03-15T17:54:17.371047: step 457, loss 0.134779, acc 0.984375\n",
      "2018-03-15T17:54:18.390025: step 458, loss 0.0997145, acc 1\n",
      "2018-03-15T17:54:19.423997: step 459, loss 0.114234, acc 0.992188\n",
      "2018-03-15T17:54:20.474525: step 460, loss 0.1558, acc 0.984375\n",
      "2018-03-15T17:54:21.545400: step 461, loss 0.102637, acc 0.992188\n",
      "2018-03-15T17:54:22.721284: step 462, loss 0.0989317, acc 1\n",
      "2018-03-15T17:54:23.828700: step 463, loss 0.115049, acc 0.992188\n",
      "2018-03-15T17:54:24.861599: step 464, loss 0.111362, acc 0.992188\n",
      "2018-03-15T17:54:25.870220: step 465, loss 0.0969432, acc 1\n",
      "2018-03-15T17:54:26.894456: step 466, loss 0.103457, acc 1\n",
      "2018-03-15T17:54:27.884864: step 467, loss 0.104266, acc 0.992188\n",
      "2018-03-15T17:54:28.905790: step 468, loss 0.0971228, acc 1\n",
      "2018-03-15T17:54:30.097734: step 469, loss 0.100798, acc 1\n",
      "2018-03-15T17:54:31.196605: step 470, loss 0.100833, acc 1\n",
      "2018-03-15T17:54:32.307908: step 471, loss 0.0973033, acc 1\n",
      "2018-03-15T17:54:33.352112: step 472, loss 0.0980187, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-15T17:54:34.430885: step 473, loss 0.104869, acc 0.992188\n",
      "2018-03-15T17:54:35.473833: step 474, loss 0.0985968, acc 1\n",
      "2018-03-15T17:54:36.639961: step 475, loss 0.101305, acc 1\n",
      "2018-03-15T17:54:37.590911: step 476, loss 0.0943907, acc 1\n",
      "2018-03-15T17:54:38.651629: step 477, loss 0.0941423, acc 1\n",
      "2018-03-15T17:54:39.712960: step 478, loss 0.109951, acc 0.992188\n",
      "2018-03-15T17:54:40.919685: step 479, loss 0.0934749, acc 1\n",
      "2018-03-15T17:54:42.098462: step 480, loss 0.10352, acc 1\n",
      "2018-03-15T17:54:43.160217: step 481, loss 0.0962551, acc 1\n",
      "2018-03-15T17:54:44.201648: step 482, loss 0.0968699, acc 1\n",
      "2018-03-15T17:54:45.244851: step 483, loss 0.0985117, acc 1\n",
      "2018-03-15T17:54:46.524977: step 484, loss 0.0947701, acc 1\n",
      "2018-03-15T17:54:47.643117: step 485, loss 0.0988312, acc 0.992188\n",
      "2018-03-15T17:54:48.797995: step 486, loss 0.0944142, acc 1\n",
      "2018-03-15T17:54:49.833843: step 487, loss 0.0940382, acc 1\n",
      "2018-03-15T17:54:50.854279: step 488, loss 0.091814, acc 1\n",
      "2018-03-15T17:54:51.995742: step 489, loss 0.0951267, acc 1\n",
      "2018-03-15T17:54:53.128512: step 490, loss 0.0998463, acc 0.992188\n",
      "2018-03-15T17:54:54.274780: step 491, loss 0.10388, acc 0.992188\n",
      "2018-03-15T17:54:55.306784: step 492, loss 0.0913306, acc 1\n",
      "2018-03-15T17:54:56.336960: step 493, loss 0.0910723, acc 1\n",
      "2018-03-15T17:54:57.436980: step 494, loss 0.149995, acc 0.992188\n",
      "2018-03-15T17:54:58.576269: step 495, loss 0.0929794, acc 1\n",
      "2018-03-15T17:54:59.603883: step 496, loss 0.0894929, acc 1\n",
      "2018-03-15T17:55:00.638905: step 497, loss 0.101911, acc 0.992188\n",
      "2018-03-15T17:55:01.713836: step 498, loss 0.093835, acc 1\n",
      "2018-03-15T17:55:02.922782: step 499, loss 0.115741, acc 0.992188\n",
      "2018-03-15T17:55:04.093436: step 500, loss 0.0962063, acc 0.992188\n",
      "\n",
      "Evaluation:\n",
      "2018-03-15T17:55:06.628605: step 500, loss 0.194696, acc 0.979821\n",
      "\n",
      "Saved model checkpoint to /Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel/checkpoints/model-500\n",
      "\n",
      "2018-03-15T17:55:07.950096: step 501, loss 0.0979436, acc 0.992188\n",
      "2018-03-15T17:55:08.974779: step 502, loss 0.102889, acc 0.992188\n",
      "2018-03-15T17:55:10.052353: step 503, loss 0.0887569, acc 1\n",
      "2018-03-15T17:55:11.040982: step 504, loss 0.0958119, acc 1\n",
      "2018-03-15T17:55:12.174377: step 505, loss 0.0919032, acc 1\n",
      "2018-03-15T17:55:13.298099: step 506, loss 0.0943798, acc 1\n",
      "2018-03-15T17:55:14.407503: step 507, loss 0.0908084, acc 1\n",
      "2018-03-15T17:55:15.602629: step 508, loss 0.0941892, acc 1\n",
      "2018-03-15T17:55:16.694209: step 509, loss 0.0914515, acc 1\n",
      "2018-03-15T17:55:17.691515: step 510, loss 0.116474, acc 0.984375\n",
      "2018-03-15T17:55:18.718370: step 511, loss 0.0872437, acc 1\n",
      "2018-03-15T17:55:19.756919: step 512, loss 0.0889239, acc 1\n",
      "2018-03-15T17:55:20.779771: step 513, loss 0.0864226, acc 1\n",
      "2018-03-15T17:55:21.770470: step 514, loss 0.102269, acc 0.992188\n",
      "2018-03-15T17:55:22.792998: step 515, loss 0.0888813, acc 1\n",
      "2018-03-15T17:55:23.898757: step 516, loss 0.0880306, acc 1\n",
      "2018-03-15T17:55:24.946617: step 517, loss 0.0968014, acc 0.992188\n",
      "2018-03-15T17:55:26.066065: step 518, loss 0.0926134, acc 1\n",
      "2018-03-15T17:55:27.082347: step 519, loss 0.0948131, acc 0.992188\n",
      "2018-03-15T17:55:28.203301: step 520, loss 0.0909616, acc 1\n",
      "2018-03-15T17:55:29.259442: step 521, loss 0.0887995, acc 1\n",
      "2018-03-15T17:55:30.322620: step 522, loss 0.0999349, acc 0.992188\n",
      "2018-03-15T17:55:31.362015: step 523, loss 0.0857628, acc 1\n",
      "2018-03-15T17:55:32.545368: step 524, loss 0.0876521, acc 1\n",
      "2018-03-15T17:55:33.620585: step 525, loss 0.0975794, acc 0.992188\n",
      "2018-03-15T17:55:34.760591: step 526, loss 0.0992353, acc 0.992188\n",
      "2018-03-15T17:55:35.883166: step 527, loss 0.0886956, acc 1\n",
      "2018-03-15T17:55:37.137376: step 528, loss 0.0872528, acc 1\n",
      "2018-03-15T17:55:38.304457: step 529, loss 0.088592, acc 1\n",
      "2018-03-15T17:55:39.410045: step 530, loss 0.0880694, acc 1\n",
      "2018-03-15T17:55:40.482799: step 531, loss 0.0852932, acc 1\n",
      "2018-03-15T17:55:41.741152: step 532, loss 0.0887861, acc 1\n",
      "2018-03-15T17:55:43.005685: step 533, loss 0.0844251, acc 1\n",
      "2018-03-15T17:55:44.071356: step 534, loss 0.0824611, acc 1\n",
      "2018-03-15T17:55:45.103081: step 535, loss 0.0820942, acc 1\n",
      "2018-03-15T17:55:46.107815: step 536, loss 0.0851279, acc 1\n",
      "2018-03-15T17:55:47.121630: step 537, loss 0.0863441, acc 1\n",
      "2018-03-15T17:55:48.208292: step 538, loss 0.0911742, acc 0.992188\n",
      "2018-03-15T17:55:49.290808: step 539, loss 0.0876428, acc 1\n",
      "2018-03-15T17:55:50.365332: step 540, loss 0.0834551, acc 1\n",
      "2018-03-15T17:55:51.376830: step 541, loss 0.0927434, acc 0.992188\n",
      "2018-03-15T17:55:52.425086: step 542, loss 0.0817288, acc 1\n",
      "2018-03-15T17:55:53.587154: step 543, loss 0.0796936, acc 1\n",
      "2018-03-15T17:55:54.609717: step 544, loss 0.0892458, acc 1\n",
      "2018-03-15T17:55:55.608574: step 545, loss 0.0815266, acc 1\n",
      "2018-03-15T17:55:56.638872: step 546, loss 0.0876638, acc 1\n",
      "2018-03-15T17:55:57.621030: step 547, loss 0.0864881, acc 1\n",
      "2018-03-15T17:55:58.586490: step 548, loss 0.0837071, acc 1\n",
      "2018-03-15T17:55:59.565221: step 549, loss 0.0799789, acc 1\n",
      "2018-03-15T17:56:00.536728: step 550, loss 0.079983, acc 1\n",
      "2018-03-15T17:56:01.536235: step 551, loss 0.0906482, acc 0.984375\n",
      "2018-03-15T17:56:02.545165: step 552, loss 0.0883616, acc 0.992188\n",
      "2018-03-15T17:56:03.541944: step 553, loss 0.0905583, acc 0.992188\n",
      "2018-03-15T17:56:04.546288: step 554, loss 0.0941368, acc 0.992188\n",
      "2018-03-15T17:56:05.557790: step 555, loss 0.0796941, acc 1\n",
      "2018-03-15T17:56:06.571732: step 556, loss 0.132577, acc 0.976562\n",
      "2018-03-15T17:56:07.607152: step 557, loss 0.0785614, acc 1\n",
      "2018-03-15T17:56:08.631757: step 558, loss 0.0782019, acc 1\n",
      "2018-03-15T17:56:09.593496: step 559, loss 0.0803619, acc 1\n",
      "2018-03-15T17:56:10.427948: step 560, loss 0.0876805, acc 0.991071\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "      log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            num_classes=y_train.shape[1],\n",
    "            vocab_size=len(vocab_processor.vocabulary_),\n",
    "            embedding_size=FLAGS.embedding_dim,\n",
    "            filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "            num_filters=FLAGS.num_filters,\n",
    "            l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "\n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", \"cnnmodel\"))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "\n",
    "        # Write vocabulary\n",
    "        vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, accuracy = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "\n",
    "        # Generate batches\n",
    "        batches = data_helpers.batch_iter(\n",
    "            list(zip(x_train, y_train)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % FLAGS.evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % FLAGS.checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testDataDir = \"/Users/SamZhang/Documents/Capstone/dataset/small/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.flags.DEFINE_string(\"positive_test_data_file\", testDataDir + \"/spam/SMS_test.spam\", \"Data source for the positive data.\")\n",
    "tf.flags.DEFINE_string(\"negative_test_data_file\", testDataDir + \"/ham/SMS_test.ham\", \"Data source for the negative data.\")\n",
    "\n",
    "# Eval Parameters\n",
    "tf.flags.DEFINE_string(\"checkpoint_dir\", \"/Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel/checkpoints/\", \"Checkpoint directory from training run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "BATCH_SIZE=128\n",
      "CHECKPOINT_DIR=/Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel/checkpoints/\n",
      "CHECKPOINT_EVERY=100\n",
      "DEV_SAMPLE_PERCENTAGE=0.2\n",
      "DROPOUT_KEEP_PROB=0.5\n",
      "EMBEDDING_DIM=128\n",
      "EVALUATE_EVERY=100\n",
      "FILTER_SIZES=3,4,5\n",
      "L2_REG_LAMBDA=0.1\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "NEGATIVE_DATA_FILE=/Users/SamZhang/Documents/Capstone/dataset/small/train/ham/SMS_train.ham\n",
      "NEGATIVE_TEST_DATA_FILE=/Users/SamZhang/Documents/Capstone/dataset/small/test/ham/SMS_test.ham\n",
      "NUM_CHECKPOINTS=5\n",
      "NUM_EPOCHS=20\n",
      "NUM_FILTERS=256\n",
      "POSITIVE_DATA_FILE=/Users/SamZhang/Documents/Capstone/dataset/small/train/spam/SMS_train.spam\n",
      "POSITIVE_TEST_DATA_FILE=/Users/SamZhang/Documents/Capstone/dataset/small/test/spam/SMS_test.spam\n",
      "\n"
     ]
    }
   ],
   "source": [
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "six chances to win cash ! from 100 to 20 , 000 pounds txt csh11 and send to 87575 cost 150p day , 6days , 16 tsandcs apply reply hl 4 info 1\n"
     ]
    }
   ],
   "source": [
    "x_raw, y_test = data_helpers.load_data_and_labels(FLAGS.positive_test_data_file, FLAGS.negative_test_data_file)\n",
    "y_test = np.argmax(y_test, axis=1) #ham = 0, spam = 1\n",
    "print(x_raw[0], y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_path = os.path.join(FLAGS.checkpoint_dir, \"..\", \"vocab\")\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor.restore(vocab_path)\n",
    "x_test = np.array(list(vocab_processor.transform(x_raw)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_file = tf.train.latest_checkpoint(FLAGS.checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel/checkpoints/model-500\n",
      "Total number of test examples: 1114\n",
      "Accuracy: 0.964991\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "      log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        # Load the saved meta graph and restore variables\n",
    "        saver = tf.train.import_meta_graph(\"{}.meta\".format(checkpoint_file))\n",
    "        saver.restore(sess, checkpoint_file)\n",
    "\n",
    "        # Get the placeholders from the graph by name\n",
    "        input_x = graph.get_operation_by_name(\"input_x\").outputs[0]\n",
    "        # input_y = graph.get_operation_by_name(\"input_y\").outputs[0]\n",
    "        dropout_keep_prob = graph.get_operation_by_name(\"dropout_keep_prob\").outputs[0]\n",
    "\n",
    "        # Tensors we want to evaluate\n",
    "        predictions = graph.get_operation_by_name(\"output/predictions\").outputs[0]\n",
    "\n",
    "        # Generate batches for one epoch\n",
    "        batches = data_helpers.batch_iter(list(x_test), FLAGS.batch_size, 1, shuffle=False)\n",
    "\n",
    "        # Collect the predictions here\n",
    "        all_predictions = []\n",
    "\n",
    "        for x_test_batch in batches:\n",
    "            batch_predictions = sess.run(predictions, {input_x: x_test_batch, dropout_keep_prob: 1.0})\n",
    "            all_predictions = np.concatenate([all_predictions, batch_predictions])\n",
    "\n",
    "# Print accuracy if y_test is defined\n",
    "if y_test is not None:\n",
    "    correct_predictions = float(sum(all_predictions == y_test))\n",
    "    print(\"Total number of test examples: {}\".format(len(y_test)))\n",
    "    print(\"Accuracy: {:g}\".format(correct_predictions/float(len(y_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving evaluation to /Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel/checkpoints/../prediction.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the evaluation to a csv\n",
    "title = np.column_stack(('text', 'prediction', 'label'))\n",
    "predictions_human_readable = np.column_stack((np.array(x_raw), all_predictions, y_test))\n",
    "out_path = os.path.join(FLAGS.checkpoint_dir, \"..\", \"prediction.csv\")\n",
    "print(\"Saving evaluation to {0}\".format(out_path))\n",
    "\n",
    "with open(out_path, 'w') as f:\n",
    "    csv.writer(f).writerows(title)\n",
    "    csv.writer(f).writerows(predictions_human_readable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
