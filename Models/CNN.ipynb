{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import data_helpers\n",
    "from text_cnn import TextCNN\n",
    "from tensorflow.contrib import learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = 'sms'\n",
    "trainingSection = 9\n",
    "\n",
    "trainDataDir = '/Users/SamZhang/Documents/Capstone/dataset/' + dataset + '/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading params\n",
    "tf.flags.DEFINE_float(\"dev_sample_percentage\", .2, \"Percentage of the training data to use for validation\")\n",
    "tf.flags.DEFINE_string(\"positive_data_file\", trainDataDir + \"/spam/\" + str(trainingSection) + '/' + dataset + \"_train.spam\", \"Data source for the positive data.\")\n",
    "tf.flags.DEFINE_string(\"negative_data_file\", trainDataDir + \"/ham/\" + str(trainingSection) + '/' + dataset + \"_train.ham\", \"Data source for the negative data.\")\n",
    "\n",
    "# Model Hyperparameters\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 256, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.1, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 128, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 40, \"Number of training epochs (default: 200)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 50, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 50, \"Save model after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "BATCH_SIZE=128\n",
      "CHECKPOINT_EVERY=50\n",
      "DEV_SAMPLE_PERCENTAGE=0.2\n",
      "DROPOUT_KEEP_PROB=0.5\n",
      "EMBEDDING_DIM=128\n",
      "EVALUATE_EVERY=50\n",
      "FILTER_SIZES=3,4,5\n",
      "L2_REG_LAMBDA=0.1\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "NEGATIVE_DATA_FILE=/Users/SamZhang/Documents/Capstone/dataset/sms/train/ham/9/sms_train.ham\n",
      "NUM_CHECKPOINTS=5\n",
      "NUM_EPOCHS=40\n",
      "NUM_FILTERS=256\n",
      "POSITIVE_DATA_FILE=/Users/SamZhang/Documents/Capstone/dataset/sms/train/spam/9/sms_train.spam\n",
      "\n"
     ]
    }
   ],
   "source": [
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def processData():\n",
    "    # Data Preparation\n",
    "    # ==================================================\n",
    "\n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    x_text, y = data_helpers.load_data_and_labels(FLAGS.positive_data_file, FLAGS.negative_data_file)\n",
    "    print(x_text[0])\n",
    "\n",
    "    # Build vocabulary\n",
    "    max_document_length = max([len(x.split(\" \")) for x in x_text])\n",
    "    vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "    x = np.array(list(vocab_processor.fit_transform(x_text)))\n",
    "\n",
    "    # Randomly shuffle data\n",
    "    np.random.seed(10)\n",
    "    shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "    x_shuffled = x[shuffle_indices]\n",
    "    y_shuffled = y[shuffle_indices]\n",
    "\n",
    "    # Split train/test set\n",
    "    # TODO: This is very crude, should use cross-validation\n",
    "    dev_sample_index = -1 * int(FLAGS.dev_sample_percentage * float(len(y)))\n",
    "    x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "    y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "\n",
    "    del x, y, x_shuffled, y_shuffled\n",
    "\n",
    "    print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
    "    print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))\n",
    "    \n",
    "    return x_train, y_train, x_dev, y_dev, vocab_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry question \\( std txt rate \\) t c 's apply 08452810075over18 's\n",
      "Vocabulary Size: 8329\n",
      "Train/Dev split: 4015/1003\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_dev, y_dev, vocab_processor = processData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to /Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel/9\n",
      "\n",
      "2018-03-24T23:43:50.142482: step 1, loss 1.86903, acc 0.507812\n",
      "2018-03-24T23:43:51.468521: step 2, loss 0.975611, acc 0.835938\n",
      "2018-03-24T23:43:52.620503: step 3, loss 0.897121, acc 0.867188\n",
      "2018-03-24T23:43:53.793866: step 4, loss 1.39545, acc 0.867188\n",
      "2018-03-24T23:43:54.998632: step 5, loss 1.15251, acc 0.859375\n",
      "2018-03-24T23:43:56.210049: step 6, loss 0.744085, acc 0.867188\n",
      "2018-03-24T23:43:57.402523: step 7, loss 0.877534, acc 0.84375\n",
      "2018-03-24T23:43:58.614906: step 8, loss 1.00379, acc 0.71875\n",
      "2018-03-24T23:43:59.935514: step 9, loss 1.04025, acc 0.757812\n",
      "2018-03-24T23:44:01.255215: step 10, loss 0.728139, acc 0.851562\n",
      "2018-03-24T23:44:02.570701: step 11, loss 0.76252, acc 0.804688\n",
      "2018-03-24T23:44:03.890181: step 12, loss 0.788789, acc 0.828125\n",
      "2018-03-24T23:44:05.222946: step 13, loss 0.829327, acc 0.820312\n",
      "2018-03-24T23:44:06.576181: step 14, loss 0.668065, acc 0.828125\n",
      "2018-03-24T23:44:07.946482: step 15, loss 1.33234, acc 0.8125\n",
      "2018-03-24T23:44:09.296821: step 16, loss 0.724748, acc 0.882812\n",
      "2018-03-24T23:44:10.617032: step 17, loss 0.932557, acc 0.828125\n",
      "2018-03-24T23:44:11.955913: step 18, loss 0.900197, acc 0.820312\n",
      "2018-03-24T23:44:13.343662: step 19, loss 0.70909, acc 0.875\n",
      "2018-03-24T23:44:14.691839: step 20, loss 0.974665, acc 0.851562\n",
      "2018-03-24T23:44:16.019334: step 21, loss 0.881419, acc 0.828125\n",
      "2018-03-24T23:44:17.441018: step 22, loss 0.938493, acc 0.828125\n",
      "2018-03-24T23:44:18.951126: step 23, loss 0.435504, acc 0.921875\n",
      "2018-03-24T23:44:20.335030: step 24, loss 0.775155, acc 0.890625\n",
      "2018-03-24T23:44:21.904596: step 25, loss 0.649441, acc 0.890625\n",
      "2018-03-24T23:44:23.433169: step 26, loss 0.866184, acc 0.835938\n",
      "2018-03-24T23:44:25.031160: step 27, loss 0.646463, acc 0.914062\n",
      "2018-03-24T23:44:26.601459: step 28, loss 0.678275, acc 0.851562\n",
      "2018-03-24T23:44:28.005690: step 29, loss 0.661865, acc 0.859375\n",
      "2018-03-24T23:44:29.446942: step 30, loss 0.688976, acc 0.867188\n",
      "2018-03-24T23:44:30.863248: step 31, loss 0.517536, acc 0.867188\n",
      "2018-03-24T23:44:31.402498: step 32, loss 0.393359, acc 0.957447\n",
      "2018-03-24T23:44:32.954540: step 33, loss 0.629083, acc 0.875\n",
      "2018-03-24T23:44:34.431387: step 34, loss 0.72333, acc 0.898438\n",
      "2018-03-24T23:44:35.951401: step 35, loss 0.598895, acc 0.890625\n",
      "2018-03-24T23:44:37.354069: step 36, loss 0.446544, acc 0.953125\n",
      "2018-03-24T23:44:38.708208: step 37, loss 0.334332, acc 0.960938\n",
      "2018-03-24T23:44:40.058568: step 38, loss 0.523423, acc 0.90625\n",
      "2018-03-24T23:44:41.458823: step 39, loss 0.493232, acc 0.921875\n",
      "2018-03-24T23:44:42.822544: step 40, loss 0.665722, acc 0.867188\n",
      "2018-03-24T23:44:44.168337: step 41, loss 0.807478, acc 0.859375\n",
      "2018-03-24T23:44:45.520678: step 42, loss 0.413055, acc 0.921875\n",
      "2018-03-24T23:44:46.909183: step 43, loss 0.479238, acc 0.890625\n",
      "2018-03-24T23:44:48.233044: step 44, loss 0.604471, acc 0.890625\n",
      "2018-03-24T23:44:49.562082: step 45, loss 0.643243, acc 0.90625\n",
      "2018-03-24T23:44:50.925414: step 46, loss 0.42031, acc 0.90625\n",
      "2018-03-24T23:44:52.312774: step 47, loss 0.510298, acc 0.890625\n",
      "2018-03-24T23:44:53.643497: step 48, loss 0.42803, acc 0.914062\n",
      "2018-03-24T23:44:55.024861: step 49, loss 0.508862, acc 0.898438\n",
      "2018-03-24T23:44:56.422225: step 50, loss 0.376651, acc 0.929688\n",
      "\n",
      "Evaluation:\n",
      "2018-03-24T23:45:00.232468: step 50, loss 0.279894, acc 0.967099\n",
      "\n",
      "Saved model checkpoint to /Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel/9/checkpoints/model-50\n",
      "\n",
      "2018-03-24T23:45:01.715361: step 51, loss 0.42679, acc 0.929688\n",
      "2018-03-24T23:45:03.097546: step 52, loss 0.381498, acc 0.945312\n",
      "2018-03-24T23:45:04.443563: step 53, loss 0.271345, acc 0.96875\n",
      "2018-03-24T23:45:05.818327: step 54, loss 0.534935, acc 0.914062\n",
      "2018-03-24T23:45:07.162198: step 55, loss 0.319458, acc 0.960938\n",
      "2018-03-24T23:45:08.507148: step 56, loss 0.396019, acc 0.953125\n",
      "2018-03-24T23:45:09.842959: step 57, loss 0.379944, acc 0.945312\n",
      "2018-03-24T23:45:11.149301: step 58, loss 0.365773, acc 0.945312\n",
      "2018-03-24T23:45:12.482904: step 59, loss 0.578525, acc 0.90625\n",
      "2018-03-24T23:45:13.813529: step 60, loss 0.565158, acc 0.898438\n",
      "2018-03-24T23:45:15.133449: step 61, loss 0.38136, acc 0.9375\n",
      "2018-03-24T23:45:16.466397: step 62, loss 0.287259, acc 0.960938\n",
      "2018-03-24T23:45:17.846323: step 63, loss 0.458679, acc 0.921875\n",
      "2018-03-24T23:45:18.359754: step 64, loss 0.42357, acc 0.93617\n",
      "2018-03-24T23:45:19.722301: step 65, loss 0.488013, acc 0.914062\n",
      "2018-03-24T23:45:21.049569: step 66, loss 0.264076, acc 0.960938\n",
      "2018-03-24T23:45:22.416044: step 67, loss 0.275137, acc 0.96875\n",
      "2018-03-24T23:45:23.804991: step 68, loss 0.317339, acc 0.96875\n",
      "2018-03-24T23:45:25.160335: step 69, loss 0.30802, acc 0.96875\n",
      "2018-03-24T23:45:26.654362: step 70, loss 0.319822, acc 0.96875\n",
      "2018-03-24T23:45:28.142375: step 71, loss 0.189356, acc 0.992188\n",
      "2018-03-24T23:45:29.475526: step 72, loss 0.215471, acc 0.976562\n",
      "2018-03-24T23:45:30.789426: step 73, loss 0.446069, acc 0.9375\n",
      "2018-03-24T23:45:32.111026: step 74, loss 0.33512, acc 0.945312\n",
      "2018-03-24T23:45:33.433072: step 75, loss 0.366061, acc 0.960938\n",
      "2018-03-24T23:45:34.759211: step 76, loss 0.336689, acc 0.953125\n",
      "2018-03-24T23:45:36.097613: step 77, loss 0.352989, acc 0.945312\n",
      "2018-03-24T23:45:37.462154: step 78, loss 0.425929, acc 0.953125\n",
      "2018-03-24T23:45:38.817784: step 79, loss 0.29226, acc 0.96875\n",
      "2018-03-24T23:45:40.204163: step 80, loss 0.282333, acc 0.953125\n",
      "2018-03-24T23:45:41.583258: step 81, loss 0.269249, acc 0.960938\n",
      "2018-03-24T23:45:42.956167: step 82, loss 0.306319, acc 0.953125\n",
      "2018-03-24T23:45:44.307788: step 83, loss 0.403436, acc 0.921875\n",
      "2018-03-24T23:45:45.646788: step 84, loss 0.312154, acc 0.96875\n",
      "2018-03-24T23:45:47.013906: step 85, loss 0.281379, acc 0.960938\n",
      "2018-03-24T23:45:48.414552: step 86, loss 0.244751, acc 0.960938\n",
      "2018-03-24T23:45:49.802325: step 87, loss 0.265711, acc 0.984375\n",
      "2018-03-24T23:45:51.176786: step 88, loss 0.357798, acc 0.945312\n",
      "2018-03-24T23:45:52.512627: step 89, loss 0.362834, acc 0.945312\n",
      "2018-03-24T23:45:53.870676: step 90, loss 0.284946, acc 0.96875\n",
      "2018-03-24T23:45:55.217652: step 91, loss 0.415684, acc 0.914062\n",
      "2018-03-24T23:45:56.557109: step 92, loss 0.202385, acc 0.992188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-24T23:45:57.855019: step 93, loss 0.405636, acc 0.9375\n",
      "2018-03-24T23:45:59.135790: step 94, loss 0.261327, acc 0.960938\n",
      "2018-03-24T23:46:00.409862: step 95, loss 0.234004, acc 0.96875\n",
      "2018-03-24T23:46:00.913277: step 96, loss 0.424342, acc 0.93617\n",
      "2018-03-24T23:46:02.197266: step 97, loss 0.236675, acc 0.953125\n",
      "2018-03-24T23:46:03.448166: step 98, loss 0.212948, acc 0.976562\n",
      "2018-03-24T23:46:04.757600: step 99, loss 0.338097, acc 0.9375\n",
      "2018-03-24T23:46:06.049658: step 100, loss 0.221785, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2018-03-24T23:46:09.648881: step 100, loss 0.241753, acc 0.976072\n",
      "\n",
      "Saved model checkpoint to /Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel/9/checkpoints/model-100\n",
      "\n",
      "2018-03-24T23:46:11.298381: step 101, loss 0.304848, acc 0.953125\n",
      "2018-03-24T23:46:12.699866: step 102, loss 0.21859, acc 0.96875\n",
      "2018-03-24T23:46:14.040606: step 103, loss 0.225417, acc 0.976562\n",
      "2018-03-24T23:46:15.387891: step 104, loss 0.223786, acc 0.976562\n",
      "2018-03-24T23:46:16.745303: step 105, loss 0.219816, acc 0.984375\n",
      "2018-03-24T23:46:18.202457: step 106, loss 0.267718, acc 0.960938\n",
      "2018-03-24T23:46:19.605286: step 107, loss 0.22212, acc 0.976562\n",
      "2018-03-24T23:46:21.012107: step 108, loss 0.263861, acc 0.953125\n",
      "2018-03-24T23:46:22.390768: step 109, loss 0.240489, acc 0.984375\n",
      "2018-03-24T23:46:23.771033: step 110, loss 0.244032, acc 0.96875\n",
      "2018-03-24T23:46:25.167110: step 111, loss 0.360313, acc 0.9375\n",
      "2018-03-24T23:46:26.536766: step 112, loss 0.316343, acc 0.9375\n",
      "2018-03-24T23:46:27.861894: step 113, loss 0.297953, acc 0.96875\n",
      "2018-03-24T23:46:29.171705: step 114, loss 0.321417, acc 0.960938\n",
      "2018-03-24T23:46:30.489550: step 115, loss 0.275096, acc 0.945312\n",
      "2018-03-24T23:46:31.806561: step 116, loss 0.344836, acc 0.945312\n",
      "2018-03-24T23:46:33.177423: step 117, loss 0.248435, acc 0.960938\n",
      "2018-03-24T23:46:34.519421: step 118, loss 0.211485, acc 0.984375\n",
      "2018-03-24T23:46:35.868606: step 119, loss 0.266162, acc 0.953125\n",
      "2018-03-24T23:46:37.201717: step 120, loss 0.285137, acc 0.953125\n",
      "2018-03-24T23:46:38.540603: step 121, loss 0.230775, acc 0.960938\n",
      "2018-03-24T23:46:39.891579: step 122, loss 0.21019, acc 0.976562\n",
      "2018-03-24T23:46:41.264885: step 123, loss 0.373005, acc 0.9375\n",
      "2018-03-24T23:46:42.617145: step 124, loss 0.288799, acc 0.960938\n",
      "2018-03-24T23:46:43.963960: step 125, loss 0.220476, acc 0.96875\n",
      "2018-03-24T23:46:45.311842: step 126, loss 0.193273, acc 0.992188\n",
      "2018-03-24T23:46:46.656105: step 127, loss 0.225515, acc 0.96875\n",
      "2018-03-24T23:46:47.185644: step 128, loss 0.195283, acc 0.978723\n",
      "2018-03-24T23:46:48.677873: step 129, loss 0.185429, acc 0.992188\n",
      "2018-03-24T23:46:50.078842: step 130, loss 0.198041, acc 0.976562\n",
      "2018-03-24T23:46:51.493227: step 131, loss 0.219132, acc 0.96875\n",
      "2018-03-24T23:46:52.883725: step 132, loss 0.239837, acc 0.953125\n",
      "2018-03-24T23:46:54.259677: step 133, loss 0.209392, acc 0.96875\n",
      "2018-03-24T23:46:55.623810: step 134, loss 0.18524, acc 0.984375\n",
      "2018-03-24T23:46:56.938181: step 135, loss 0.23811, acc 0.976562\n",
      "2018-03-24T23:46:58.272900: step 136, loss 0.181473, acc 0.992188\n",
      "2018-03-24T23:46:59.611779: step 137, loss 0.276495, acc 0.96875\n",
      "2018-03-24T23:47:00.989868: step 138, loss 0.251229, acc 0.976562\n",
      "2018-03-24T23:47:02.383966: step 139, loss 0.205612, acc 0.976562\n",
      "2018-03-24T23:47:03.771415: step 140, loss 0.185906, acc 0.992188\n",
      "2018-03-24T23:47:05.158241: step 141, loss 0.190298, acc 0.992188\n",
      "2018-03-24T23:47:06.546647: step 142, loss 0.198305, acc 0.992188\n",
      "2018-03-24T23:47:07.951836: step 143, loss 0.237035, acc 0.976562\n",
      "2018-03-24T23:47:09.362561: step 144, loss 0.27323, acc 0.953125\n",
      "2018-03-24T23:47:10.793701: step 145, loss 0.226118, acc 0.960938\n",
      "2018-03-24T23:47:12.192392: step 146, loss 0.264716, acc 0.960938\n",
      "2018-03-24T23:47:13.572110: step 147, loss 0.24911, acc 0.960938\n",
      "2018-03-24T23:47:14.989209: step 148, loss 0.208686, acc 0.96875\n",
      "2018-03-24T23:47:16.389732: step 149, loss 0.281896, acc 0.960938\n",
      "2018-03-24T23:47:17.815837: step 150, loss 0.215314, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2018-03-24T23:47:21.292394: step 150, loss 0.254157, acc 0.968096\n",
      "\n",
      "Saved model checkpoint to /Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel/9/checkpoints/model-150\n",
      "\n",
      "2018-03-24T23:47:22.871033: step 151, loss 0.175006, acc 0.984375\n",
      "2018-03-24T23:47:24.263365: step 152, loss 0.187255, acc 0.984375\n",
      "2018-03-24T23:47:25.676393: step 153, loss 0.221625, acc 0.96875\n",
      "2018-03-24T23:47:27.064900: step 154, loss 0.179183, acc 0.984375\n",
      "2018-03-24T23:47:28.437135: step 155, loss 0.180186, acc 0.992188\n",
      "2018-03-24T23:47:29.846588: step 156, loss 0.202658, acc 0.976562\n",
      "2018-03-24T23:47:31.282121: step 157, loss 0.205821, acc 0.976562\n",
      "2018-03-24T23:47:32.711165: step 158, loss 0.204286, acc 0.976562\n",
      "2018-03-24T23:47:34.155691: step 159, loss 0.296753, acc 0.960938\n",
      "2018-03-24T23:47:34.703253: step 160, loss 0.214388, acc 0.978723\n",
      "2018-03-24T23:47:36.149051: step 161, loss 0.166434, acc 0.992188\n",
      "2018-03-24T23:47:37.534599: step 162, loss 0.149773, acc 1\n",
      "2018-03-24T23:47:38.954430: step 163, loss 0.219433, acc 0.984375\n",
      "2018-03-24T23:47:40.385578: step 164, loss 0.212179, acc 0.976562\n",
      "2018-03-24T23:47:41.809753: step 165, loss 0.248433, acc 0.96875\n",
      "2018-03-24T23:47:43.212976: step 166, loss 0.330351, acc 0.960938\n",
      "2018-03-24T23:47:44.597295: step 167, loss 0.188417, acc 0.984375\n",
      "2018-03-24T23:47:45.996902: step 168, loss 0.21003, acc 0.976562\n",
      "2018-03-24T23:47:47.384025: step 169, loss 0.160495, acc 1\n",
      "2018-03-24T23:47:48.866271: step 170, loss 0.190529, acc 0.96875\n",
      "2018-03-24T23:47:50.304618: step 171, loss 0.17262, acc 0.984375\n",
      "2018-03-24T23:47:51.719237: step 172, loss 0.166086, acc 0.992188\n",
      "2018-03-24T23:47:53.128955: step 173, loss 0.159214, acc 0.992188\n",
      "2018-03-24T23:47:54.541192: step 174, loss 0.174903, acc 0.992188\n",
      "2018-03-24T23:47:55.967464: step 175, loss 0.339781, acc 0.945312\n",
      "2018-03-24T23:47:57.379567: step 176, loss 0.200312, acc 0.984375\n",
      "2018-03-24T23:47:58.751617: step 177, loss 0.195135, acc 0.984375\n",
      "2018-03-24T23:48:00.126586: step 178, loss 0.249502, acc 0.96875\n",
      "2018-03-24T23:48:01.534346: step 179, loss 0.154046, acc 1\n",
      "2018-03-24T23:48:02.935925: step 180, loss 0.152625, acc 1\n",
      "2018-03-24T23:48:04.313047: step 181, loss 0.226075, acc 0.960938\n",
      "2018-03-24T23:48:05.724999: step 182, loss 0.176837, acc 0.992188\n",
      "2018-03-24T23:48:07.128619: step 183, loss 0.172862, acc 0.976562\n",
      "2018-03-24T23:48:08.542158: step 184, loss 0.168394, acc 0.984375\n",
      "2018-03-24T23:48:09.940160: step 185, loss 0.146867, acc 1\n",
      "2018-03-24T23:48:11.373507: step 186, loss 0.166228, acc 0.992188\n",
      "2018-03-24T23:48:12.778246: step 187, loss 0.193962, acc 0.992188\n",
      "2018-03-24T23:48:14.181497: step 188, loss 0.178852, acc 0.984375\n",
      "2018-03-24T23:48:15.582487: step 189, loss 0.148772, acc 1\n",
      "2018-03-24T23:48:16.982232: step 190, loss 0.1487, acc 1\n",
      "2018-03-24T23:48:18.355616: step 191, loss 0.16351, acc 0.984375\n",
      "2018-03-24T23:48:18.892919: step 192, loss 0.187764, acc 0.978723\n",
      "2018-03-24T23:48:20.388807: step 193, loss 0.231903, acc 0.96875\n",
      "2018-03-24T23:48:21.784057: step 194, loss 0.166178, acc 0.984375\n",
      "2018-03-24T23:48:23.170764: step 195, loss 0.172009, acc 0.984375\n",
      "2018-03-24T23:48:24.560379: step 196, loss 0.16353, acc 0.992188\n",
      "2018-03-24T23:48:26.011996: step 197, loss 0.166583, acc 0.992188\n",
      "2018-03-24T23:48:27.406714: step 198, loss 0.175417, acc 0.984375\n",
      "2018-03-24T23:48:28.827369: step 199, loss 0.171704, acc 0.96875\n",
      "2018-03-24T23:48:30.253734: step 200, loss 0.20601, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2018-03-24T23:48:33.660015: step 200, loss 0.22106, acc 0.978066\n",
      "\n",
      "Saved model checkpoint to /Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel/9/checkpoints/model-200\n",
      "\n",
      "2018-03-24T23:48:35.225713: step 201, loss 0.188855, acc 0.984375\n",
      "2018-03-24T23:48:36.643876: step 202, loss 0.201506, acc 0.96875\n",
      "2018-03-24T23:48:38.046636: step 203, loss 0.167999, acc 0.984375\n",
      "2018-03-24T23:48:39.434871: step 204, loss 0.183185, acc 0.96875\n",
      "2018-03-24T23:48:40.826319: step 205, loss 0.183267, acc 0.96875\n",
      "2018-03-24T23:48:42.220279: step 206, loss 0.191453, acc 0.992188\n",
      "2018-03-24T23:48:43.611594: step 207, loss 0.159484, acc 0.992188\n",
      "2018-03-24T23:48:45.013611: step 208, loss 0.17366, acc 0.984375\n",
      "2018-03-24T23:48:46.438958: step 209, loss 0.193479, acc 0.984375\n",
      "2018-03-24T23:48:47.848504: step 210, loss 0.138169, acc 0.992188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-24T23:48:49.253050: step 211, loss 0.152739, acc 0.992188\n",
      "2018-03-24T23:48:50.717991: step 212, loss 0.172618, acc 0.984375\n",
      "2018-03-24T23:48:52.157394: step 213, loss 0.164128, acc 0.992188\n",
      "2018-03-24T23:48:53.557570: step 214, loss 0.1557, acc 0.992188\n",
      "2018-03-24T23:48:54.999811: step 215, loss 0.14963, acc 0.992188\n",
      "2018-03-24T23:48:56.510192: step 216, loss 0.151243, acc 0.992188\n",
      "2018-03-24T23:48:58.082469: step 217, loss 0.158456, acc 0.984375\n",
      "2018-03-24T23:48:59.584826: step 218, loss 0.138427, acc 1\n",
      "2018-03-24T23:49:01.105911: step 219, loss 0.250942, acc 0.96875\n",
      "2018-03-24T23:49:02.539201: step 220, loss 0.137984, acc 1\n",
      "2018-03-24T23:49:03.979014: step 221, loss 0.149076, acc 0.984375\n",
      "2018-03-24T23:49:05.447007: step 222, loss 0.175291, acc 0.984375\n",
      "2018-03-24T23:49:06.878472: step 223, loss 0.135023, acc 1\n",
      "2018-03-24T23:49:07.456174: step 224, loss 0.126711, acc 1\n",
      "2018-03-24T23:49:08.900778: step 225, loss 0.193423, acc 0.96875\n",
      "2018-03-24T23:49:10.399528: step 226, loss 0.159721, acc 0.984375\n",
      "2018-03-24T23:49:11.877218: step 227, loss 0.134583, acc 0.992188\n",
      "2018-03-24T23:49:13.307441: step 228, loss 0.132829, acc 1\n",
      "2018-03-24T23:49:14.746457: step 229, loss 0.150402, acc 0.984375\n",
      "2018-03-24T23:49:16.161000: step 230, loss 0.166935, acc 0.984375\n",
      "2018-03-24T23:49:17.544506: step 231, loss 0.159835, acc 0.976562\n",
      "2018-03-24T23:49:19.001424: step 232, loss 0.132891, acc 0.992188\n",
      "2018-03-24T23:49:20.511011: step 233, loss 0.144192, acc 0.992188\n",
      "2018-03-24T23:49:22.006089: step 234, loss 0.173816, acc 0.976562\n",
      "2018-03-24T23:49:23.490280: step 235, loss 0.151961, acc 0.984375\n",
      "2018-03-24T23:49:24.964864: step 236, loss 0.184895, acc 0.96875\n",
      "2018-03-24T23:49:26.475443: step 237, loss 0.135178, acc 0.992188\n",
      "2018-03-24T23:49:27.980413: step 238, loss 0.171834, acc 0.96875\n",
      "2018-03-24T23:49:29.440034: step 239, loss 0.131276, acc 0.992188\n",
      "2018-03-24T23:49:30.914236: step 240, loss 0.140143, acc 0.992188\n",
      "2018-03-24T23:49:32.416482: step 241, loss 0.17379, acc 0.976562\n",
      "2018-03-24T23:49:33.897958: step 242, loss 0.126281, acc 1\n",
      "2018-03-24T23:49:35.320003: step 243, loss 0.193037, acc 0.976562\n",
      "2018-03-24T23:49:36.743746: step 244, loss 0.151301, acc 0.992188\n",
      "2018-03-24T23:49:38.163324: step 245, loss 0.14253, acc 0.992188\n",
      "2018-03-24T23:49:39.586530: step 246, loss 0.135691, acc 0.992188\n",
      "2018-03-24T23:49:41.001658: step 247, loss 0.129693, acc 1\n",
      "2018-03-24T23:49:42.404573: step 248, loss 0.218152, acc 0.976562\n",
      "2018-03-24T23:49:43.807267: step 249, loss 0.127214, acc 1\n",
      "2018-03-24T23:49:45.204000: step 250, loss 0.120983, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-03-24T23:49:48.641371: step 250, loss 0.197979, acc 0.978066\n",
      "\n",
      "Saved model checkpoint to /Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel/9/checkpoints/model-250\n",
      "\n",
      "2018-03-24T23:49:50.291307: step 251, loss 0.145435, acc 0.984375\n",
      "2018-03-24T23:49:51.721692: step 252, loss 0.15813, acc 0.992188\n",
      "2018-03-24T23:49:53.139540: step 253, loss 0.14337, acc 0.992188\n",
      "2018-03-24T23:49:54.543273: step 254, loss 0.222531, acc 0.984375\n",
      "2018-03-24T23:49:56.014320: step 255, loss 0.130643, acc 0.992188\n",
      "2018-03-24T23:49:56.569299: step 256, loss 0.127119, acc 1\n",
      "2018-03-24T23:49:57.977308: step 257, loss 0.117303, acc 1\n",
      "2018-03-24T23:49:59.396045: step 258, loss 0.14565, acc 0.984375\n",
      "2018-03-24T23:50:00.801990: step 259, loss 0.128464, acc 0.992188\n",
      "2018-03-24T23:50:02.209597: step 260, loss 0.145802, acc 0.984375\n",
      "2018-03-24T23:50:03.612402: step 261, loss 0.119149, acc 1\n",
      "2018-03-24T23:50:05.046099: step 262, loss 0.127713, acc 0.992188\n",
      "2018-03-24T23:50:06.458598: step 263, loss 0.134531, acc 0.992188\n",
      "2018-03-24T23:50:07.850116: step 264, loss 0.13243, acc 0.992188\n",
      "2018-03-24T23:50:09.242653: step 265, loss 0.142383, acc 0.976562\n",
      "2018-03-24T23:50:10.677738: step 266, loss 0.16346, acc 0.984375\n",
      "2018-03-24T23:50:12.072223: step 267, loss 0.124756, acc 0.992188\n",
      "2018-03-24T23:50:13.473792: step 268, loss 0.123363, acc 1\n",
      "2018-03-24T23:50:14.884430: step 269, loss 0.136158, acc 0.992188\n",
      "2018-03-24T23:50:16.286457: step 270, loss 0.122897, acc 0.992188\n",
      "2018-03-24T23:50:17.693294: step 271, loss 0.127972, acc 0.992188\n",
      "2018-03-24T23:50:19.091247: step 272, loss 0.12127, acc 0.992188\n",
      "2018-03-24T23:50:20.506330: step 273, loss 0.148234, acc 0.984375\n",
      "2018-03-24T23:50:21.968019: step 274, loss 0.137321, acc 0.992188\n",
      "2018-03-24T23:50:23.368930: step 275, loss 0.119081, acc 1\n",
      "2018-03-24T23:50:24.769229: step 276, loss 0.181676, acc 0.96875\n",
      "2018-03-24T23:50:26.340537: step 277, loss 0.114137, acc 1\n",
      "2018-03-24T23:50:27.920560: step 278, loss 0.152488, acc 0.984375\n",
      "2018-03-24T23:50:29.455587: step 279, loss 0.11881, acc 1\n",
      "2018-03-24T23:50:31.000175: step 280, loss 0.117634, acc 1\n",
      "2018-03-24T23:50:32.429112: step 281, loss 0.141298, acc 0.984375\n",
      "2018-03-24T23:50:33.885206: step 282, loss 0.141533, acc 0.992188\n",
      "2018-03-24T23:50:35.348942: step 283, loss 0.129727, acc 0.992188\n",
      "2018-03-24T23:50:36.769174: step 284, loss 0.118403, acc 1\n",
      "2018-03-24T23:50:38.201172: step 285, loss 0.115997, acc 0.992188\n",
      "2018-03-24T23:50:39.680930: step 286, loss 0.114722, acc 1\n",
      "2018-03-24T23:50:41.133821: step 287, loss 0.118935, acc 1\n",
      "2018-03-24T23:50:41.744297: step 288, loss 0.145999, acc 0.978723\n",
      "2018-03-24T23:50:43.200282: step 289, loss 0.122787, acc 0.992188\n",
      "2018-03-24T23:50:44.637289: step 290, loss 0.117739, acc 1\n",
      "2018-03-24T23:50:46.076758: step 291, loss 0.134294, acc 0.984375\n",
      "2018-03-24T23:50:47.529233: step 292, loss 0.114368, acc 1\n",
      "2018-03-24T23:50:48.990793: step 293, loss 0.11186, acc 1\n",
      "2018-03-24T23:50:50.430577: step 294, loss 0.118386, acc 1\n",
      "2018-03-24T23:50:51.936643: step 295, loss 0.181145, acc 0.984375\n",
      "2018-03-24T23:50:53.369357: step 296, loss 0.128957, acc 0.992188\n",
      "2018-03-24T23:50:54.917229: step 297, loss 0.157053, acc 0.96875\n",
      "2018-03-24T23:50:56.463024: step 298, loss 0.15059, acc 0.984375\n",
      "2018-03-24T23:50:57.953087: step 299, loss 0.121292, acc 0.992188\n",
      "2018-03-24T23:50:59.427110: step 300, loss 0.107965, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-03-24T23:51:03.111812: step 300, loss 0.192032, acc 0.976072\n",
      "\n",
      "Saved model checkpoint to /Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel/9/checkpoints/model-300\n",
      "\n",
      "2018-03-24T23:51:04.735842: step 301, loss 0.12239, acc 0.992188\n",
      "2018-03-24T23:51:06.183348: step 302, loss 0.108472, acc 1\n",
      "2018-03-24T23:51:07.631052: step 303, loss 0.109536, acc 1\n",
      "2018-03-24T23:51:09.033183: step 304, loss 0.13238, acc 0.976562\n",
      "2018-03-24T23:51:10.443605: step 305, loss 0.158742, acc 0.960938\n",
      "2018-03-24T23:51:11.914063: step 306, loss 0.106763, acc 1\n",
      "2018-03-24T23:51:13.353548: step 307, loss 0.117865, acc 0.992188\n",
      "2018-03-24T23:51:14.782393: step 308, loss 0.124964, acc 0.984375\n",
      "2018-03-24T23:51:16.227444: step 309, loss 0.10385, acc 1\n",
      "2018-03-24T23:51:17.658490: step 310, loss 0.126844, acc 0.992188\n",
      "2018-03-24T23:51:19.077561: step 311, loss 0.106586, acc 1\n",
      "2018-03-24T23:51:20.549790: step 312, loss 0.111453, acc 0.992188\n",
      "2018-03-24T23:51:22.085381: step 313, loss 0.12814, acc 0.984375\n",
      "2018-03-24T23:51:23.575603: step 314, loss 0.126088, acc 0.984375\n",
      "2018-03-24T23:51:25.038827: step 315, loss 0.10812, acc 1\n",
      "2018-03-24T23:51:26.476465: step 316, loss 0.114169, acc 0.984375\n",
      "2018-03-24T23:51:27.904314: step 317, loss 0.116623, acc 1\n",
      "2018-03-24T23:51:29.328014: step 318, loss 0.142937, acc 0.976562\n",
      "2018-03-24T23:51:30.734421: step 319, loss 0.0998392, acc 1\n",
      "2018-03-24T23:51:31.277052: step 320, loss 0.111181, acc 1\n",
      "2018-03-24T23:51:32.711958: step 321, loss 0.11193, acc 1\n",
      "2018-03-24T23:51:34.097978: step 322, loss 0.10769, acc 1\n",
      "2018-03-24T23:51:35.505531: step 323, loss 0.101544, acc 1\n",
      "2018-03-24T23:51:36.924316: step 324, loss 0.0992467, acc 1\n",
      "2018-03-24T23:51:38.328265: step 325, loss 0.116765, acc 0.992188\n",
      "2018-03-24T23:51:39.814229: step 326, loss 0.145289, acc 0.984375\n",
      "2018-03-24T23:51:41.266921: step 327, loss 0.103974, acc 1\n",
      "2018-03-24T23:51:42.706073: step 328, loss 0.154405, acc 0.976562\n",
      "2018-03-24T23:51:44.097851: step 329, loss 0.0978265, acc 1\n",
      "2018-03-24T23:51:45.530649: step 330, loss 0.123695, acc 0.992188\n",
      "2018-03-24T23:51:46.938616: step 331, loss 0.117139, acc 0.992188\n",
      "2018-03-24T23:51:48.344359: step 332, loss 0.0990904, acc 1\n",
      "2018-03-24T23:51:49.836109: step 333, loss 0.105358, acc 0.992188\n",
      "2018-03-24T23:51:51.276803: step 334, loss 0.127872, acc 0.984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-24T23:51:52.750565: step 335, loss 0.104654, acc 1\n",
      "2018-03-24T23:51:54.161392: step 336, loss 0.114674, acc 0.992188\n",
      "2018-03-24T23:51:55.605860: step 337, loss 0.155392, acc 0.984375\n",
      "2018-03-24T23:51:57.031420: step 338, loss 0.105027, acc 0.992188\n",
      "2018-03-24T23:51:58.475618: step 339, loss 0.105424, acc 0.992188\n",
      "2018-03-24T23:51:59.925058: step 340, loss 0.125963, acc 0.992188\n",
      "2018-03-24T23:52:01.325110: step 341, loss 0.097188, acc 1\n",
      "2018-03-24T23:52:02.708959: step 342, loss 0.105961, acc 0.992188\n",
      "2018-03-24T23:52:04.080418: step 343, loss 0.102101, acc 0.992188\n",
      "2018-03-24T23:52:05.481032: step 344, loss 0.140011, acc 0.984375\n",
      "2018-03-24T23:52:06.857952: step 345, loss 0.102184, acc 0.992188\n",
      "2018-03-24T23:52:08.289555: step 346, loss 0.093546, acc 1\n",
      "2018-03-24T23:52:09.702097: step 347, loss 0.128583, acc 0.984375\n",
      "2018-03-24T23:52:11.120179: step 348, loss 0.0963412, acc 1\n",
      "2018-03-24T23:52:12.524519: step 349, loss 0.102474, acc 1\n",
      "2018-03-24T23:52:13.971606: step 350, loss 0.149523, acc 0.976562\n",
      "\n",
      "Evaluation:\n",
      "2018-03-24T23:52:17.485363: step 350, loss 0.159707, acc 0.98006\n",
      "\n",
      "Saved model checkpoint to /Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel/9/checkpoints/model-350\n",
      "\n",
      "2018-03-24T23:52:19.036440: step 351, loss 0.103071, acc 0.992188\n",
      "2018-03-24T23:52:19.605574: step 352, loss 0.0921807, acc 1\n",
      "2018-03-24T23:52:21.004975: step 353, loss 0.100209, acc 0.992188\n",
      "2018-03-24T23:52:22.456986: step 354, loss 0.100909, acc 0.992188\n",
      "2018-03-24T23:52:23.848369: step 355, loss 0.111094, acc 0.992188\n",
      "2018-03-24T23:52:25.262101: step 356, loss 0.0946552, acc 1\n",
      "2018-03-24T23:52:26.641012: step 357, loss 0.102661, acc 0.992188\n",
      "2018-03-24T23:52:28.091657: step 358, loss 0.0979969, acc 1\n",
      "2018-03-24T23:52:29.506854: step 359, loss 0.098262, acc 0.992188\n",
      "2018-03-24T23:52:31.017610: step 360, loss 0.0929978, acc 1\n",
      "2018-03-24T23:52:32.680875: step 361, loss 0.09486, acc 0.992188\n",
      "2018-03-24T23:52:34.246615: step 362, loss 0.0956151, acc 1\n",
      "2018-03-24T23:52:35.897734: step 363, loss 0.101889, acc 0.992188\n",
      "2018-03-24T23:52:37.454092: step 364, loss 0.100729, acc 0.984375\n",
      "2018-03-24T23:52:38.953365: step 365, loss 0.0955165, acc 1\n",
      "2018-03-24T23:52:40.356298: step 366, loss 0.138205, acc 0.992188\n",
      "2018-03-24T23:52:41.746286: step 367, loss 0.112034, acc 0.984375\n",
      "2018-03-24T23:52:43.120849: step 368, loss 0.0944206, acc 0.992188\n",
      "2018-03-24T23:52:44.525535: step 369, loss 0.0938891, acc 1\n",
      "2018-03-24T23:52:46.084833: step 370, loss 0.0960709, acc 0.992188\n",
      "2018-03-24T23:52:47.670948: step 371, loss 0.0912511, acc 1\n",
      "2018-03-24T23:52:49.371459: step 372, loss 0.0926813, acc 1\n",
      "2018-03-24T23:52:51.091644: step 373, loss 0.0869552, acc 1\n",
      "2018-03-24T23:52:52.915255: step 374, loss 0.0894875, acc 1\n",
      "2018-03-24T23:52:54.653310: step 375, loss 0.0888829, acc 1\n",
      "2018-03-24T23:52:56.350273: step 376, loss 0.0863296, acc 1\n",
      "2018-03-24T23:52:57.843360: step 377, loss 0.095201, acc 1\n",
      "2018-03-24T23:52:59.343240: step 378, loss 0.105337, acc 0.984375\n",
      "2018-03-24T23:53:00.930094: step 379, loss 0.0837193, acc 1\n",
      "2018-03-24T23:53:02.451650: step 380, loss 0.0977818, acc 0.992188\n",
      "2018-03-24T23:53:04.112360: step 381, loss 0.0861375, acc 1\n",
      "2018-03-24T23:53:05.712351: step 382, loss 0.0927383, acc 1\n",
      "2018-03-24T23:53:07.387063: step 383, loss 0.10313, acc 0.992188\n",
      "2018-03-24T23:53:07.998307: step 384, loss 0.101035, acc 1\n",
      "2018-03-24T23:53:09.629828: step 385, loss 0.0861059, acc 1\n",
      "2018-03-24T23:53:11.194666: step 386, loss 0.0871219, acc 1\n",
      "2018-03-24T23:53:12.917191: step 387, loss 0.0824066, acc 1\n",
      "2018-03-24T23:53:14.522465: step 388, loss 0.0849959, acc 1\n",
      "2018-03-24T23:53:16.337883: step 389, loss 0.0852827, acc 1\n",
      "2018-03-24T23:53:18.020859: step 390, loss 0.0964085, acc 0.992188\n",
      "2018-03-24T23:53:19.547823: step 391, loss 0.124967, acc 0.984375\n",
      "2018-03-24T23:53:21.070514: step 392, loss 0.11079, acc 0.992188\n",
      "2018-03-24T23:53:22.539354: step 393, loss 0.0809554, acc 1\n",
      "2018-03-24T23:53:24.062955: step 394, loss 0.0827203, acc 1\n",
      "2018-03-24T23:53:25.530110: step 395, loss 0.108707, acc 0.976562\n",
      "2018-03-24T23:53:26.966757: step 396, loss 0.0891261, acc 0.992188\n",
      "2018-03-24T23:53:28.441773: step 397, loss 0.117808, acc 0.976562\n",
      "2018-03-24T23:53:29.891068: step 398, loss 0.0914975, acc 1\n",
      "2018-03-24T23:53:31.343480: step 399, loss 0.0893157, acc 0.992188\n",
      "2018-03-24T23:53:32.806788: step 400, loss 0.108854, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2018-03-24T23:53:36.372933: step 400, loss 0.137178, acc 0.984048\n",
      "\n",
      "Saved model checkpoint to /Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel/9/checkpoints/model-400\n",
      "\n",
      "2018-03-24T23:53:37.945698: step 401, loss 0.0879592, acc 1\n",
      "2018-03-24T23:53:39.386128: step 402, loss 0.0818743, acc 1\n",
      "2018-03-24T23:53:40.847725: step 403, loss 0.0925962, acc 0.992188\n",
      "2018-03-24T23:53:42.282843: step 404, loss 0.103485, acc 0.984375\n",
      "2018-03-24T23:53:43.879471: step 405, loss 0.0995363, acc 0.984375\n",
      "2018-03-24T23:53:45.535763: step 406, loss 0.0830131, acc 1\n",
      "2018-03-24T23:53:47.056383: step 407, loss 0.0827054, acc 1\n",
      "2018-03-24T23:53:48.605205: step 408, loss 0.0828144, acc 1\n",
      "2018-03-24T23:53:50.132047: step 409, loss 0.0852641, acc 0.992188\n",
      "2018-03-24T23:53:51.711974: step 410, loss 0.0851067, acc 1\n",
      "2018-03-24T23:53:53.284714: step 411, loss 0.0886663, acc 0.992188\n",
      "2018-03-24T23:53:54.838453: step 412, loss 0.0844633, acc 0.992188\n",
      "2018-03-24T23:53:56.345650: step 413, loss 0.0778609, acc 1\n",
      "2018-03-24T23:53:57.819548: step 414, loss 0.0783966, acc 1\n",
      "2018-03-24T23:53:59.305956: step 415, loss 0.0905241, acc 0.992188\n",
      "2018-03-24T23:54:00.037745: step 416, loss 0.0780655, acc 1\n",
      "2018-03-24T23:54:01.858299: step 417, loss 0.0776006, acc 1\n",
      "2018-03-24T23:54:03.399890: step 418, loss 0.075282, acc 1\n",
      "2018-03-24T23:54:04.962885: step 419, loss 0.0784387, acc 1\n",
      "2018-03-24T23:54:06.555411: step 420, loss 0.0885522, acc 0.992188\n",
      "2018-03-24T23:54:08.146802: step 421, loss 0.0995327, acc 0.992188\n",
      "2018-03-24T23:54:09.621836: step 422, loss 0.0854192, acc 0.992188\n",
      "2018-03-24T23:54:11.076676: step 423, loss 0.0925453, acc 0.992188\n",
      "2018-03-24T23:54:12.530744: step 424, loss 0.0794521, acc 1\n",
      "2018-03-24T23:54:14.006019: step 425, loss 0.078411, acc 1\n",
      "2018-03-24T23:54:15.456297: step 426, loss 0.0752448, acc 1\n",
      "2018-03-24T23:54:16.906702: step 427, loss 0.101923, acc 0.984375\n",
      "2018-03-24T23:54:18.373409: step 428, loss 0.0926551, acc 0.992188\n",
      "2018-03-24T23:54:19.918978: step 429, loss 0.0723924, acc 1\n",
      "2018-03-24T23:54:21.377329: step 430, loss 0.0902359, acc 0.992188\n",
      "2018-03-24T23:54:22.907808: step 431, loss 0.0805251, acc 1\n",
      "2018-03-24T23:54:24.433105: step 432, loss 0.0817, acc 0.992188\n",
      "2018-03-24T23:54:25.942099: step 433, loss 0.0712637, acc 1\n",
      "2018-03-24T23:54:27.457134: step 434, loss 0.0880416, acc 0.992188\n",
      "2018-03-24T23:54:28.929388: step 435, loss 0.0706822, acc 1\n",
      "2018-03-24T23:54:30.386705: step 436, loss 0.105194, acc 0.984375\n",
      "2018-03-24T23:54:31.834500: step 437, loss 0.100831, acc 0.992188\n",
      "2018-03-24T23:54:33.300230: step 438, loss 0.0847512, acc 0.992188\n",
      "2018-03-24T23:54:34.690091: step 439, loss 0.0762134, acc 1\n",
      "2018-03-24T23:54:36.122568: step 440, loss 0.101979, acc 0.992188\n",
      "2018-03-24T23:54:37.577566: step 441, loss 0.072168, acc 1\n",
      "2018-03-24T23:54:39.040371: step 442, loss 0.0984439, acc 0.984375\n",
      "2018-03-24T23:54:40.559297: step 443, loss 0.0838405, acc 0.992188\n",
      "2018-03-24T23:54:42.042337: step 444, loss 0.103219, acc 0.976562\n",
      "2018-03-24T23:54:43.519922: step 445, loss 0.0721031, acc 1\n",
      "2018-03-24T23:54:45.010607: step 446, loss 0.110788, acc 0.984375\n",
      "2018-03-24T23:54:46.489923: step 447, loss 0.0710891, acc 1\n",
      "2018-03-24T23:54:47.081264: step 448, loss 0.0864451, acc 1\n",
      "2018-03-24T23:54:48.591150: step 449, loss 0.0750157, acc 1\n",
      "2018-03-24T23:54:50.055110: step 450, loss 0.0701478, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-03-24T23:54:53.624890: step 450, loss 0.12423, acc 0.983051\n",
      "\n",
      "Saved model checkpoint to /Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel/9/checkpoints/model-450\n",
      "\n",
      "2018-03-24T23:54:55.277520: step 451, loss 0.08643, acc 0.984375\n",
      "2018-03-24T23:54:56.764116: step 452, loss 0.0812484, acc 0.992188\n",
      "2018-03-24T23:54:58.188963: step 453, loss 0.079042, acc 0.992188\n",
      "2018-03-24T23:54:59.576804: step 454, loss 0.0767472, acc 1\n",
      "2018-03-24T23:55:01.018837: step 455, loss 0.0855437, acc 0.992188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-24T23:55:02.449222: step 456, loss 0.101612, acc 0.992188\n",
      "2018-03-24T23:55:03.871154: step 457, loss 0.0667234, acc 1\n",
      "2018-03-24T23:55:05.306472: step 458, loss 0.0683279, acc 1\n",
      "2018-03-24T23:55:06.735177: step 459, loss 0.0795865, acc 0.992188\n",
      "2018-03-24T23:55:08.168222: step 460, loss 0.0687777, acc 1\n",
      "2018-03-24T23:55:09.606675: step 461, loss 0.0841455, acc 0.992188\n",
      "2018-03-24T23:55:11.088712: step 462, loss 0.0692338, acc 1\n",
      "2018-03-24T23:55:12.669963: step 463, loss 0.0933503, acc 0.984375\n",
      "2018-03-24T23:55:14.217849: step 464, loss 0.064266, acc 1\n",
      "2018-03-24T23:55:15.744561: step 465, loss 0.0735775, acc 1\n",
      "2018-03-24T23:55:17.285070: step 466, loss 0.0683816, acc 1\n",
      "2018-03-24T23:55:18.810407: step 467, loss 0.0679728, acc 1\n",
      "2018-03-24T23:55:20.560463: step 468, loss 0.0656657, acc 1\n",
      "2018-03-24T23:55:22.055557: step 469, loss 0.0760533, acc 1\n",
      "2018-03-24T23:55:23.527490: step 470, loss 0.0745917, acc 0.992188\n",
      "2018-03-24T23:55:25.010365: step 471, loss 0.0776616, acc 1\n",
      "2018-03-24T23:55:26.704620: step 472, loss 0.0860578, acc 0.992188\n",
      "2018-03-24T23:55:28.145844: step 473, loss 0.0691143, acc 1\n",
      "2018-03-24T23:55:29.666456: step 474, loss 0.066804, acc 1\n",
      "2018-03-24T23:55:31.136412: step 475, loss 0.067315, acc 1\n",
      "2018-03-24T23:55:32.589652: step 476, loss 0.0671743, acc 1\n",
      "2018-03-24T23:55:33.998378: step 477, loss 0.0680501, acc 1\n",
      "2018-03-24T23:55:35.432264: step 478, loss 0.0655021, acc 1\n",
      "2018-03-24T23:55:36.930080: step 479, loss 0.0648052, acc 1\n",
      "2018-03-24T23:55:37.472381: step 480, loss 0.072049, acc 1\n",
      "2018-03-24T23:55:38.862254: step 481, loss 0.060982, acc 1\n",
      "2018-03-24T23:55:40.292138: step 482, loss 0.0655185, acc 1\n",
      "2018-03-24T23:55:41.706994: step 483, loss 0.0647887, acc 1\n",
      "2018-03-24T23:55:43.091305: step 484, loss 0.0615867, acc 1\n",
      "2018-03-24T23:55:44.510422: step 485, loss 0.0628884, acc 1\n",
      "2018-03-24T23:55:45.939571: step 486, loss 0.068114, acc 1\n",
      "2018-03-24T23:55:47.362679: step 487, loss 0.0680968, acc 1\n",
      "2018-03-24T23:55:48.799052: step 488, loss 0.0683516, acc 0.992188\n",
      "2018-03-24T23:55:50.264969: step 489, loss 0.0671396, acc 0.992188\n",
      "2018-03-24T23:55:51.788572: step 490, loss 0.0622245, acc 1\n",
      "2018-03-24T23:55:53.151176: step 491, loss 0.0753493, acc 0.984375\n",
      "2018-03-24T23:55:54.571157: step 492, loss 0.063452, acc 1\n",
      "2018-03-24T23:55:55.920368: step 493, loss 0.0633821, acc 1\n",
      "2018-03-24T23:55:57.295572: step 494, loss 0.0764911, acc 0.992188\n",
      "2018-03-24T23:55:58.664830: step 495, loss 0.059313, acc 1\n",
      "2018-03-24T23:56:00.241869: step 496, loss 0.0612098, acc 1\n",
      "2018-03-24T23:56:01.706752: step 497, loss 0.0590027, acc 1\n",
      "2018-03-24T23:56:03.073084: step 498, loss 0.0588132, acc 1\n",
      "2018-03-24T23:56:04.478370: step 499, loss 0.0696572, acc 0.992188\n",
      "2018-03-24T23:56:05.871269: step 500, loss 0.0852349, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2018-03-24T23:56:09.232175: step 500, loss 0.117855, acc 0.982054\n",
      "\n",
      "Saved model checkpoint to /Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel/9/checkpoints/model-500\n",
      "\n",
      "2018-03-24T23:56:10.772433: step 501, loss 0.0627318, acc 1\n",
      "2018-03-24T23:56:12.116908: step 502, loss 0.0631567, acc 1\n",
      "2018-03-24T23:56:13.458008: step 503, loss 0.0661501, acc 0.992188\n",
      "2018-03-24T23:56:14.787028: step 504, loss 0.0588419, acc 1\n",
      "2018-03-24T23:56:16.144747: step 505, loss 0.0606519, acc 1\n",
      "2018-03-24T23:56:17.497692: step 506, loss 0.0614918, acc 1\n",
      "2018-03-24T23:56:18.838242: step 507, loss 0.0653037, acc 1\n",
      "2018-03-24T23:56:20.200252: step 508, loss 0.0681205, acc 1\n",
      "2018-03-24T23:56:21.562664: step 509, loss 0.0633245, acc 1\n",
      "2018-03-24T23:56:22.920923: step 510, loss 0.0673815, acc 0.992188\n",
      "2018-03-24T23:56:24.269660: step 511, loss 0.0570545, acc 1\n",
      "2018-03-24T23:56:24.860047: step 512, loss 0.0569757, acc 1\n",
      "2018-03-24T23:56:26.226999: step 513, loss 0.0616933, acc 0.992188\n",
      "2018-03-24T23:56:27.598447: step 514, loss 0.0735253, acc 0.984375\n",
      "2018-03-24T23:56:28.942145: step 515, loss 0.070562, acc 0.992188\n",
      "2018-03-24T23:56:30.299526: step 516, loss 0.0553828, acc 1\n",
      "2018-03-24T23:56:31.650508: step 517, loss 0.055329, acc 1\n",
      "2018-03-24T23:56:32.991665: step 518, loss 0.0642195, acc 1\n",
      "2018-03-24T23:56:34.336215: step 519, loss 0.0557171, acc 1\n",
      "2018-03-24T23:56:35.683697: step 520, loss 0.0573612, acc 1\n",
      "2018-03-24T23:56:37.042343: step 521, loss 0.0567611, acc 1\n",
      "2018-03-24T23:56:38.411669: step 522, loss 0.0645078, acc 0.992188\n",
      "2018-03-24T23:56:39.760173: step 523, loss 0.0691435, acc 1\n",
      "2018-03-24T23:56:41.127102: step 524, loss 0.0562286, acc 1\n",
      "2018-03-24T23:56:42.498420: step 525, loss 0.0548803, acc 1\n",
      "2018-03-24T23:56:43.856006: step 526, loss 0.0548018, acc 1\n",
      "2018-03-24T23:56:45.214308: step 527, loss 0.0588829, acc 1\n",
      "2018-03-24T23:56:46.564136: step 528, loss 0.0664244, acc 0.992188\n",
      "2018-03-24T23:56:47.901267: step 529, loss 0.0579687, acc 1\n",
      "2018-03-24T23:56:49.260077: step 530, loss 0.0541949, acc 1\n",
      "2018-03-24T23:56:50.626431: step 531, loss 0.0529249, acc 1\n",
      "2018-03-24T23:56:51.994161: step 532, loss 0.0535898, acc 1\n",
      "2018-03-24T23:56:53.370983: step 533, loss 0.054573, acc 1\n",
      "2018-03-24T23:56:54.717453: step 534, loss 0.0529189, acc 1\n",
      "2018-03-24T23:56:56.133859: step 535, loss 0.0593408, acc 1\n",
      "2018-03-24T23:56:57.515539: step 536, loss 0.0588846, acc 1\n",
      "2018-03-24T23:56:58.886177: step 537, loss 0.0522562, acc 1\n",
      "2018-03-24T23:57:00.276688: step 538, loss 0.0599112, acc 0.992188\n",
      "2018-03-24T23:57:01.632233: step 539, loss 0.0568182, acc 1\n",
      "2018-03-24T23:57:03.002089: step 540, loss 0.0526911, acc 1\n",
      "2018-03-24T23:57:04.329250: step 541, loss 0.0569205, acc 1\n",
      "2018-03-24T23:57:05.715901: step 542, loss 0.0614782, acc 0.992188\n",
      "2018-03-24T23:57:07.068557: step 543, loss 0.0589506, acc 1\n",
      "2018-03-24T23:57:07.608337: step 544, loss 0.0508391, acc 1\n",
      "2018-03-24T23:57:08.961422: step 545, loss 0.0538137, acc 1\n",
      "2018-03-24T23:57:10.325212: step 546, loss 0.0545054, acc 1\n",
      "2018-03-24T23:57:11.685230: step 547, loss 0.0494644, acc 1\n",
      "2018-03-24T23:57:13.056092: step 548, loss 0.0518605, acc 1\n",
      "2018-03-24T23:57:14.430182: step 549, loss 0.055888, acc 0.992188\n",
      "2018-03-24T23:57:15.809219: step 550, loss 0.0529021, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-03-24T23:57:19.136276: step 550, loss 0.126083, acc 0.977069\n",
      "\n",
      "Saved model checkpoint to /Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel/9/checkpoints/model-550\n",
      "\n",
      "2018-03-24T23:57:20.664033: step 551, loss 0.0553514, acc 1\n",
      "2018-03-24T23:57:22.037239: step 552, loss 0.0783944, acc 0.96875\n",
      "2018-03-24T23:57:23.409743: step 553, loss 0.0489996, acc 1\n",
      "2018-03-24T23:57:24.777422: step 554, loss 0.0499621, acc 1\n",
      "2018-03-24T23:57:26.178227: step 555, loss 0.0544808, acc 0.992188\n",
      "2018-03-24T23:57:27.566854: step 556, loss 0.0556797, acc 1\n",
      "2018-03-24T23:57:28.905967: step 557, loss 0.0691389, acc 0.992188\n",
      "2018-03-24T23:57:30.255410: step 558, loss 0.0510307, acc 1\n",
      "2018-03-24T23:57:31.600998: step 559, loss 0.049345, acc 1\n",
      "2018-03-24T23:57:32.955162: step 560, loss 0.0808177, acc 0.984375\n",
      "2018-03-24T23:57:34.315993: step 561, loss 0.0524471, acc 1\n",
      "2018-03-24T23:57:35.680409: step 562, loss 0.0614977, acc 0.992188\n",
      "2018-03-24T23:57:37.030124: step 563, loss 0.0525358, acc 1\n",
      "2018-03-24T23:57:38.362143: step 564, loss 0.0594181, acc 0.992188\n",
      "2018-03-24T23:57:39.706912: step 565, loss 0.0618476, acc 0.992188\n",
      "2018-03-24T23:57:41.060000: step 566, loss 0.0601259, acc 1\n",
      "2018-03-24T23:57:42.414970: step 567, loss 0.0714557, acc 0.992188\n",
      "2018-03-24T23:57:43.760171: step 568, loss 0.0488586, acc 1\n",
      "2018-03-24T23:57:45.103420: step 569, loss 0.0493388, acc 1\n",
      "2018-03-24T23:57:46.501116: step 570, loss 0.0495556, acc 1\n",
      "2018-03-24T23:57:47.859088: step 571, loss 0.0489096, acc 1\n",
      "2018-03-24T23:57:49.264652: step 572, loss 0.0492813, acc 1\n",
      "2018-03-24T23:57:50.629134: step 573, loss 0.0451793, acc 1\n",
      "2018-03-24T23:57:52.035265: step 574, loss 0.0506058, acc 1\n",
      "2018-03-24T23:57:53.396033: step 575, loss 0.0519569, acc 1\n",
      "2018-03-24T23:57:53.919592: step 576, loss 0.0469569, acc 1\n",
      "2018-03-24T23:57:55.304194: step 577, loss 0.0594184, acc 0.984375\n",
      "2018-03-24T23:57:56.713772: step 578, loss 0.0651028, acc 0.984375\n",
      "2018-03-24T23:57:58.085506: step 579, loss 0.0534224, acc 0.992188\n",
      "2018-03-24T23:57:59.448204: step 580, loss 0.0487067, acc 1\n",
      "2018-03-24T23:58:00.820857: step 581, loss 0.0583554, acc 0.992188\n",
      "2018-03-24T23:58:02.173892: step 582, loss 0.0493344, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-24T23:58:03.536890: step 583, loss 0.0490652, acc 1\n",
      "2018-03-24T23:58:04.863835: step 584, loss 0.0556186, acc 1\n",
      "2018-03-24T23:58:06.254535: step 585, loss 0.047074, acc 1\n",
      "2018-03-24T23:58:07.641371: step 586, loss 0.0468399, acc 1\n",
      "2018-03-24T23:58:09.013948: step 587, loss 0.0464365, acc 1\n",
      "2018-03-24T23:58:10.395982: step 588, loss 0.051115, acc 0.992188\n",
      "2018-03-24T23:58:11.795335: step 589, loss 0.0486601, acc 1\n",
      "2018-03-24T23:58:13.169840: step 590, loss 0.049056, acc 1\n",
      "2018-03-24T23:58:14.554982: step 591, loss 0.0505049, acc 1\n",
      "2018-03-24T23:58:16.023597: step 592, loss 0.0479089, acc 1\n",
      "2018-03-24T23:58:17.396569: step 593, loss 0.0425799, acc 1\n",
      "2018-03-24T23:58:18.770237: step 594, loss 0.0438996, acc 1\n",
      "2018-03-24T23:58:20.124846: step 595, loss 0.0436092, acc 1\n",
      "2018-03-24T23:58:21.491368: step 596, loss 0.0496138, acc 0.992188\n",
      "2018-03-24T23:58:22.838947: step 597, loss 0.0494702, acc 1\n",
      "2018-03-24T23:58:24.184668: step 598, loss 0.0475006, acc 1\n",
      "2018-03-24T23:58:25.536489: step 599, loss 0.0566721, acc 0.992188\n",
      "2018-03-24T23:58:26.958915: step 600, loss 0.0565874, acc 0.992188\n",
      "\n",
      "Evaluation:\n",
      "2018-03-24T23:58:30.223443: step 600, loss 0.112826, acc 0.979063\n",
      "\n",
      "Saved model checkpoint to /Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel/9/checkpoints/model-600\n",
      "\n",
      "2018-03-24T23:58:31.746763: step 601, loss 0.0446341, acc 1\n",
      "2018-03-24T23:58:33.129828: step 602, loss 0.0417317, acc 1\n",
      "2018-03-24T23:58:34.505905: step 603, loss 0.0424884, acc 1\n",
      "2018-03-24T23:58:35.873653: step 604, loss 0.0465416, acc 1\n",
      "2018-03-24T23:58:37.226914: step 605, loss 0.0441169, acc 1\n",
      "2018-03-24T23:58:38.583680: step 606, loss 0.0440395, acc 1\n",
      "2018-03-24T23:58:39.948299: step 607, loss 0.0445086, acc 1\n",
      "2018-03-24T23:58:40.479860: step 608, loss 0.041328, acc 1\n",
      "2018-03-24T23:58:41.842924: step 609, loss 0.0453925, acc 1\n",
      "2018-03-24T23:58:43.177077: step 610, loss 0.042954, acc 1\n",
      "2018-03-24T23:58:44.502148: step 611, loss 0.0430409, acc 1\n",
      "2018-03-24T23:58:45.869975: step 612, loss 0.0402734, acc 1\n",
      "2018-03-24T23:58:47.202669: step 613, loss 0.042269, acc 1\n",
      "2018-03-24T23:58:48.564580: step 614, loss 0.0416228, acc 1\n",
      "2018-03-24T23:58:49.929099: step 615, loss 0.0403716, acc 1\n",
      "2018-03-24T23:58:51.295250: step 616, loss 0.0443639, acc 1\n",
      "2018-03-24T23:58:52.640346: step 617, loss 0.0447368, acc 1\n",
      "2018-03-24T23:58:53.976885: step 618, loss 0.0451444, acc 1\n",
      "2018-03-24T23:58:55.299661: step 619, loss 0.0404182, acc 1\n",
      "2018-03-24T23:58:56.693195: step 620, loss 0.0413817, acc 1\n",
      "2018-03-24T23:58:58.041053: step 621, loss 0.0431979, acc 1\n",
      "2018-03-24T23:58:59.456120: step 622, loss 0.0491287, acc 1\n",
      "2018-03-24T23:59:00.818533: step 623, loss 0.0534053, acc 0.984375\n",
      "2018-03-24T23:59:02.169808: step 624, loss 0.0405705, acc 1\n",
      "2018-03-24T23:59:03.525417: step 625, loss 0.041298, acc 1\n",
      "2018-03-24T23:59:04.870347: step 626, loss 0.0457531, acc 1\n",
      "2018-03-24T23:59:06.263161: step 627, loss 0.0392511, acc 1\n",
      "2018-03-24T23:59:07.629988: step 628, loss 0.0444696, acc 1\n",
      "2018-03-24T23:59:08.994436: step 629, loss 0.0414538, acc 1\n",
      "2018-03-24T23:59:10.355743: step 630, loss 0.0463082, acc 1\n",
      "2018-03-24T23:59:11.715535: step 631, loss 0.0380655, acc 1\n",
      "2018-03-24T23:59:13.070982: step 632, loss 0.0379513, acc 1\n",
      "2018-03-24T23:59:14.436084: step 633, loss 0.0387103, acc 1\n",
      "2018-03-24T23:59:15.784480: step 634, loss 0.0428481, acc 1\n",
      "2018-03-24T23:59:17.130925: step 635, loss 0.0457613, acc 1\n",
      "2018-03-24T23:59:18.474030: step 636, loss 0.0386768, acc 1\n",
      "2018-03-24T23:59:19.819478: step 637, loss 0.03883, acc 1\n",
      "2018-03-24T23:59:21.173774: step 638, loss 0.0418487, acc 1\n",
      "2018-03-24T23:59:22.553834: step 639, loss 0.046267, acc 1\n",
      "2018-03-24T23:59:23.075793: step 640, loss 0.0364629, acc 1\n",
      "2018-03-24T23:59:24.440138: step 641, loss 0.0388208, acc 1\n",
      "2018-03-24T23:59:25.803829: step 642, loss 0.0389665, acc 1\n",
      "2018-03-24T23:59:27.226192: step 643, loss 0.0382978, acc 1\n",
      "2018-03-24T23:59:28.609723: step 644, loss 0.0459695, acc 1\n",
      "2018-03-24T23:59:29.957351: step 645, loss 0.0413463, acc 1\n",
      "2018-03-24T23:59:31.306528: step 646, loss 0.0371406, acc 1\n",
      "2018-03-24T23:59:32.688489: step 647, loss 0.046889, acc 0.992188\n",
      "2018-03-24T23:59:34.108995: step 648, loss 0.0390978, acc 1\n",
      "2018-03-24T23:59:35.604808: step 649, loss 0.0487707, acc 1\n",
      "2018-03-24T23:59:37.096425: step 650, loss 0.0397413, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-03-24T23:59:40.833892: step 650, loss 0.0930675, acc 0.982054\n",
      "\n",
      "Saved model checkpoint to /Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel/9/checkpoints/model-650\n",
      "\n",
      "2018-03-24T23:59:42.524280: step 651, loss 0.039377, acc 1\n",
      "2018-03-24T23:59:44.032427: step 652, loss 0.0378667, acc 1\n",
      "2018-03-24T23:59:45.624583: step 653, loss 0.0389199, acc 1\n",
      "2018-03-24T23:59:47.055436: step 654, loss 0.0458835, acc 1\n",
      "2018-03-24T23:59:48.530180: step 655, loss 0.0413427, acc 1\n",
      "2018-03-24T23:59:50.002780: step 656, loss 0.0392225, acc 1\n",
      "2018-03-24T23:59:51.487760: step 657, loss 0.0496681, acc 0.992188\n",
      "2018-03-24T23:59:52.881606: step 658, loss 0.0363611, acc 1\n",
      "2018-03-24T23:59:54.384290: step 659, loss 0.057613, acc 0.984375\n",
      "2018-03-24T23:59:55.837386: step 660, loss 0.0353395, acc 1\n",
      "2018-03-24T23:59:57.377463: step 661, loss 0.039196, acc 1\n",
      "2018-03-24T23:59:58.769019: step 662, loss 0.0392518, acc 1\n",
      "2018-03-25T00:00:00.234719: step 663, loss 0.0362621, acc 1\n",
      "2018-03-25T00:00:01.661566: step 664, loss 0.0359656, acc 1\n",
      "2018-03-25T00:00:03.082240: step 665, loss 0.0366771, acc 1\n",
      "2018-03-25T00:00:04.503646: step 666, loss 0.0336712, acc 1\n",
      "2018-03-25T00:00:06.197335: step 667, loss 0.0351914, acc 1\n",
      "2018-03-25T00:00:07.851222: step 668, loss 0.0358935, acc 1\n",
      "2018-03-25T00:00:09.502696: step 669, loss 0.0356082, acc 1\n",
      "2018-03-25T00:00:10.937489: step 670, loss 0.0331334, acc 1\n",
      "2018-03-25T00:00:12.372929: step 671, loss 0.0400785, acc 0.992188\n",
      "2018-03-25T00:00:12.928528: step 672, loss 0.0346565, acc 1\n",
      "2018-03-25T00:00:14.405484: step 673, loss 0.0348396, acc 1\n",
      "2018-03-25T00:00:15.874455: step 674, loss 0.0372917, acc 1\n",
      "2018-03-25T00:00:17.316026: step 675, loss 0.0392673, acc 0.992188\n",
      "2018-03-25T00:00:18.776003: step 676, loss 0.0334345, acc 1\n",
      "2018-03-25T00:00:20.220541: step 677, loss 0.0338326, acc 1\n",
      "2018-03-25T00:00:21.650209: step 678, loss 0.0349446, acc 1\n",
      "2018-03-25T00:00:23.023560: step 679, loss 0.0388532, acc 1\n",
      "2018-03-25T00:00:24.426066: step 680, loss 0.0350594, acc 1\n",
      "2018-03-25T00:00:25.913238: step 681, loss 0.0344542, acc 1\n",
      "2018-03-25T00:00:27.467049: step 682, loss 0.0341475, acc 1\n",
      "2018-03-25T00:00:28.870432: step 683, loss 0.0337478, acc 1\n",
      "2018-03-25T00:00:30.261742: step 684, loss 0.0331635, acc 1\n",
      "2018-03-25T00:00:31.745270: step 685, loss 0.0380257, acc 0.992188\n",
      "2018-03-25T00:00:33.103887: step 686, loss 0.0401498, acc 1\n",
      "2018-03-25T00:00:34.472097: step 687, loss 0.034338, acc 1\n",
      "2018-03-25T00:00:35.853311: step 688, loss 0.0338017, acc 1\n",
      "2018-03-25T00:00:37.241369: step 689, loss 0.0369794, acc 1\n",
      "2018-03-25T00:00:38.625077: step 690, loss 0.0345378, acc 1\n",
      "2018-03-25T00:00:40.018808: step 691, loss 0.0316048, acc 1\n",
      "2018-03-25T00:00:41.378539: step 692, loss 0.0360963, acc 1\n",
      "2018-03-25T00:00:42.763511: step 693, loss 0.0347805, acc 1\n",
      "2018-03-25T00:00:44.197356: step 694, loss 0.0352072, acc 1\n",
      "2018-03-25T00:00:45.631792: step 695, loss 0.034486, acc 1\n",
      "2018-03-25T00:00:47.007381: step 696, loss 0.0320164, acc 1\n",
      "2018-03-25T00:00:48.447299: step 697, loss 0.0315608, acc 1\n",
      "2018-03-25T00:00:49.892475: step 698, loss 0.0348112, acc 1\n",
      "2018-03-25T00:00:51.335063: step 699, loss 0.0366808, acc 1\n",
      "2018-03-25T00:00:52.787059: step 700, loss 0.0332225, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-03-25T00:00:56.258764: step 700, loss 0.102302, acc 0.977069\n",
      "\n",
      "Saved model checkpoint to /Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel/9/checkpoints/model-700\n",
      "\n",
      "2018-03-25T00:00:58.100597: step 701, loss 0.0317112, acc 1\n",
      "2018-03-25T00:00:59.900213: step 702, loss 0.0348119, acc 1\n",
      "2018-03-25T00:01:01.443590: step 703, loss 0.0311642, acc 1\n",
      "2018-03-25T00:01:02.004882: step 704, loss 0.0299345, acc 1\n",
      "2018-03-25T00:01:03.543284: step 705, loss 0.0324157, acc 1\n",
      "2018-03-25T00:01:04.986497: step 706, loss 0.0367105, acc 1\n",
      "2018-03-25T00:01:06.579963: step 707, loss 0.0302961, acc 1\n",
      "2018-03-25T00:01:08.035261: step 708, loss 0.0347452, acc 1\n",
      "2018-03-25T00:01:09.491878: step 709, loss 0.0305029, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-25T00:01:10.977965: step 710, loss 0.030653, acc 1\n",
      "2018-03-25T00:01:12.471227: step 711, loss 0.03557, acc 1\n",
      "2018-03-25T00:01:13.960413: step 712, loss 0.0436663, acc 0.992188\n",
      "2018-03-25T00:01:15.417364: step 713, loss 0.0334332, acc 1\n",
      "2018-03-25T00:01:16.933673: step 714, loss 0.0334988, acc 1\n",
      "2018-03-25T00:01:18.724244: step 715, loss 0.0328226, acc 1\n",
      "2018-03-25T00:01:20.365227: step 716, loss 0.0427418, acc 0.992188\n",
      "2018-03-25T00:01:21.841256: step 717, loss 0.0344037, acc 1\n",
      "2018-03-25T00:01:23.389478: step 718, loss 0.0327512, acc 1\n",
      "2018-03-25T00:01:24.923410: step 719, loss 0.0289252, acc 1\n",
      "2018-03-25T00:01:26.715642: step 720, loss 0.0330934, acc 1\n",
      "2018-03-25T00:01:28.565087: step 721, loss 0.036761, acc 1\n",
      "2018-03-25T00:01:30.123627: step 722, loss 0.0284597, acc 1\n",
      "2018-03-25T00:01:31.893085: step 723, loss 0.0285528, acc 1\n",
      "2018-03-25T00:01:33.608208: step 724, loss 0.0325185, acc 1\n",
      "2018-03-25T00:01:35.440329: step 725, loss 0.0287573, acc 1\n",
      "2018-03-25T00:01:37.092411: step 726, loss 0.0314827, acc 1\n",
      "2018-03-25T00:01:38.808444: step 727, loss 0.0295462, acc 1\n",
      "2018-03-25T00:01:40.360521: step 728, loss 0.0296338, acc 1\n",
      "2018-03-25T00:01:41.913466: step 729, loss 0.0303814, acc 1\n",
      "2018-03-25T00:01:43.327065: step 730, loss 0.0347892, acc 1\n",
      "2018-03-25T00:01:44.645524: step 731, loss 0.0316578, acc 1\n",
      "2018-03-25T00:01:45.954782: step 732, loss 0.0290992, acc 1\n",
      "2018-03-25T00:01:47.276917: step 733, loss 0.0299465, acc 1\n",
      "2018-03-25T00:01:48.612496: step 734, loss 0.0321085, acc 1\n",
      "2018-03-25T00:01:49.986382: step 735, loss 0.0342521, acc 1\n",
      "2018-03-25T00:01:50.501890: step 736, loss 0.0280873, acc 1\n",
      "2018-03-25T00:01:51.877862: step 737, loss 0.0290107, acc 1\n",
      "2018-03-25T00:01:53.229861: step 738, loss 0.0309152, acc 1\n",
      "2018-03-25T00:01:54.571309: step 739, loss 0.0300566, acc 1\n",
      "2018-03-25T00:01:55.900049: step 740, loss 0.0303859, acc 1\n",
      "2018-03-25T00:01:57.383650: step 741, loss 0.0358693, acc 0.992188\n",
      "2018-03-25T00:01:58.963207: step 742, loss 0.0304438, acc 1\n",
      "2018-03-25T00:02:00.348716: step 743, loss 0.0291195, acc 1\n",
      "2018-03-25T00:02:01.836368: step 744, loss 0.029364, acc 1\n",
      "2018-03-25T00:02:03.442011: step 745, loss 0.0255856, acc 1\n",
      "2018-03-25T00:02:05.176007: step 746, loss 0.0290318, acc 1\n",
      "2018-03-25T00:02:06.928321: step 747, loss 0.0316307, acc 1\n",
      "2018-03-25T00:02:08.448072: step 748, loss 0.0288135, acc 1\n",
      "2018-03-25T00:02:09.893938: step 749, loss 0.0337059, acc 0.992188\n",
      "2018-03-25T00:02:11.735327: step 750, loss 0.0282247, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-03-25T00:02:16.083064: step 750, loss 0.0943813, acc 0.979063\n",
      "\n",
      "Saved model checkpoint to /Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel/9/checkpoints/model-750\n",
      "\n",
      "2018-03-25T00:02:17.800133: step 751, loss 0.0275709, acc 1\n",
      "2018-03-25T00:02:19.223846: step 752, loss 0.0286133, acc 1\n",
      "2018-03-25T00:02:20.582287: step 753, loss 0.0281779, acc 1\n",
      "2018-03-25T00:02:21.940169: step 754, loss 0.0301735, acc 1\n",
      "2018-03-25T00:02:23.280791: step 755, loss 0.0304707, acc 1\n",
      "2018-03-25T00:02:24.645637: step 756, loss 0.0297961, acc 1\n",
      "2018-03-25T00:02:26.040211: step 757, loss 0.0283732, acc 1\n",
      "2018-03-25T00:02:27.426517: step 758, loss 0.0478351, acc 0.992188\n",
      "2018-03-25T00:02:29.104983: step 759, loss 0.0282739, acc 1\n",
      "2018-03-25T00:02:30.585425: step 760, loss 0.0302839, acc 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-4b19db09f088>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0mcurrent_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcurrent_step\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-4b19db09f088>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(x_batch, y_batch)\u001b[0m\n\u001b[1;32m     72\u001b[0m             _, step, summaries, loss, accuracy = sess.run(\n\u001b[1;32m     73\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_summary_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                 feed_dict)\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mtime_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misoformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}: step {}, loss {:g}, acc {:g}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "      log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            num_classes=y_train.shape[1],\n",
    "            vocab_size=len(vocab_processor.vocabulary_),\n",
    "            embedding_size=FLAGS.embedding_dim,\n",
    "            filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "            num_filters=FLAGS.num_filters,\n",
    "            l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "\n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", \"cnnmodel\", str(trainingSection)))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "\n",
    "        # Write vocabulary\n",
    "        vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, accuracy = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "\n",
    "        # Generate batches\n",
    "        batches = data_helpers.batch_iter(\n",
    "            list(zip(x_train, y_train)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % FLAGS.evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % FLAGS.checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testDataDir = \"/Users/SamZhang/Documents/Capstone/dataset/\" + dataset + \"/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.flags.DEFINE_string(\"positive_test_data_file\", testDataDir + \"/spam/\" + str(trainingSection) + '/' + dataset + \"_test.spam\", \"Data source for the positive data.\")\n",
    "tf.flags.DEFINE_string(\"negative_test_data_file\", testDataDir + \"/ham/\" + str(trainingSection) + '/' + dataset + \"_test.ham\", \"Data source for the negative data.\")\n",
    "\n",
    "# Eval Parameters\n",
    "tf.flags.DEFINE_string(\"checkpoint_dir\", \"/Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel/\" + str(trainingSection) + \"/checkpoints/\", \"Checkpoint directory from training run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "BATCH_SIZE=128\n",
      "CHECKPOINT_DIR=/Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel/9/checkpoints/\n",
      "CHECKPOINT_EVERY=50\n",
      "DEV_SAMPLE_PERCENTAGE=0.2\n",
      "DROPOUT_KEEP_PROB=0.5\n",
      "EMBEDDING_DIM=128\n",
      "EVALUATE_EVERY=50\n",
      "FILTER_SIZES=3,4,5\n",
      "L2_REG_LAMBDA=0.1\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "NEGATIVE_DATA_FILE=/Users/SamZhang/Documents/Capstone/dataset/sms/train/ham/9/sms_train.ham\n",
      "NEGATIVE_TEST_DATA_FILE=/Users/SamZhang/Documents/Capstone/dataset/sms/test/ham/9/sms_test.ham\n",
      "NUM_CHECKPOINTS=5\n",
      "NUM_EPOCHS=40\n",
      "NUM_FILTERS=256\n",
      "POSITIVE_DATA_FILE=/Users/SamZhang/Documents/Capstone/dataset/sms/train/spam/9/sms_train.spam\n",
      "POSITIVE_TEST_DATA_FILE=/Users/SamZhang/Documents/Capstone/dataset/sms/test/spam/9/sms_test.spam\n",
      "\n"
     ]
    }
   ],
   "source": [
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07732584351 rodger burns msg we tried to call you re your reply to our sms for a free nokia mobile free camcorder please call now 08000930705 for delivery tomorrow 1\n"
     ]
    }
   ],
   "source": [
    "x_raw, y_test = data_helpers.load_data_and_labels(FLAGS.positive_test_data_file, FLAGS.negative_test_data_file)\n",
    "y_test = np.argmax(y_test, axis=1) #ham = 0, spam = 1\n",
    "print(x_raw[0], y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_path = os.path.join(FLAGS.checkpoint_dir, \"..\", \"vocab\")\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor.restore(vocab_path)\n",
    "x_test = np.array(list(vocab_processor.transform(x_raw)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint_file = tf.train.latest_checkpoint(FLAGS.checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel/9/checkpoints/model-750\n",
      "Total number of test examples: 556\n",
      "Accuracy: 0.96223\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "      log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        # Load the saved meta graph and restore variables\n",
    "        saver = tf.train.import_meta_graph(\"{}.meta\".format(checkpoint_file))\n",
    "        saver.restore(sess, checkpoint_file)\n",
    "\n",
    "        # Get the placeholders from the graph by name\n",
    "        input_x = graph.get_operation_by_name(\"input_x\").outputs[0]\n",
    "        # input_y = graph.get_operation_by_name(\"input_y\").outputs[0]\n",
    "        dropout_keep_prob = graph.get_operation_by_name(\"dropout_keep_prob\").outputs[0]\n",
    "\n",
    "        # Tensors we want to evaluate\n",
    "        predictions = graph.get_operation_by_name(\"output/predictions\").outputs[0]\n",
    "\n",
    "        # Generate batches for one epoch\n",
    "        batches = data_helpers.batch_iter(list(x_test), FLAGS.batch_size, 1, shuffle=False)\n",
    "\n",
    "        # Collect the predictions here\n",
    "        all_predictions = []\n",
    "\n",
    "        for x_test_batch in batches:\n",
    "            batch_predictions = sess.run(predictions, {input_x: x_test_batch, dropout_keep_prob: 1.0})\n",
    "            all_predictions = np.concatenate([all_predictions, batch_predictions])\n",
    "\n",
    "# Print accuracy if y_test is defined\n",
    "if y_test is not None:\n",
    "    correct_predictions = float(sum(all_predictions == y_test))\n",
    "    print(\"Total number of test examples: {}\".format(len(y_test)))\n",
    "    print(\"Accuracy: {:g}\".format(correct_predictions/float(len(y_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving evaluation to /Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel/9/checkpoints/../prediction.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the evaluation to a csv\n",
    "title = np.column_stack(('text', 'prediction', 'label'))\n",
    "predictions_human_readable = np.column_stack((np.array(x_raw), all_predictions, y_test))\n",
    "out_path = os.path.join(FLAGS.checkpoint_dir, \"..\", \"prediction.csv\")\n",
    "print(\"Saving evaluation to {0}\".format(out_path))\n",
    "\n",
    "with open(out_path, 'w') as f:\n",
    "    csv.writer(f).writerows(title)\n",
    "    csv.writer(f).writerows(predictions_human_readable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation and Noise Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53 0 482 21\n",
      "accuracy : 0.962 \n",
      "precision : 1.000 \n",
      "recall : 0.716 \n",
      "f1 : 0.835 \n",
      "\n",
      "52 0 482 22\n",
      "accuracy : 0.960 \n",
      "precision : 1.000 \n",
      "recall : 0.703 \n",
      "f1 : 0.825 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import ES_interface as esi\n",
    "\n",
    "esi.metric(dataset + '_' + str(trainingSection), out_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
