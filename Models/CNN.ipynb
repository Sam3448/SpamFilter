{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import data_helpers\n",
    "from text_cnn import TextCNN\n",
    "from tensorflow.contrib import learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = 'ling'\n",
    "trainDataDir = '/Users/SamZhang/Documents/Capstone/dataset/' + dataset + '/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading params\n",
    "tf.flags.DEFINE_float(\"dev_sample_percentage\", .2, \"Percentage of the training data to use for validation\")\n",
    "tf.flags.DEFINE_string(\"positive_data_file\", trainDataDir + \"/spam/\" + dataset + \"_train.spam\", \"Data source for the positive data.\")\n",
    "tf.flags.DEFINE_string(\"negative_data_file\", trainDataDir + \"/ham/\" + dataset + \"_train.ham\", \"Data source for the negative data.\")\n",
    "\n",
    "# Model Hyperparameters\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 256, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.1, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 128, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 10, \"Number of training epochs (default: 200)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "BATCH_SIZE=128\n",
      "CHECKPOINT_EVERY=100\n",
      "DEV_SAMPLE_PERCENTAGE=0.2\n",
      "DROPOUT_KEEP_PROB=0.5\n",
      "EMBEDDING_DIM=128\n",
      "EVALUATE_EVERY=100\n",
      "FILTER_SIZES=3,4,5\n",
      "L2_REG_LAMBDA=0.1\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "NEGATIVE_DATA_FILE=/Users/SamZhang/Documents/Capstone/dataset/ling/train/ham/ling_train.ham\n",
      "NUM_CHECKPOINTS=5\n",
      "NUM_EPOCHS=10\n",
      "NUM_FILTERS=256\n",
      "POSITIVE_DATA_FILE=/Users/SamZhang/Documents/Capstone/dataset/ling/train/spam/ling_train.spam\n",
      "\n"
     ]
    }
   ],
   "source": [
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def processData():\n",
    "    # Data Preparation\n",
    "    # ==================================================\n",
    "\n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    x_text, y = data_helpers.load_data_and_labels(FLAGS.positive_data_file, FLAGS.negative_data_file)\n",
    "    print(x_text[0])\n",
    "\n",
    "    # Build vocabulary\n",
    "    max_document_length = max([len(x.split(\" \")) for x in x_text])\n",
    "    vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "    x = np.array(list(vocab_processor.fit_transform(x_text)))\n",
    "\n",
    "    # Randomly shuffle data\n",
    "    np.random.seed(10)\n",
    "    shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "    x_shuffled = x[shuffle_indices]\n",
    "    y_shuffled = y[shuffle_indices]\n",
    "\n",
    "    # Split train/test set\n",
    "    # TODO: This is very crude, should use cross-validation\n",
    "    dev_sample_index = -1 * int(FLAGS.dev_sample_percentage * float(len(y)))\n",
    "    x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "    y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "\n",
    "    del x, y, x_shuffled, y_shuffled\n",
    "\n",
    "    print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
    "    print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))\n",
    "    \n",
    "    return x_train, y_train, x_dev, y_dev, vocab_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "great part time summer job ! display box credit application need place small owner operate store area here 1 introduce yourself store owner manager 2 our 90 effective script tell little display box save customer hundred dollar , draw card business , 5 0 15 0 every app send 3 spot counter , place box , nothing need , need name address company send commission check compensaation 10 every box place become representative earn commission 10 each application store course much profitable plan , pay month small effort call 1 888 703 5390 code 3 24 hours receive detail ! ! removed our mailing list , type b2998 hotmail com \\( \\) area \\( remove \\) subject area e mail send\n",
      "Vocabulary Size: 38556\n",
      "Train/Dev split: 1312/327\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_dev, y_dev, vocab_processor = processData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to /Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel\n",
      "\n",
      "2018-03-23T22:00:49.005608: step 1, loss 3.56388, acc 0.460938\n",
      "2018-03-23T22:01:05.495488: step 2, loss 1.96493, acc 0.648438\n",
      "2018-03-23T22:01:21.880521: step 3, loss 2.79858, acc 0.625\n",
      "2018-03-23T22:01:38.336485: step 4, loss 2.78579, acc 0.742188\n",
      "2018-03-23T22:01:54.986885: step 5, loss 2.34654, acc 0.71875\n",
      "2018-03-23T22:02:11.759703: step 6, loss 3.39101, acc 0.65625\n",
      "2018-03-23T22:02:28.480771: step 7, loss 2.25772, acc 0.671875\n",
      "2018-03-23T22:02:46.066648: step 8, loss 1.95908, acc 0.710938\n",
      "2018-03-23T22:03:03.988191: step 9, loss 2.21781, acc 0.65625\n",
      "2018-03-23T22:03:21.232388: step 10, loss 2.13842, acc 0.648438\n",
      "2018-03-23T22:03:25.476382: step 11, loss 1.72566, acc 0.53125\n",
      "2018-03-23T22:03:42.636924: step 12, loss 1.73847, acc 0.664062\n",
      "2018-03-23T22:04:00.142266: step 13, loss 1.61353, acc 0.71875\n",
      "2018-03-23T22:04:17.923136: step 14, loss 1.80585, acc 0.640625\n",
      "2018-03-23T22:04:34.661221: step 15, loss 1.6353, acc 0.6875\n",
      "2018-03-23T22:04:52.267849: step 16, loss 1.56912, acc 0.71875\n",
      "2018-03-23T22:05:09.162888: step 17, loss 1.65571, acc 0.710938\n",
      "2018-03-23T22:05:26.017279: step 18, loss 1.62014, acc 0.757812\n",
      "2018-03-23T22:05:43.555977: step 19, loss 1.64254, acc 0.742188\n",
      "2018-03-23T22:06:01.011832: step 20, loss 1.1459, acc 0.789062\n",
      "2018-03-23T22:06:17.753300: step 21, loss 1.21465, acc 0.796875\n",
      "2018-03-23T22:06:22.614121: step 22, loss 1.34969, acc 0.65625\n",
      "2018-03-23T22:06:41.728224: step 23, loss 1.1653, acc 0.789062\n",
      "2018-03-23T22:06:58.811098: step 24, loss 1.16972, acc 0.796875\n",
      "2018-03-23T22:07:16.254476: step 25, loss 1.15782, acc 0.804688\n",
      "2018-03-23T22:07:33.408828: step 26, loss 0.874914, acc 0.820312\n",
      "2018-03-23T22:07:50.792574: step 27, loss 0.967935, acc 0.835938\n",
      "2018-03-23T22:08:08.776651: step 28, loss 0.749049, acc 0.789062\n",
      "2018-03-23T22:08:26.680433: step 29, loss 1.07255, acc 0.78125\n",
      "2018-03-23T22:08:44.118061: step 30, loss 1.1295, acc 0.8125\n",
      "2018-03-23T22:09:01.222391: step 31, loss 0.776824, acc 0.8125\n",
      "2018-03-23T22:09:18.218070: step 32, loss 0.938103, acc 0.773438\n",
      "2018-03-23T22:09:22.464202: step 33, loss 1.7066, acc 0.71875\n",
      "2018-03-23T22:09:39.501666: step 34, loss 0.815776, acc 0.867188\n",
      "2018-03-23T22:09:56.060461: step 35, loss 0.932936, acc 0.851562\n",
      "2018-03-23T22:10:14.124317: step 36, loss 1.2609, acc 0.75\n",
      "2018-03-23T22:10:31.878105: step 37, loss 0.686159, acc 0.914062\n",
      "2018-03-23T22:10:50.784997: step 38, loss 0.900756, acc 0.851562\n",
      "2018-03-23T22:11:09.479174: step 39, loss 0.929469, acc 0.828125\n",
      "2018-03-23T22:11:26.682519: step 40, loss 0.731797, acc 0.867188\n",
      "2018-03-23T22:11:43.530009: step 41, loss 0.662757, acc 0.835938\n",
      "2018-03-23T22:12:00.483234: step 42, loss 0.62079, acc 0.867188\n",
      "2018-03-23T22:12:18.568025: step 43, loss 1.02748, acc 0.84375\n",
      "2018-03-23T22:12:23.270436: step 44, loss 0.804865, acc 0.90625\n",
      "2018-03-23T22:12:41.691369: step 45, loss 0.618816, acc 0.9375\n",
      "2018-03-23T22:12:59.713923: step 46, loss 0.72981, acc 0.859375\n",
      "2018-03-23T22:13:18.441686: step 47, loss 0.466998, acc 0.898438\n",
      "2018-03-23T22:13:36.147143: step 48, loss 0.55544, acc 0.875\n",
      "2018-03-23T22:13:52.782366: step 49, loss 0.690416, acc 0.84375\n",
      "2018-03-23T22:14:09.353837: step 50, loss 0.743184, acc 0.859375\n",
      "2018-03-23T22:14:26.170214: step 51, loss 0.784192, acc 0.890625\n",
      "2018-03-23T22:14:43.508464: step 52, loss 0.487496, acc 0.90625\n",
      "2018-03-23T22:15:01.933305: step 53, loss 0.68574, acc 0.875\n",
      "2018-03-23T22:15:20.116075: step 54, loss 0.536034, acc 0.90625\n",
      "2018-03-23T22:15:24.584953: step 55, loss 0.911778, acc 0.8125\n",
      "2018-03-23T22:15:43.578228: step 56, loss 0.57261, acc 0.898438\n",
      "2018-03-23T22:16:01.907149: step 57, loss 0.279131, acc 0.96875\n",
      "2018-03-23T22:16:20.136782: step 58, loss 0.403545, acc 0.921875\n",
      "2018-03-23T22:16:38.631598: step 59, loss 0.475583, acc 0.898438\n",
      "2018-03-23T22:16:56.027169: step 60, loss 0.634531, acc 0.851562\n",
      "2018-03-23T22:17:13.478948: step 61, loss 0.429428, acc 0.914062\n",
      "2018-03-23T22:17:32.090743: step 62, loss 0.489315, acc 0.90625\n",
      "2018-03-23T22:17:49.950842: step 63, loss 0.704612, acc 0.867188\n",
      "2018-03-23T22:18:07.654760: step 64, loss 0.547167, acc 0.898438\n",
      "2018-03-23T22:18:25.057168: step 65, loss 0.696681, acc 0.882812\n",
      "2018-03-23T22:18:29.841803: step 66, loss 0.531261, acc 0.875\n",
      "2018-03-23T22:18:47.310808: step 67, loss 0.4703, acc 0.921875\n",
      "2018-03-23T22:19:05.224191: step 68, loss 0.472016, acc 0.929688\n",
      "2018-03-23T22:19:24.127537: step 69, loss 0.435504, acc 0.914062\n",
      "2018-03-23T22:19:41.787856: step 70, loss 0.518862, acc 0.921875\n",
      "2018-03-23T22:20:00.676593: step 71, loss 0.430616, acc 0.921875\n",
      "2018-03-23T22:20:20.225080: step 72, loss 0.6315, acc 0.921875\n",
      "2018-03-23T22:20:38.338457: step 73, loss 0.325461, acc 0.9375\n",
      "2018-03-23T22:20:57.084232: step 74, loss 0.449316, acc 0.890625\n",
      "2018-03-23T22:21:15.257478: step 75, loss 0.390832, acc 0.9375\n",
      "2018-03-23T22:21:33.308111: step 76, loss 0.477943, acc 0.914062\n",
      "2018-03-23T22:21:37.656381: step 77, loss 0.364232, acc 0.875\n",
      "2018-03-23T22:21:55.348604: step 78, loss 0.382119, acc 0.9375\n",
      "2018-03-23T22:22:12.636650: step 79, loss 0.461221, acc 0.929688\n",
      "2018-03-23T22:22:31.130546: step 80, loss 0.389618, acc 0.914062\n",
      "2018-03-23T22:22:49.921715: step 81, loss 0.326408, acc 0.945312\n",
      "2018-03-23T22:23:08.071008: step 82, loss 0.357992, acc 0.960938\n",
      "2018-03-23T22:23:25.770675: step 83, loss 0.508345, acc 0.90625\n",
      "2018-03-23T22:23:44.195506: step 84, loss 0.328899, acc 0.953125\n",
      "2018-03-23T22:24:02.074426: step 85, loss 0.377557, acc 0.9375\n",
      "2018-03-23T22:24:19.716889: step 86, loss 0.459301, acc 0.921875\n",
      "2018-03-23T22:24:37.737422: step 87, loss 0.507736, acc 0.929688\n",
      "2018-03-23T22:24:42.315050: step 88, loss 0.236058, acc 0.96875\n",
      "2018-03-23T22:25:00.594373: step 89, loss 0.359632, acc 0.945312\n",
      "2018-03-23T22:25:18.579871: step 90, loss 0.340696, acc 0.929688\n",
      "2018-03-23T22:25:36.206921: step 91, loss 0.372879, acc 0.929688\n",
      "2018-03-23T22:25:54.097569: step 92, loss 0.327, acc 0.953125\n",
      "2018-03-23T22:26:11.827119: step 93, loss 0.395505, acc 0.929688\n",
      "2018-03-23T22:26:30.255181: step 94, loss 0.352018, acc 0.929688\n",
      "2018-03-23T22:26:47.876649: step 95, loss 0.316338, acc 0.960938\n",
      "2018-03-23T22:27:06.303970: step 96, loss 0.312997, acc 0.960938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-23T22:27:25.389066: step 97, loss 0.307092, acc 0.984375\n",
      "2018-03-23T22:27:45.128512: step 98, loss 0.253738, acc 0.96875\n",
      "2018-03-23T22:27:49.464903: step 99, loss 0.184802, acc 1\n",
      "2018-03-23T22:28:07.105455: step 100, loss 0.3578, acc 0.960938\n",
      "\n",
      "Evaluation:\n",
      "2018-03-23T22:28:27.342586: step 100, loss 0.256721, acc 0.972477\n",
      "\n",
      "Saved model checkpoint to /Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel/checkpoints/model-100\n",
      "\n",
      "2018-03-23T22:28:45.985141: step 101, loss 0.201092, acc 0.992188\n",
      "2018-03-23T22:29:03.577290: step 102, loss 0.270978, acc 0.976562\n",
      "2018-03-23T22:29:21.316743: step 103, loss 0.289877, acc 0.960938\n",
      "2018-03-23T22:29:40.045398: step 104, loss 0.304361, acc 0.945312\n",
      "2018-03-23T22:29:58.536739: step 105, loss 0.362145, acc 0.953125\n",
      "2018-03-23T22:30:17.313836: step 106, loss 0.305774, acc 0.96875\n",
      "2018-03-23T22:30:36.428031: step 107, loss 0.463474, acc 0.953125\n",
      "2018-03-23T22:30:54.810420: step 108, loss 0.379766, acc 0.945312\n",
      "2018-03-23T22:31:14.047797: step 109, loss 0.304439, acc 0.953125\n",
      "2018-03-23T22:31:18.725190: step 110, loss 0.199149, acc 0.96875\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "      log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            num_classes=y_train.shape[1],\n",
    "            vocab_size=len(vocab_processor.vocabulary_),\n",
    "            embedding_size=FLAGS.embedding_dim,\n",
    "            filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "            num_filters=FLAGS.num_filters,\n",
    "            l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "\n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", \"cnnmodel\"))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "\n",
    "        # Write vocabulary\n",
    "        vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, accuracy = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "\n",
    "        # Generate batches\n",
    "        batches = data_helpers.batch_iter(\n",
    "            list(zip(x_train, y_train)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % FLAGS.evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % FLAGS.checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testDataDir = \"/Users/SamZhang/Documents/Capstone/dataset/\" + dataset + \"/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.flags.DEFINE_string(\"positive_test_data_file\", testDataDir + \"/spam/\" + dataset + \"_test.spam\", \"Data source for the positive data.\")\n",
    "tf.flags.DEFINE_string(\"negative_test_data_file\", testDataDir + \"/ham/\" + dataset + \"_test.ham\", \"Data source for the negative data.\")\n",
    "\n",
    "# Eval Parameters\n",
    "tf.flags.DEFINE_string(\"checkpoint_dir\", \"/Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel/checkpoints/\", \"Checkpoint directory from training run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "BATCH_SIZE=128\n",
      "CHECKPOINT_DIR=/Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel/checkpoints/\n",
      "CHECKPOINT_EVERY=100\n",
      "DEV_SAMPLE_PERCENTAGE=0.2\n",
      "DROPOUT_KEEP_PROB=0.5\n",
      "EMBEDDING_DIM=128\n",
      "EVALUATE_EVERY=100\n",
      "FILTER_SIZES=3,4,5\n",
      "L2_REG_LAMBDA=0.1\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "NEGATIVE_DATA_FILE=/Users/SamZhang/Documents/Capstone/dataset/ling/train/ham/ling_train.ham\n",
      "NEGATIVE_TEST_DATA_FILE=/Users/SamZhang/Documents/Capstone/dataset/ling/test/ham/ling_test.ham\n",
      "NUM_CHECKPOINTS=5\n",
      "NUM_EPOCHS=10\n",
      "NUM_FILTERS=256\n",
      "POSITIVE_DATA_FILE=/Users/SamZhang/Documents/Capstone/dataset/ling/train/spam/ling_train.spam\n",
      "POSITIVE_TEST_DATA_FILE=/Users/SamZhang/Documents/Capstone/dataset/ling/test/spam/ling_test.spam\n",
      "\n"
     ]
    }
   ],
   "source": [
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3500 per week home computer ! put free software computer start huge amount cash without work ! ! ! ! computer does work money ! ! ! never talk anybody ! ! load free software computer let believe much money rolling efforts computer ! ! ! information email manboca hotmail com 1\n"
     ]
    }
   ],
   "source": [
    "x_raw, y_test = data_helpers.load_data_and_labels(FLAGS.positive_test_data_file, FLAGS.negative_test_data_file)\n",
    "y_test = np.argmax(y_test, axis=1) #ham = 0, spam = 1\n",
    "print(x_raw[0], y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_path = os.path.join(FLAGS.checkpoint_dir, \"..\", \"vocab\")\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor.restore(vocab_path)\n",
    "x_test = np.array(list(vocab_processor.transform(x_raw)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint_file = tf.train.latest_checkpoint(FLAGS.checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel/checkpoints/model-100\n",
      "Total number of test examples: 1254\n",
      "Accuracy: 0.992823\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "      log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        # Load the saved meta graph and restore variables\n",
    "        saver = tf.train.import_meta_graph(\"{}.meta\".format(checkpoint_file))\n",
    "        saver.restore(sess, checkpoint_file)\n",
    "\n",
    "        # Get the placeholders from the graph by name\n",
    "        input_x = graph.get_operation_by_name(\"input_x\").outputs[0]\n",
    "        # input_y = graph.get_operation_by_name(\"input_y\").outputs[0]\n",
    "        dropout_keep_prob = graph.get_operation_by_name(\"dropout_keep_prob\").outputs[0]\n",
    "\n",
    "        # Tensors we want to evaluate\n",
    "        predictions = graph.get_operation_by_name(\"output/predictions\").outputs[0]\n",
    "\n",
    "        # Generate batches for one epoch\n",
    "        batches = data_helpers.batch_iter(list(x_test), FLAGS.batch_size, 1, shuffle=False)\n",
    "\n",
    "        # Collect the predictions here\n",
    "        all_predictions = []\n",
    "\n",
    "        for x_test_batch in batches:\n",
    "            batch_predictions = sess.run(predictions, {input_x: x_test_batch, dropout_keep_prob: 1.0})\n",
    "            all_predictions = np.concatenate([all_predictions, batch_predictions])\n",
    "\n",
    "# Print accuracy if y_test is defined\n",
    "if y_test is not None:\n",
    "    correct_predictions = float(sum(all_predictions == y_test))\n",
    "    print(\"Total number of test examples: {}\".format(len(y_test)))\n",
    "    print(\"Accuracy: {:g}\".format(correct_predictions/float(len(y_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving evaluation to /Users/SamZhang/Documents/Capstone/Models/runs/cnnmodel/checkpoints/../prediction.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the evaluation to a csv\n",
    "title = np.column_stack(('text', 'prediction', 'label'))\n",
    "predictions_human_readable = np.column_stack((np.array(x_raw), all_predictions, y_test))\n",
    "out_path = os.path.join(FLAGS.checkpoint_dir, \"..\", \"prediction.csv\")\n",
    "print(\"Saving evaluation to {0}\".format(out_path))\n",
    "\n",
    "with open(out_path, 'w') as f:\n",
    "    csv.writer(f).writerows(title)\n",
    "    csv.writer(f).writerows(predictions_human_readable)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
